[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISLR tidymodels labs",
    "section": "",
    "text": "This book aims to be a complement to the 2nd version An Introduction to Statistical Learning book with translations of the labs into using the tidymodels set of packages.\nThe labs will be mirrored quite closely to stay true to the original material.\n\nall listed changes will be relative to the 1st version.\n\n\nNaive Bayes has been added to chapter 4 on Classification\n\n\nPoisson Regression has been added to chapter 4 on Classification\n\n“Application to Caravan Insurance Data” section is no longer treated as its own section and is now part of the K-Nearest Neighbors section\n\nBayesian Additive Regression Trees has been added to chapter 8 on Tree-Based Methods\n\nchapter on Unsupervised Learning has been renumbered to chapter 12 instead of 10\n\nMatrix Completion has been added to chapter 12 on Unsupervised Learning\n\nchapter on Deep learning has been added as chapter 10\nchapter on Survival Analysis and Censored Data as been added as chapter 11\nchapter on Multiple Testing as been added as chapter 13\n\nThis book was written in RStudio using quarto. The website is hosted via GitHub Pages, and the complete source is available on GitHub.\nThis version of the book was built with R version 4.2.1 (2022-06-23) and the following packages:\n\n\n\npackage\nversion\nsource\n\n\n\nbroom\n1.0.1\nCRAN (R 4.2.0)\n\n\ncorrr\n0.4.4\nCRAN (R 4.2.0)\n\n\ndials\n1.1.0\nCRAN (R 4.2.1)\n\n\ndiscrim\n1.0.0\nCRAN (R 4.2.0)\n\n\ndownlit\n0.4.2\nCRAN (R 4.2.0)\n\n\ndplyr\n1.0.10\nCRAN (R 4.2.0)\n\n\nfactoextra\n1.0.7\nCRAN (R 4.2.0)\n\n\nggplot2\n3.4.0\nCRAN (R 4.2.1)\n\n\nglmnet\n4.1-4\nCRAN (R 4.2.0)\n\n\ninfer\n1.0.3\nCRAN (R 4.2.0)\n\n\nISLR\n1.4\nCRAN (R 4.2.0)\n\n\nISLR2\n1.3-1\nCRAN (R 4.2.0)\n\n\nkernlab\n0.9-31\nCRAN (R 4.2.0)\n\n\nkknn\n1.3.1\nCRAN (R 4.2.0)\n\n\nklaR\n1.7-1\nCRAN (R 4.2.0)\n\n\nMASS\n7.3-57\nCRAN (R 4.2.1)\n\n\nmclust\n6.0.0\nCRAN (R 4.2.0)\n\n\nmodeldata\n1.0.1\nCRAN (R 4.2.1)\n\n\npaletteer\n1.5.0\nCRAN (R 4.2.0)\n\n\nparsnip\n1.0.2\nCRAN (R 4.2.0)\n\n\npatchwork\n1.1.2\nCRAN (R 4.2.0)\n\n\nproxy\n0.4-27\nCRAN (R 4.2.0)\n\n\npurrr\n0.3.5\nCRAN (R 4.2.0)\n\n\nrandomForest\n4.7-1.1\nCRAN (R 4.2.0)\n\n\nreadr\n2.1.3\nCRAN (R 4.2.0)\n\n\nrecipes\n1.0.2\nCRAN (R 4.2.0)\n\n\nrpart\n4.1.16\nCRAN (R 4.2.1)\n\n\nrpart.plot\n3.1.1\nCRAN (R 4.2.0)\n\n\nrsample\n1.1.0\nCRAN (R 4.2.0)\n\n\nscico\n1.3.1\nCRAN (R 4.2.0)\n\n\ntibble\n3.1.8\nCRAN (R 4.2.0)\n\n\ntidymodels\n1.0.0\nCRAN (R 4.2.0)\n\n\ntidyr\n1.2.1\nCRAN (R 4.2.0)\n\n\ntidyverse\n1.3.2\nCRAN (R 4.2.0)\n\n\ntune\n1.0.1\nCRAN (R 4.2.0)\n\n\nvip\n0.3.2\nCRAN (R 4.2.0)\n\n\nworkflows\n1.1.0\nCRAN (R 4.2.0)\n\n\nworkflowsets\n1.0.0\nCRAN (R 4.2.0)\n\n\nxgboost\n1.6.0.1\nCRAN (R 4.2.0)\n\n\nyardstick\n1.1.0\nCRAN (R 4.2.0)"
  },
  {
    "objectID": "02-statistical-learning.html",
    "href": "02-statistical-learning.html",
    "title": "2  Statistical learning",
    "section": "",
    "text": "The primary purpose of this book is to rewrite the labs of ISLR using the tidymodels packages. A great introduction to tidymodels can be found on the tidymodels website.\nProper introductions to the various tidymodels packages will not be present in these labs since they aim to mirror the labs in ISLR. The getting started page has good introductions to\n\nparsnip\nrecipes and workflows\nrsample\ntune\n\nThe charts whenever possible will be created using ggplot2, if you are unfamiliar with ggplot2 I would recommend you start by reading the Data Visualisation chapter of the R for Data Science book.\nThe tidymodels packages can be installed as a whole using install.packages(\"tidymodels\") and the ISLR package which contains many of the data set we will be using can be installed using install.packages(\"ISLR\")."
  },
  {
    "objectID": "03-linear-regression.html",
    "href": "03-linear-regression.html",
    "title": "\n3  Linear Regression\n",
    "section": "",
    "text": "This lab will go over how to perform linear regression. This will include simple linear regression and multiple linear regression in addition to how you can apply transformations to the predictors. This chapter will use parsnip for model fitting and recipes and workflows to perform the transformations."
  },
  {
    "objectID": "03-linear-regression.html#libraries",
    "href": "03-linear-regression.html#libraries",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.1 Libraries",
    "text": "3.1 Libraries\nWe load tidymodels and ISLR and MASS for data sets.\n\nlibrary(MASS) # For Boston data set\nlibrary(tidymodels)\nlibrary(ISLR)"
  },
  {
    "objectID": "03-linear-regression.html#simple-linear-regression",
    "href": "03-linear-regression.html#simple-linear-regression",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.2 Simple linear regression",
    "text": "3.2 Simple linear regression\nThe Boston data set contains various statistics for 506 neighborhoods in Boston. We will build a simple linear regression model that related the median value of owner-occupied homes (medv) as the response with a variable indicating the percentage of the population that belongs to a lower status (lstat) as the predictor.\n\n\n\n\n\n\nImportant\n\n\n\nThe Boston data set is quite outdated and contains some really unfortunate variables.\n\n\nWe start by creating a parsnip specification for a linear regression model.\n\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n\nWhile it is unnecessary to set the mode for a linear regression since it can only be regression, we continue to do it in these labs to be explicit.\nThe specification doesn’t perform any calculations by itself. It is just a specification of what we want to do.\n\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOnce we have the specification we can fit it by supplying a formula expression and the data we want to fit the model on. The formula is written on the form y ~ x where y is the name of the response and x is the name of the predictors. The names used in the formula should match the names of the variables in the data set passed to data.\n\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat, data = Boston)\n\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\nThe result of this fit is a parsnip model object. This object contains the underlying fit as well as some parsnip-specific information. If we want to look at the underlying fit object we can access it with lm_fit$fit or with\n\nlm_fit %>% \n  pluck(\"fit\")\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\nThe lm object has a nice summary() method that shows more information about the fit, including parameter estimates and lack-of-fit statistics.\n\nlm_fit %>% \n  pluck(\"fit\") %>%\n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\nWe can use packages from the broom package to extract key information out of the model objects in tidy formats.\nthe tidy() function returns the parameter estimates of a lm object\n\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236\n2 lstat         -0.950    0.0387     -24.5 5.08e- 88\n\n\nand glance() can be used to extract the model statistics.\n\nglance(lm_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.544         0.543  6.22      602. 5.08e-88     1 -1641. 3289. 3302.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nSuppose that we like the model fit and we want to generate predictions, we would typically use the predict() function like so:\n\npredict(lm_fit)\n\nError in predict_numeric(object = object, new_data = new_data, ...): argument \"new_data\" is missing, with no default\n\n\nBut this produces an error when used on a parsnip model object. This is happening because we need to explicitly supply the data set that the predictions should be performed on via the new_data argument\n\npredict(lm_fit, new_data = Boston)\n\n# A tibble: 506 × 1\n   .pred\n   <dbl>\n 1 29.8 \n 2 25.9 \n 3 30.7 \n 4 31.8 \n 5 29.5 \n 6 29.6 \n 7 22.7 \n 8 16.4 \n 9  6.12\n10 18.3 \n# … with 496 more rows\n\n\nNotice how the predictions are returned as a tibble. This will always be the case for parsnip models, no matter what engine is used. This is very useful since consistency allows us to combine data sets easily.\nWe can also return other types of predicts by specifying the type argument. Setting type = \"conf_int\" return a 95% confidence interval.\n\npredict(lm_fit, new_data = Boston, type = \"conf_int\")\n\n# A tibble: 506 × 2\n   .pred_lower .pred_upper\n         <dbl>       <dbl>\n 1       29.0        30.6 \n 2       25.3        26.5 \n 3       29.9        31.6 \n 4       30.8        32.7 \n 5       28.7        30.3 \n 6       28.8        30.4 \n 7       22.2        23.3 \n 8       15.6        17.1 \n 9        4.70        7.54\n10       17.7        18.9 \n# … with 496 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nNot all engines can return all types of predictions.\n\n\nIf you want to evaluate the performance of a model, you might want to compare the observed value and the predicted value for a data set. You\n\nbind_cols(\n  predict(lm_fit, new_data = Boston),\n  Boston\n) %>%\n  select(medv, .pred)\n\n# A tibble: 506 × 2\n    medv .pred\n   <dbl> <dbl>\n 1  24   29.8 \n 2  21.6 25.9 \n 3  34.7 30.7 \n 4  33.4 31.8 \n 5  36.2 29.5 \n 6  28.7 29.6 \n 7  22.9 22.7 \n 8  27.1 16.4 \n 9  16.5  6.12\n10  18.9 18.3 \n# … with 496 more rows\n\n\nYou can get the same results using the augment() function to save you a little bit of typing.\n\naugment(lm_fit, new_data = Boston) %>% \n  select(medv, .pred)\n\n# A tibble: 506 × 2\n    medv .pred\n   <dbl> <dbl>\n 1  24   29.8 \n 2  21.6 25.9 \n 3  34.7 30.7 \n 4  33.4 31.8 \n 5  36.2 29.5 \n 6  28.7 29.6 \n 7  22.9 22.7 \n 8  27.1 16.4 \n 9  16.5  6.12\n10  18.9 18.3 \n# … with 496 more rows"
  },
  {
    "objectID": "03-linear-regression.html#multiple-linear-regression",
    "href": "03-linear-regression.html#multiple-linear-regression",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.3 Multiple linear regression",
    "text": "3.3 Multiple linear regression\nThe multiple linear regression model can be fit in much the same way as the simple linear regression model. The only difference is how we specify the predictors. We are using the same formula expression y ~ x, but we can specify multiple values by separating them with +s.\n\nlm_fit2 <- lm_spec %>% \n  fit(medv ~ lstat + age, data = Boston)\n\nlm_fit2\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat + age, data = data)\n\nCoefficients:\n(Intercept)        lstat          age  \n   33.22276     -1.03207      0.03454  \n\n\nEverything else works the same. From extracting parameter estimates\n\ntidy(lm_fit2)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180\n2 lstat        -1.03      0.0482    -21.4  8.42e- 73\n3 age           0.0345    0.0122      2.83 4.91e-  3\n\n\nto predicting new values\n\npredict(lm_fit2, new_data = Boston)\n\n# A tibble: 506 × 1\n   .pred\n   <dbl>\n 1 30.3 \n 2 26.5 \n 3 31.2 \n 4 31.8 \n 5 29.6 \n 6 29.9 \n 7 22.7 \n 8 16.8 \n 9  5.79\n10 18.5 \n# … with 496 more rows\n\n\nA shortcut when using formulas is to use the form y ~ . which means; set y as the response and set the remaining variables as predictors. This is very useful if you have a lot of variables and you don’t want to type them out.\n\nlm_fit3 <- lm_spec %>% \n  fit(medv ~ ., data = Boston)\n\nlm_fit3\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ ., data = data)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  \n         rm          age          dis          rad          tax      ptratio  \n  3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  \n      black        lstat  \n  9.312e-03   -5.248e-01  \n\n\nFor more formula syntax look at ?formula."
  },
  {
    "objectID": "03-linear-regression.html#interaction-terms",
    "href": "03-linear-regression.html#interaction-terms",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.4 Interaction terms",
    "text": "3.4 Interaction terms\nAdding interaction terms is quite easy to do using formula expressions. However, the syntax used to describe them isn’t accepted by all engines so we will go over how to include interaction terms using recipes as well.\nThere are two ways on including an interaction term; x:y and x * y\n\n\nx:y will include the interaction between x and y,\n\nx * y will include the interaction between x and y, x, and y, i.e. it is short for x:y + x + y.\n\nwith that out of the way let expand lm_fit2 by adding an interaction term\n\nlm_fit4 <- lm_spec %>%\n  fit(medv ~ lstat * age, data = Boston)\n\nlm_fit4\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat * age, data = data)\n\nCoefficients:\n(Intercept)        lstat          age    lstat:age  \n 36.0885359   -1.3921168   -0.0007209    0.0041560  \n\n\nnote that the interaction term is named lstat:age.\nSometimes we want to perform transformations, and we want those transformations to be applied, as part of the model fit as a pre-processing step. We will use the recipes package for this task.\nWe use the step_interact() to specify the interaction term. Next, we create a workflow object to combine the linear regression model specification lm_spec with the pre-processing specification rec_spec_interact which can then be fitted much like a parsnip model specification.\n\nrec_spec_interact <- recipe(medv ~ lstat + age, data = Boston) %>%\n  step_interact(~ lstat:age)\n\nlm_wf_interact <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_interact)\n\nlm_wf_interact %>% fit(Boston)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat          age  lstat_x_age  \n 36.0885359   -1.3921168   -0.0007209    0.0041560  \n\n\nNotice that since we specified the variables in the recipe we don’t need to specify them when fitting the workflow object. Furthermore, take note of the name of the interaction term. step_interact() tries to avoid special characters in variables."
  },
  {
    "objectID": "03-linear-regression.html#non-linear-transformations-of-the-predictors",
    "href": "03-linear-regression.html#non-linear-transformations-of-the-predictors",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.5 Non-linear transformations of the predictors",
    "text": "3.5 Non-linear transformations of the predictors\nMuch like we could use recipes to create interaction terms between values are we able to apply transformations to individual variables as well. If you are familiar with the dplyr package then you know how to mutate() which works in much the same way using step_mutate().\nYou would want to keep as much of the pre-processing inside recipes such that the transformation will be applied consistently to new data.\n\nrec_spec_pow2 <- recipe(medv ~ lstat, data = Boston) %>%\n  step_mutate(lstat2 = lstat ^ 2)\n\nlm_wf_pow2 <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_pow2)\n\nlm_wf_pow2 %>% fit(Boston)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_mutate()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat       lstat2  \n   42.86201     -2.33282      0.04355  \n\n\nYou don’t have to hand-craft every type of linear transformation since recipes have a bunch created already here such as step_log() to take logarithms of variables.\n\nrec_spec_log <- recipe(medv ~ lstat, data = Boston) %>%\n  step_log(lstat)\n\nlm_wf_log <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_log)\n\nlm_wf_log %>% fit(Boston)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_log()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      52.12       -12.48"
  },
  {
    "objectID": "03-linear-regression.html#qualitative-predictors",
    "href": "03-linear-regression.html#qualitative-predictors",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.6 Qualitative predictors",
    "text": "3.6 Qualitative predictors\nWe will now turn our attention to the Carseats data set. We will attempt to predict Sales of child car seats in 400 locations based on a number of predictors. One of these variables is ShelveLoc which is a qualitative predictor that indicates the quality of the shelving location. ShelveLoc takes on three possible values\n\nBad\nMedium\nGood\n\nIf you pass such a variable to lm() it will read it and generate dummy variables automatically using the following convention.\n\nCarseats %>%\n  pull(ShelveLoc) %>%\n  contrasts()\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nSo we have no problems including qualitative predictors when using lm as the engine.\n\nlm_spec %>% \n  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sales ~ . + Income:Advertising + Price:Age, \n    data = data)\n\nCoefficients:\n       (Intercept)           CompPrice              Income         Advertising  \n         6.5755654           0.0929371           0.0108940           0.0702462  \n        Population               Price       ShelveLocGood     ShelveLocMedium  \n         0.0001592          -0.1008064           4.8486762           1.9532620  \n               Age           Education            UrbanYes               USYes  \n        -0.0579466          -0.0208525           0.1401597          -0.1575571  \nIncome:Advertising           Price:Age  \n         0.0007510           0.0001068  \n\n\nHowever, as with so many things, we can not always guarantee that the underlying engine knows how to deal with qualitative variables. Recipes can be used to handle this as well. The step_dummy() will perform the same transformation of turning 1 qualitative with C levels into C-1 indicator variables. While this might seem unnecessary right now, some of the engines, later on, do not handle qualitative variables and this step would be necessary. We are also using the all_nominal_predictors() selector to select all character and factor predictor variables. This allows us to select by type rather than having to type out the names.\n\nrec_spec <- recipe(Sales ~ ., data = Carseats) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~ Income:Advertising + Price:Age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Carseats)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n         (Intercept)             CompPrice                Income  \n           6.5755654             0.0929371             0.0108940  \n         Advertising            Population                 Price  \n           0.0702462             0.0001592            -0.1008064  \n                 Age             Education        ShelveLoc_Good  \n          -0.0579466            -0.0208525             4.8486762  \n    ShelveLoc_Medium             Urban_Yes                US_Yes  \n           1.9532620             0.1401597            -0.1575571  \nIncome_x_Advertising           Price_x_Age  \n           0.0007510             0.0001068"
  },
  {
    "objectID": "03-linear-regression.html#writing-functions",
    "href": "03-linear-regression.html#writing-functions",
    "title": "\n3  Linear Regression\n",
    "section": "\n3.7 Writing functions",
    "text": "3.7 Writing functions\nThis book will not talk about how to write functions in R. If you still want to know how to write functions we recommend the functions of R for Data Science."
  },
  {
    "objectID": "04-classification.html",
    "href": "04-classification.html",
    "title": "\n4  Classification\n",
    "section": "",
    "text": "This lab will be our first experience with classification models. These models differ from the regression model we saw in the last chapter by the fact that the response variable is a qualitative variable instead of a continuous variable. This chapter will use parsnip for model fitting and recipes and workflows to perform the transformations."
  },
  {
    "objectID": "04-classification.html#the-stock-market-data",
    "href": "04-classification.html#the-stock-market-data",
    "title": "\n4  Classification\n",
    "section": "\n4.1 The Stock Market Data",
    "text": "4.1 The Stock Market Data\nWe load the tidymodels for modeling functions, ISLR and ISLR2 for data sets, discrim to give us access to discriminant analysis models such as LDA and QDA as well as the Naive Bayes model and poissonreg for Poisson Regression.\n\nlibrary(tidymodels)\nlibrary(ISLR) # For the Smarket data set\nlibrary(ISLR2) # For the Bikeshare data set\nlibrary(discrim)\nlibrary(poissonreg)\n\n\n\n\nWe will be examining the Smarket data set for this lab. It contains a number of numeric variables plus a variable called Direction which has the two labels \"Up\" and \"Down\". Before we do on to modeling, let us take a look at the correlation between the variables.\nTo look at the correlation, we will use the corrr package. The correlate() function will calculate the correlation matrix between all the variables that it is being fed. We will therefore remove Direction as it is not numeric. Then we pass that to rplot() to quickly visualize the correlation matrix. I have also changed the colours argument to better see what is going on.\n\nlibrary(corrr)\ncor_Smarket <- Smarket %>%\n  select(-Direction) %>%\n  correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))\n\n\n\n\nAnd we see that these variables are more or less uncorrelated with each other. The other pair is Year and Volume that is a little correlated.\nIf you want to create heatmap styled correlation chart you can also create it manually.\n\nlibrary(paletteer)\ncor_Smarket %>%\n  stretch() %>%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1)\n\n\n\n\nIf we plot Year against Volume we see that there is an upwards trend in Volume with time.\n\nggplot(Smarket, aes(Year, Volume)) +\n  geom_jitter(height = 0)"
  },
  {
    "objectID": "04-classification.html#logistic-regression",
    "href": "04-classification.html#logistic-regression",
    "title": "\n4  Classification\n",
    "section": "\n4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nNow we will fit a logistic regression model. We will again use the parsnip package, and we will use logistic_reg() to create a logistic regression model specification.\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nNotice that while I did set the engine and mode, they are just restating the defaults.\nWe can now fit the model like normal. We want to model the Direction of the stock market based on the percentage return from the 5 previous days plus the volume of shares traded. When fitting a classification with parsnip requires that the response variable is a factor. This is the case for the Smarket data set so we don’t need to do adjustments.\n\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket\n    )\n\nlr_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  \n  -0.126000    -0.073074    -0.042301     0.011085     0.009359     0.010313  \n     Volume  \n   0.135441  \n\nDegrees of Freedom: 1249 Total (i.e. Null);  1243 Residual\nNull Deviance:      1731 \nResidual Deviance: 1728     AIC: 1742\n\n\nthis fit is done using the glm() function, and it comes with a very handy summary() method as well.\n\nlr_fit %>%\n  pluck(\"fit\") %>%\n  summary()\n\n\nCall:\nstats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.446  -1.203   1.065   1.145   1.326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThis lets us see a couple of different things such as; parameter estimates, standard errors, p-values, and model fit statistics. we can use the tidy() function to extract some of these model attributes for further analysis or presentation.\n\ntidy(lr_fit)\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept) -0.126      0.241     -0.523   0.601\n2 Lag1        -0.0731     0.0502    -1.46    0.145\n3 Lag2        -0.0423     0.0501    -0.845   0.398\n4 Lag3         0.0111     0.0499     0.222   0.824\n5 Lag4         0.00936    0.0500     0.187   0.851\n6 Lag5         0.0103     0.0495     0.208   0.835\n7 Volume       0.135      0.158      0.855   0.392\n\n\nPredictions are done much the same way. Here we use the model to predict on the data it was trained on.\n\npredict(lr_fit, new_data = Smarket)\n\n# A tibble: 1,250 × 1\n   .pred_class\n   <fct>      \n 1 Up         \n 2 Down       \n 3 Down       \n 4 Up         \n 5 Up         \n 6 Up         \n 7 Down       \n 8 Up         \n 9 Up         \n10 Down       \n# … with 1,240 more rows\n\n\nThe result is a tibble with a single column .pred_class which will be a factor variable of the same labels as the original training data set.\nWe can also get back probability predictions, by specifying type = \"prob\".\n\npredict(lr_fit, new_data = Smarket, type = \"prob\")\n\n# A tibble: 1,250 × 2\n   .pred_Down .pred_Up\n        <dbl>    <dbl>\n 1      0.493    0.507\n 2      0.519    0.481\n 3      0.519    0.481\n 4      0.485    0.515\n 5      0.489    0.511\n 6      0.493    0.507\n 7      0.507    0.493\n 8      0.491    0.509\n 9      0.482    0.518\n10      0.511    0.489\n# … with 1,240 more rows\n\n\nnote that we get back a column for each of the classes. This is a little reductive since we could easily calculate the inverse, but once we get to multi-classification models it becomes quite handy.\nUsing augment() we can add the predictions to the data.frame and then use that to look at model performance metrics. before we calculate the metrics directly, I find it useful to look at the confusion matrix. This will show you how well your predictive model is performing by given a table of predicted values against the true value.\n\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class)\n\n          Truth\nPrediction Down  Up\n      Down  145 141\n      Up    457 507\n\n\nA good performing model would ideally have high numbers along the diagonal (up-left to down-right) with small numbers on the off-diagonal. We see here that the model isn’t great, as it tends to predict \"Down\" as \"Up\" more often than it should.\nif you want a more visual representation of the confusion matrix you can pipe the result of conf_mat() into autoplot() to generate a ggplot2 chart.\n\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\n\nWe can also calculate various performance metrics. One of the most common metrics is accuracy, which is how often the model predicted correctly as a percentage.\n\naugment(lr_fit, new_data = Smarket) %>%\n  accuracy(truth = Direction, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.522\n\n\nand we see that the accuracy isn’t great either which is obvious already looking at the confusion matrix.\nWe just fit a model and evaluated it on the same data. This doesn’t give us that much information about the model performance. Let us instead split up the data, train it on some of it and then evaluate it on the other part of the data. Since we are working with some data that has a time component, it is natural to fit the model using the first year’s worth of data and evaluate it on the last year. This would more closely match how such a model would be used in real life.\n\nSmarket_train <- Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- Smarket %>%\n  filter(Year == 2005)\n\nNow that we have split the data into Smarket_train and Smarket_test we can fit a logistic regression model to Smarket_train and evaluate it on Smarket_test to see how well the model generalizes.\n\nlr_fit2 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket_train\n    )\n\nAnd we will evaluate on the testing data set.\n\naugment(lr_fit2, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down Up\n      Down   77 97\n      Up     34 44\n\naugment(lr_fit2, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.480\n\n\nWe see that this model is not more likely to predict \"Down\" rather than \"Up\". Also, note how the model performs worse than the last model. This is expected since we are evaluating on new data.\nWe recall that the logistic regression model had underwhelming p-values. Let us see what happens if we remove some of the variables that appear not to be helpful we might achieve a more predictive model since the variables that do not have a relationship with the response will cause an increase in variance without a decrease in bias.\n\nlr_fit3 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2,\n    data = Smarket_train\n    )\n\naugment(lr_fit3, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naugment(lr_fit3, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560\n\n\nAnd we see an increase in performance. The model is still not perfect but it is starting to perform better.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and −0.8.\nFor this we start by creating a tibble corresponding to the scenarios we want to predict for\n\nSmarket_new <- tibble(\n  Lag1 = c(1.2, 1.5), \n  Lag2 = c(1.1, -0.8)\n)\n\nAnd then we will use predict()\n\npredict(\n  lr_fit3,\n  new_data = Smarket_new, \n  type = \"prob\"\n)\n\n# A tibble: 2 × 2\n  .pred_Down .pred_Up\n       <dbl>    <dbl>\n1      0.521    0.479\n2      0.504    0.496"
  },
  {
    "objectID": "04-classification.html#linear-discriminant-analysis",
    "href": "04-classification.html#linear-discriminant-analysis",
    "title": "\n4  Classification\n",
    "section": "\n4.3 Linear Discriminant Analysis",
    "text": "4.3 Linear Discriminant Analysis\nNow we will perform LDA on the Smarket data. We will use the discrim_linear() function to create a LDA specification. We will continue to use 2 predictors for easy comparison.\n\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nlda_fit <- lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nlda_fit\n\nparsnip model object\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = data)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n\nOne of the things to look for in the LDA output is the group means. We see that there is a slight difference between the means of the two groups. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous day’s returns to be positive on days when the market declines.\nPredictions are done just the same as with logistic regression:\n\npredict(lda_fit, new_data = Smarket_test)\n\n# A tibble: 252 × 1\n   .pred_class\n   <fct>      \n 1 Up         \n 2 Up         \n 3 Up         \n 4 Up         \n 5 Up         \n 6 Up         \n 7 Up         \n 8 Up         \n 9 Up         \n10 Up         \n# … with 242 more rows\n\npredict(lda_fit, new_data = Smarket_test, type = \"prob\")\n\n# A tibble: 252 × 2\n   .pred_Down .pred_Up\n        <dbl>    <dbl>\n 1      0.490    0.510\n 2      0.479    0.521\n 3      0.467    0.533\n 4      0.474    0.526\n 5      0.493    0.507\n 6      0.494    0.506\n 7      0.495    0.505\n 8      0.487    0.513\n 9      0.491    0.509\n10      0.484    0.516\n# … with 242 more rows\n\n\nAnd we can take a look at the performance.\n\naugment(lda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naugment(lda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560\n\n\nAnd we see no markedly different performance between this model and the logistic regression model."
  },
  {
    "objectID": "04-classification.html#quadratic-discriminant-analysis",
    "href": "04-classification.html#quadratic-discriminant-analysis",
    "title": "\n4  Classification\n",
    "section": "\n4.4 Quadratic Discriminant Analysis",
    "text": "4.4 Quadratic Discriminant Analysis\nWe will now fit a QDA model. The discrim_quad() function is used here.\nOnce we have the model specification fitting the model is just like before.\n\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nqda_fit <- qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\n\naugment(qda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   30  20\n      Up     81 121\n\naugment(qda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.599\n\n\nAnd we are seeing another increase in accuracy. However this model still rarely predicts \"Down\". This make it appear that the quadratic form assumed by QDA captures the relationship more clearly."
  },
  {
    "objectID": "04-classification.html#naive-bayes",
    "href": "04-classification.html#naive-bayes",
    "title": "\n4  Classification\n",
    "section": "\n4.5 Naive Bayes",
    "text": "4.5 Naive Bayes\nWe will now fit a Naive Bayes model to the Smarket data. For this, we will be using the naive_Bayes() function to create the specification and also set the usekernel argument to FALSE. This means that we are assuming that the predictors Lag1 and Lag2 are drawn from Gaussian distributions.\nOnce the model is specified, the fitting process is exactly like before:\n\nnb_spec <- naive_Bayes() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"klaR\") %>% \n  set_args(usekernel = FALSE)  \n\nnb_fit <- nb_spec %>% \n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nOnce the model is fit, we can create the confusion matrix based on the testing data and also assess the model accuracy.\n\naugment(nb_fit, new_data = Smarket_test) %>% \n  conf_mat(truth = Direction, estimate = .pred_class)\n\n          Truth\nPrediction Down  Up\n      Down   28  20\n      Up     83 121\n\n\n\naugment(nb_fit, new_data = Smarket_test) %>% \n  accuracy(truth = Direction, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.591\n\n\nThe accuracy of the Naive Bayes is very similar to that of the QDA model. This seems reasonable since the below scatter plot shows that there is no apparent relationship between Lag1 vs Lag2 and thus the Naive Bayes’ assumption of independently distributed predictors is not unreasonable.\n\nggplot(Smarket, aes(Lag1, Lag2)) +\n  geom_point(alpha = 0.1, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"No apparent correlation between Lag1 and Lag2\")"
  },
  {
    "objectID": "04-classification.html#k-nearest-neighbors",
    "href": "04-classification.html#k-nearest-neighbors",
    "title": "\n4  Classification\n",
    "section": "\n4.6 K-Nearest Neighbors",
    "text": "4.6 K-Nearest Neighbors\nLastly let us take a look at a K-Nearest Neighbors model. This is the first model we have looked at that has a hyperparameter we need to specify. I have set it to 3 with neighbors = 3. Fitting is done like normal.\n\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\nknn_fit <- knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nknn_fit\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.492986\nBest kernel: optimal\nBest k: 3\n\n\nAnd evaluation is done the same way:\n\naugment(knn_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down Up\n      Down   43 58\n      Up     68 83\n\naugment(knn_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary           0.5\n\n\nIt appears that this model is not performing that well.\nWe will try using a K-nearest neighbors model in an application to caravan insurance data. This data set includes 85 predictors that measure demographic characteristics for 5822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\nWe want to build a predictive model that uses the demographic characteristics to predict whether an individual is going to purchase a caravan insurance. Before we go on, we split the data set into a training data set and testing data set. (This is a not the proper way this should be done. See next chapter for the correct way.)\n\nCaravan_test <- Caravan[seq_len(1000), ]\nCaravan_train <- Caravan[-seq_len(1000), ]\n\nSince we are using a K-nearest neighbor model, it is importance that the variables are centered and scaled to make sure that the variables have a uniform influence. We can accomplish this transformation with step_normalize(), which does centering and scaling in one go.\n\nrec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%\n  step_normalize(all_numeric_predictors())\n\nWe will be trying different values of K to see how the number of neighbors affect the model performance. A workflow object is created, with just the recipe added.\n\nCaravan_wf <- workflow() %>%\n  add_recipe(rec_spec)\n\nNext we create a general KNN model specification.\n\nknn_spec <- nearest_neighbor() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\nWe can then use this model specification along with Caravan_wf to create 3 full workflow objects for K = 1,3,5.\n\nknn1_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 1))\n\nknn3_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 3))\n\nknn5_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 5))\n\nWith all these workflow specification we can fit all the models one by one.\n\nknn1_fit <- fit(knn1_wf, data = Caravan_train)\nknn3_fit <- fit(knn3_wf, data = Caravan_train)\nknn5_fit <- fit(knn5_wf, data = Caravan_train)\n\nAnd we can calculate all the confusion matrices.\n\naugment(knn1_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  874  50\n       Yes  67   9\n\n\n\naugment(knn3_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  875  50\n       Yes  66   9\n\n\n\naugment(knn5_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  874  50\n       Yes  67   9\n\n\nAnd it appears that the model performance doesn’t change much when changing from 1 to 5."
  },
  {
    "objectID": "04-classification.html#poisson-regression",
    "href": "04-classification.html#poisson-regression",
    "title": "\n4  Classification\n",
    "section": "\n4.7 Poisson Regression",
    "text": "4.7 Poisson Regression\nSo far we have been using the Smarket data set to predict the stock price movement. We will now shift to a new data set, Bikeshare, and look at the number of bike rentals per hour in Washington, D.C.\nThe variable of interest, number of bike rentals per hour, can take on non-negative integer values. This makes Poisson Regression a suitable candidate to model the same.\nWe start with specifying the model using the poisson_reg() function.\n\npois_spec <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\n\nHere we will be predicting bikers using the following predictors:\n\nmnth - month of the year, coded as a factor\nhr - hour of the day, coded as a factor from 0 to 23\nworkingday - Is it a workday? Already coded as a dummy variable with Yes = 1, No = 0\ntemp - normalized temperature in Celsius\n\nweathersit - weather condition, again coded as a factor with the following levels:\n\nclear\ncloudy/misty\nlight rain/snow\nheavy rain/snow\n\n\n\nAs we can see, apart from temp all other predictors are categorical in nature. Thus, we will first create a recipe to convert these into dummy variables and then bundle the model spec and recipe using a workflow.\n\npois_rec_spec <- recipe(\n  bikers ~ mnth + hr + workingday + temp + weathersit,\n  data = Bikeshare\n) %>% \n  step_dummy(all_nominal_predictors())\n\n\npois_wf <- workflow() %>% \n  add_recipe(pois_rec_spec) %>% \n  add_model(pois_spec)\n\nWith the workflow in place, we follow the same pattern to fit the model and look at the predictions.\n\npois_fit <- pois_wf %>% fit(data = Bikeshare)\n\naugment(pois_fit, new_data = Bikeshare, type.predict = \"response\") %>% \n  ggplot(aes(bikers, .pred)) +\n  geom_point(alpha = 0.1) +\n  geom_abline(slope = 1, size = 1, color = \"grey40\") +\n  labs(title = \"Predicting the number of bikers per hour using Poission Regression\",\n       x = \"Actual\", y = \"Predicted\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe can also look at the model coefficients to get a feel for the working of the model and comparing it with our own understanding.\nLooking at the coefficients corresponding to the mnth variable, we note that it is lower in the winter months and higher in the summer months. This seems logical as we would expect the number of bike rentals to be higher during summertime.\n\npois_fit_coef_mnths <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^mnth\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"mnth_\", \"\"),\n    term = forcats::fct_inorder(term)\n  ) \n\npois_fit_coef_mnths %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"Month\", y = \"Coefficient\")\n\n\n\n\nWe can similarly also look at the coefficients corresponding to the hr variable. Here the peaks occur at 8:00 AM and 5:00 PM, i.e. during normal office start and end times.\n\npois_fit_coef_hr <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^hr\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"hr_X\", \"\"),\n    term = forcats::fct_inorder(term)\n  )\n\npois_fit_coef_hr %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"hours\", y = \"Coefficient\")"
  },
  {
    "objectID": "04-classification.html#extra---comparing-multiple-models",
    "href": "04-classification.html#extra---comparing-multiple-models",
    "title": "\n4  Classification\n",
    "section": "\n4.8 Extra - comparing multiple models",
    "text": "4.8 Extra - comparing multiple models\nThis section is new and not part of ISLR. We have fitted a lot of different models in this lab. And we were able to calculate the performance metrics one by one, but it is not ideal if we want to compare the different models. Below is an example of how you can more conveniently calculate performance metrics for multiple models at the same time.\nStart of by creating a named list of the fitted models you want to evaluate. I have made sure only to include models that were fitted on the same parameters to make it easier to compare them.\n\nmodels <- list(\"logistic regression\" = lr_fit3,\n               \"LDA\" = lda_fit,\n               \"QDA\" = qda_fit,\n               \"KNN\" = knn_fit)\n\nNext use imap_dfr() from the purrr package to apply augment() to each of the models using the testing data set. .id = \"model\" creates a column named \"model\" that is added to the resulting tibble using the names of models.\n\npreds <- imap_dfr(models, augment, \n                  new_data = Smarket_test, .id = \"model\")\n\npreds %>%\n  select(model, Direction, .pred_class, .pred_Down, .pred_Up)\n\n# A tibble: 1,008 × 5\n   model               Direction .pred_class .pred_Down .pred_Up\n   <chr>               <fct>     <fct>            <dbl>    <dbl>\n 1 logistic regression Down      Up               0.490    0.510\n 2 logistic regression Down      Up               0.479    0.521\n 3 logistic regression Down      Up               0.467    0.533\n 4 logistic regression Up        Up               0.474    0.526\n 5 logistic regression Down      Up               0.493    0.507\n 6 logistic regression Up        Up               0.494    0.506\n 7 logistic regression Down      Up               0.495    0.505\n 8 logistic regression Up        Up               0.487    0.513\n 9 logistic regression Down      Up               0.491    0.509\n10 logistic regression Up        Up               0.484    0.516\n# … with 998 more rows\n\n\nWe have seen how to use accuracy() a lot of times by now, but it is not the only metric to use for classification, and yardstick provides many more. You can combine multiple different metrics together with metric_set()\n\nmulti_metric <- metric_set(accuracy, sensitivity, specificity)\n\nand then the resulting function can be applied to calculate multiple metrics at the same time. All of the yardstick works with grouped tibbles so by calling group_by(model) we can calculate the metrics for each of the models in one go.\n\npreds %>%\n  group_by(model) %>%\n  multi_metric(truth = Direction, estimate = .pred_class)\n\n# A tibble: 12 × 4\n   model               .metric     .estimator .estimate\n   <chr>               <chr>       <chr>          <dbl>\n 1 KNN                 accuracy    binary         0.5  \n 2 LDA                 accuracy    binary         0.560\n 3 logistic regression accuracy    binary         0.560\n 4 QDA                 accuracy    binary         0.599\n 5 KNN                 sensitivity binary         0.387\n 6 LDA                 sensitivity binary         0.315\n 7 logistic regression sensitivity binary         0.315\n 8 QDA                 sensitivity binary         0.270\n 9 KNN                 specificity binary         0.589\n10 LDA                 specificity binary         0.752\n11 logistic regression specificity binary         0.752\n12 QDA                 specificity binary         0.858\n\n\nThe same technique can be used to create ROC curves.\n\npreds %>%\n  group_by(model) %>%\n  roc_curve(Direction, .pred_Down) %>%\n  autoplot()\n\n\n\n\nHere you can’t see the LDA because it lies perfectly under the logistic regression."
  },
  {
    "objectID": "05-resampling-methods.html",
    "href": "05-resampling-methods.html",
    "title": "\n5  Resampling Methods\n",
    "section": "",
    "text": "This lab will show us how to perform different resampling techniques. Some of these tasks are quite general and useful in many different areas. The bootstrap being such an example. This chapter introduces a lot of new packages. This chapter will bring rsample into view for creating resampled data frames as well as yardstick to calculate performance metrics. Finally, we will use tune to fit our models within resamples and dials to help with the selection of hyperparameter tuning values."
  },
  {
    "objectID": "05-resampling-methods.html#the-validation-set-approach",
    "href": "05-resampling-methods.html#the-validation-set-approach",
    "title": "\n5  Resampling Methods\n",
    "section": "\n5.1 The Validation Set Approach",
    "text": "5.1 The Validation Set Approach\nWhen fitting a model it is often desired to be able to calculate a performance metric to quantify how well the model fits the data. If a model is evaluated on the data it was fit on you are quite likely to get over-optimistic results. It is therefore we split our data into testing and training. This way we can fit the model to data and evaluate it on some other that that is similar.\nSplitting of the data is done using random sampling, so it is advised to set a seed before splitting to assure we can reproduce the results. The initial_split() function takes a data.frame and returns a rsplit object. This object contains information about which observations belong to which data set, testing, and training. This is where you would normally set a proportion of data that is used for training and how much is used for evaluation. This is set using the prop argument which I set to 0.5 to closely match what happened in ISLR. I’m also setting the strata argument. This argument makes sure that both sides of the split have roughly the same distribution for each value of strata. If a numeric variable is passed to strata then it is binned and distributions are matched within bins.\n\nset.seed(1)\nAuto_split <- initial_split(Auto, strata = mpg, prop = 0.5)\nAuto_split\n\n<Training/Testing/Total>\n<194/198/392>\n\n\nThe testing and training data sets can be materialized using the testing() and training() functions respectively.\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\n\nAnd by looking at Auto_train and Auto_test we see that the lengths match what we expect.\n\nAuto_train\n\n# A tibble: 194 × 9\n     mpg cylinders displacement horsepower weight acceleration  year origin\n   <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>  <dbl>\n 1    15         8          350        165   3693         11.5    70      1\n 2    16         8          304        150   3433         12      70      1\n 3    14         8          440        215   4312          8.5    70      1\n 4    14         8          455        225   4425         10      70      1\n 5    10         8          307        200   4376         15      70      1\n 6    17         6          250        100   3329         15.5    71      1\n 7    14         8          400        175   4464         11.5    71      1\n 8    14         8          351        153   4154         13.5    71      1\n 9    14         8          318        150   4096         13      71      1\n10    13         8          400        170   4746         12      71      1\n# … with 184 more rows, and 1 more variable: name <fct>\n\nAuto_test\n\n# A tibble: 198 × 9\n     mpg cylinders displacement horsepower weight acceleration  year origin\n   <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>  <dbl>\n 1    18         8          318        150   3436         11      70      1\n 2    17         8          302        140   3449         10.5    70      1\n 3    15         8          429        198   4341         10      70      1\n 4    14         8          454        220   4354          9      70      1\n 5    15         8          390        190   3850          8.5    70      1\n 6    15         8          383        170   3563         10      70      1\n 7    14         8          340        160   3609          8      70      1\n 8    15         8          400        150   3761          9.5    70      1\n 9    14         8          455        225   3086         10      70      1\n10    22         6          198         95   2833         15.5    70      1\n# … with 188 more rows, and 1 more variable: name <fct>\n\n\nNow that we have a train-test split let us fit some models and evaluate their performance. Before we move on it is important to reiterate that you should only use the testing data set once! Once you have looked at the performance on the testing data set you should not modify your models. If you do you might overfit the model due to data leakage.\nOur modeling goal is to predict mpg by horsepower using a simple linear regression model, and a polynomial regression model. First, we set up a linear regression specification.\n\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n\nAnd we fit it like normal. Note that we are fitting it using Auto_train.\n\nlm_fit <- lm_spec %>% \n  fit(mpg ~ horsepower, data = Auto_train)\n\nWe can now use the augment() function to extract the prediction and rmse() to calculate the root mean squared error. This will be the testing RMSE since we are evaluating on Auto_test.\n\naugment(lm_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        5.06\n\n\nand we get a RMSE of 5.0583165. This particular value is going to vary depending on what seed number you picked since the random sampling used in splitting the data set will be slightly different.\nUsing this framework makes it easy for us to calculate the training RMSE\n\naugment(lm_fit, new_data = Auto_train) %>%\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        4.74\n\n\nComparing these two values can give us a look into how generalizable the model is to data it hasn’t seen before. We do expect that the training RMSE to be lower than the testing RMSE but if you see a large difference there is an indication of overfitting or a shift between the training data set and testing data set. We don’t expect a shift here since the data sets were created with random sampling.\nNext we will fit a polynomial regression model. We can use the linear model specification lm_spec to add a preprocessing unit with recipe() and step_poly() to create the polynomial expansion of horsepower. we can combine these two with workflow() to create a workflow object.\n\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = 2)\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\npoly_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nWe can now fit this model. Again remember to fit it on the training data set Auto_train.\n\npoly_fit <- fit(poly_wf, data = Auto_train)\n\nThe testing RMSE is then calculated as\n\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        4.37\n\n\nWhich is a little bit lower. So it would appear just from this, that the polynomial regression model has a better fit. Note that we are making decisions using the testing performance metrics, not the training performance metrics.\nLastly, we show below how changing the seed results in a slightly different estimate.\n\nset.seed(2)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\n\npoly_fit <- fit(poly_wf, data = Auto_train)\n\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        4.35"
  },
  {
    "objectID": "05-resampling-methods.html#leave-one-out-cross-validation",
    "href": "05-resampling-methods.html#leave-one-out-cross-validation",
    "title": "\n5  Resampling Methods\n",
    "section": "\n5.2 Leave-One-Out Cross-Validation",
    "text": "5.2 Leave-One-Out Cross-Validation\nLeave-One-Out Cross-Validation is not integrated into the broader tidymodels framework. For more information read here."
  },
  {
    "objectID": "05-resampling-methods.html#k-fold-cross-validation",
    "href": "05-resampling-methods.html#k-fold-cross-validation",
    "title": "\n5  Resampling Methods\n",
    "section": "\n5.3 k-Fold Cross-Validation",
    "text": "5.3 k-Fold Cross-Validation\nEarlier we set degree = 2 to create a second-degree polynomial regression model. But suppose we want to find the best value of degree that yields the “closest” fit. This is known as hyperparameter tuning and it is a case where we can use k-Fold Cross-Validation. To use k-Fold Cross-Validation we will be using the tune package, and we need 3 things to get it working:\n\nA parsnip/workflow object with one or more arguments marked for tuning,\nA vfold_cv rsample object of the cross-validation resamples,\nA tibble denoting the values of hyperparameter values to be explored.\n\nwe are doing the hyperparameter tuning on just one parameter, namely the degree argument in step_poly(). Creating a new recipe with degree = tune() indicated that we intend for degree to be tuned.\n\npoly_tuned_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = tune())\n\npoly_tuned_wf <- workflow() %>%\n  add_recipe(poly_tuned_rec) %>%\n  add_model(lm_spec)\n\nThis means that would not be able to fit this workflow right now as the value of degree is unspecified, and if we try we get an error:\n\nfit(poly_tuned_wf, data = Auto_train)\n\nError in `recipes::prep()` at ]8;line = 216:col = 2;file:///Users/emilhvitfeldt/Github/hardhat/R/blueprint-recipe-default.Rhardhat/R/blueprint-recipe-default.R:216:2]8;;:\n! You cannot `prep()` a tuneable recipe. Argument(s) with `tune()`: 'degree'. Do you want to use a tuning function such as `tune_grid()`?\n\n\nThe next thing we need to create is the k-Fold data set. This can be done using the vfold_cv() function. Note that the function uses v instead of k which is the terminology of ISLR. we set v = 10 as a common choice for k.\n\nAuto_folds <- vfold_cv(Auto_train, v = 10)\nAuto_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [264/30]> Fold01\n 2 <split [264/30]> Fold02\n 3 <split [264/30]> Fold03\n 4 <split [264/30]> Fold04\n 5 <split [265/29]> Fold05\n 6 <split [265/29]> Fold06\n 7 <split [265/29]> Fold07\n 8 <split [265/29]> Fold08\n 9 <split [265/29]> Fold09\n10 <split [265/29]> Fold10\n\n\nThe result is a tibble of vfold_splits which is quite similar to the rsplit object we saw earlier.\nThe last thing we need is a tibble of possible values we want to explore. Each of the tunable parameters in tidymodels has an associated function in the dials package. We need to use the degree() function here, and we extend the range to have a max of 10. This dials function is then passed to grid_regular() to create a regular grid of values.\n\ndegree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)\n\nUsing grid_regular() is a little overkill for this application since the following code would provide the same result. But once you have multiple parameters you want to tune it makes sure that everything is in check and properly named.\n\ndegree_grid <- tibble(degree = seq(1, 10))\n\nNow that all the necessary objects have been created we can pass them to tune_grid() which will fit the models within each fold for each value specified in degree_grid.\n\ntune_res <- tune_grid(\n  object = poly_tuned_wf, \n  resamples = Auto_folds, \n  grid = degree_grid\n)\n\nIt can be helpful to add control = control_grid(verbose = TRUE), this will print out the progress. Especially helpful when the models take a while to fit. tune_res by itself isn’t easily readable. Luckily tune provides a handful of helper functions.\nautoplot() gives a visual overview of the performance of different hyperparameter pairs.\n\nautoplot(tune_res)\n\n\n\n\nIt appears that the biggest jump in performance comes from going to degree = 2. Afterward, there might be a little bit of improvement but it isn’t as obvious.\nThe number used for plotting can be extracted directly with collect_metrics(). We also get an estimate of the standard error of the performance metric. We get this since we have 10 different estimates, one for each fold.\n\ncollect_metrics(tune_res)\n\n# A tibble: 20 × 7\n   degree .metric .estimator  mean     n std_err .config              \n    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1      1 rmse    standard   4.81     10  0.172  Preprocessor01_Model1\n 2      1 rsq     standard   0.621    10  0.0316 Preprocessor01_Model1\n 3      2 rmse    standard   4.37     10  0.209  Preprocessor02_Model1\n 4      2 rsq     standard   0.677    10  0.0436 Preprocessor02_Model1\n 5      3 rmse    standard   4.40     10  0.217  Preprocessor03_Model1\n 6      3 rsq     standard   0.675    10  0.0446 Preprocessor03_Model1\n 7      4 rmse    standard   4.43     10  0.218  Preprocessor04_Model1\n 8      4 rsq     standard   0.670    10  0.0453 Preprocessor04_Model1\n 9      5 rmse    standard   4.42     10  0.203  Preprocessor05_Model1\n10      5 rsq     standard   0.674    10  0.0436 Preprocessor05_Model1\n11      6 rmse    standard   4.41     10  0.189  Preprocessor06_Model1\n12      6 rsq     standard   0.670    10  0.0423 Preprocessor06_Model1\n13      7 rmse    standard   4.40     10  0.176  Preprocessor07_Model1\n14      7 rsq     standard   0.670    10  0.0420 Preprocessor07_Model1\n15      8 rmse    standard   4.41     10  0.175  Preprocessor08_Model1\n16      8 rsq     standard   0.670    10  0.0420 Preprocessor08_Model1\n17      9 rmse    standard   4.47     10  0.207  Preprocessor09_Model1\n18      9 rsq     standard   0.663    10  0.0445 Preprocessor09_Model1\n19     10 rmse    standard   4.50     10  0.227  Preprocessor10_Model1\n20     10 rsq     standard   0.658    10  0.0465 Preprocessor10_Model1\n\n\nYou can also use show_best() to only show the best performing models.\n\nshow_best(tune_res, metric = \"rmse\")\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1      2 rmse    standard    4.37    10   0.209 Preprocessor02_Model1\n2      3 rmse    standard    4.40    10   0.217 Preprocessor03_Model1\n3      7 rmse    standard    4.40    10   0.176 Preprocessor07_Model1\n4      6 rmse    standard    4.41    10   0.189 Preprocessor06_Model1\n5      8 rmse    standard    4.41    10   0.175 Preprocessor08_Model1\n\n\nWe did see that the performance plateaued after degree = 2. There are a couple of function to select models by more sophisticated rules. select_by_one_std_err() and select_by_pct_loss(). Here we use select_by_one_std_err() which selects the most simple model that is within one standard error of the numerically optimal results. We need to specify degree to tell select_by_one_std_err() which direction is more simple.\nYou want to\n\nuse desc(you_model_parameter) if larger values lead to a simpler model\nuse you_model_parameter if smaller values lead to a simpler model\n\nlower polynomials models are simpler so we ditch desc().\n\nselect_by_one_std_err(tune_res, degree, metric = \"rmse\")\n\n# A tibble: 1 × 9\n  degree .metric .estimator  mean     n std_err .config             .best .bound\n   <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               <dbl>  <dbl>\n1      2 rmse    standard    4.37    10   0.209 Preprocessor02_Mod…  4.37   4.58\n\n\nThis selected degree = 2. And we will use this value since we simpler models sometimes can be very beneficial. Especially if we want to explain what happens in it.\n\nbest_degree <- select_by_one_std_err(tune_res, degree, metric = \"rmse\")\n\nThis selected value can be now be used to specify the previous unspecified degree argument in poly_wf using finalize_workflow().\n\nfinal_wf <- finalize_workflow(poly_wf, best_degree)\n\nfinal_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThis workflow can now be fitted. And we want to make sure we fit it on the full training data set.\n\nfinal_fit <- fit(final_wf, Auto_train)\n\nfinal_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n      (Intercept)  horsepower_poly_1  horsepower_poly_2  \n            23.34            -104.85              34.39"
  },
  {
    "objectID": "05-resampling-methods.html#the-bootstrap",
    "href": "05-resampling-methods.html#the-bootstrap",
    "title": "\n5  Resampling Methods\n",
    "section": "\n5.4 The Bootstrap",
    "text": "5.4 The Bootstrap\nThis section illustrates the use of the bootstrap in the simple Section 5.2 of ISLR, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\nFirst, we want to look at the accuracy of a statistic of interest. This statistic is justified in ISLR. We want to calculate the metric within many different bootstraps. We start by calculating 1000 bootstraps of the Portfolio data set.\n\nPortfolio_boots <- bootstraps(Portfolio, times = 1000)\nPortfolio_boots\n\n# Bootstrap sampling \n# A tibble: 1,000 × 2\n   splits           id           \n   <list>           <chr>        \n 1 <split [100/36]> Bootstrap0001\n 2 <split [100/39]> Bootstrap0002\n 3 <split [100/39]> Bootstrap0003\n 4 <split [100/33]> Bootstrap0004\n 5 <split [100/39]> Bootstrap0005\n 6 <split [100/34]> Bootstrap0006\n 7 <split [100/40]> Bootstrap0007\n 8 <split [100/38]> Bootstrap0008\n 9 <split [100/36]> Bootstrap0009\n10 <split [100/41]> Bootstrap0010\n# … with 990 more rows\n\n\nThe result is a tibble of boot_split objects. The rsample has constructed these splits in such a way that these 1000 bootstraps take up way less than 1000 times the space as Portfolio.\nNext, we create a function that takes a boot_split object and returns the calculated metric.\n\nalpha.fn <- function(split) {\n  data <- analysis(split)\n  X <- data$X\n  Y <- data$Y\n  \n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nNow we can use mutate() and map_dbl() from dplyr and purrr respectively to apply alpha.fn to each of the bootstraps.\n\nalpha_res <- Portfolio_boots %>%\n  mutate(alpha = map_dbl(splits, alpha.fn))\n\nalpha_res\n\n# Bootstrap sampling \n# A tibble: 1,000 × 3\n   splits           id            alpha\n   <list>           <chr>         <dbl>\n 1 <split [100/36]> Bootstrap0001 0.516\n 2 <split [100/39]> Bootstrap0002 0.687\n 3 <split [100/39]> Bootstrap0003 0.599\n 4 <split [100/33]> Bootstrap0004 0.556\n 5 <split [100/39]> Bootstrap0005 0.549\n 6 <split [100/34]> Bootstrap0006 0.619\n 7 <split [100/40]> Bootstrap0007 0.387\n 8 <split [100/38]> Bootstrap0008 0.675\n 9 <split [100/36]> Bootstrap0009 0.538\n10 <split [100/41]> Bootstrap0010 0.407\n# … with 990 more rows\n\n\nand now we have all the bootstrap sample values. These can now further be analyzed.\nIn the next example do we want to study the variability of the slope and intercept estimate of the linear regression model. And it follows the same structure. First, we create some bootstraps of the data. Then we create a function that takes a split and returns some values. This function will return a tibble for each bootstrap.\n\nAuto_boots <- bootstraps(Auto)\n\nboot.fn <- function(split) {\n  lm_fit <- lm_spec %>% fit(mpg ~ horsepower, data = analysis(split))\n  tidy(lm_fit)\n}\n\nthen we use mutate() and map() to apply the function to each of the bootstraps.\n\nboot_res <- Auto_boots %>%\n  mutate(models = map(splits, boot.fn))\n\nAnd we can now unnest() and use group_by() and summarise() to get an estimate of the variability of the slope and intercept in this linear regression model.\n\nboot_res %>%\n  unnest(cols = c(models)) %>%\n  group_by(term) %>%\n  summarise(mean = mean(estimate),\n            sd = sd(estimate))\n\n# A tibble: 2 × 3\n  term          mean      sd\n  <chr>        <dbl>   <dbl>\n1 (Intercept) 39.8   0.759  \n2 horsepower  -0.156 0.00593"
  },
  {
    "objectID": "06-regularization.html",
    "href": "06-regularization.html",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "",
    "text": "This lab will take a look at regularization models and hyperparameter tuning. These models are related to the models we saw in chapter 3 and 4, with the difference that they contain a regularization term. This chapter will use parsnip for model fitting and recipes and workflows to perform the transformations, and tune and dials to tune the hyperparameters of the model.\nWe will be using the Hitters data set from the ISLR package. We wish to predict the baseball players Salary based on several different characteristics which are included in the data set. Since we wish to predict Salary, then we need to remove any missing data from that column. Otherwise, we won’t be able to run the models."
  },
  {
    "objectID": "06-regularization.html#best-subset-selection",
    "href": "06-regularization.html#best-subset-selection",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.1 Best Subset Selection",
    "text": "6.1 Best Subset Selection\ntidymodels does not currently support subset selection methods, and it unlikely to include it in the near future."
  },
  {
    "objectID": "06-regularization.html#forward-and-backward-stepwise-selection",
    "href": "06-regularization.html#forward-and-backward-stepwise-selection",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.2 Forward and Backward Stepwise Selection",
    "text": "6.2 Forward and Backward Stepwise Selection\ntidymodels does not currently support forward and backward stepwise selection methods, and it unlikely to include it in the near future."
  },
  {
    "objectID": "06-regularization.html#ridge-regression",
    "href": "06-regularization.html#ridge-regression",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.3 Ridge Regression",
    "text": "6.3 Ridge Regression\nWe will use the glmnet package to perform ridge regression. parsnip does not have a dedicated function to create a ridge regression model specification. You need to use linear_reg() and set mixture = 0 to specify a ridge model. The mixture argument specifies the amount of different types of regularization, mixture = 0 specifies only ridge regularization and mixture = 1 specifies only lasso regularization. Setting mixture to a value between 0 and 1 lets us use both. When using the glmnet engine we also need to set a penalty to be able to fit the model. We will set this value to 0 for now, it is not the best value, but we will look at how to select the best value in a little bit.\n\nridge_spec <- linear_reg(mixture = 0, penalty = 0) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\")\n\nOnce the specification is created we can fit it to our data. We will use all the predictors.\n\nridge_fit <- fit(ridge_spec, Salary ~ ., data = Hitters)\n\nThe glmnet package will fit the model for all values of penalty at once, so let us see what the parameter estimate for the model is now that we have penalty = 0.\n\ntidy(ridge_fit)\n\n# A tibble: 20 × 3\n   term          estimate penalty\n   <chr>            <dbl>   <dbl>\n 1 (Intercept)   81.1           0\n 2 AtBat         -0.682         0\n 3 Hits           2.77          0\n 4 HmRun         -1.37          0\n 5 Runs           1.01          0\n 6 RBI            0.713         0\n 7 Walks          3.38          0\n 8 Years         -9.07          0\n 9 CAtBat        -0.00120       0\n10 CHits          0.136         0\n11 CHmRun         0.698         0\n12 CRuns          0.296         0\n13 CRBI           0.257         0\n14 CWalks        -0.279         0\n15 LeagueN       53.2           0\n16 DivisionW   -123.            0\n17 PutOuts        0.264         0\n18 Assists        0.170         0\n19 Errors        -3.69          0\n20 NewLeagueN   -18.1           0\n\n\nLet us instead see what the estimates would be if the penalty was 11498.\n\ntidy(ridge_fit, penalty = 11498)\n\n# A tibble: 20 × 3\n   term         estimate penalty\n   <chr>           <dbl>   <dbl>\n 1 (Intercept) 407.        11498\n 2 AtBat         0.0370    11498\n 3 Hits          0.138     11498\n 4 HmRun         0.525     11498\n 5 Runs          0.231     11498\n 6 RBI           0.240     11498\n 7 Walks         0.290     11498\n 8 Years         1.11      11498\n 9 CAtBat        0.00314   11498\n10 CHits         0.0117    11498\n11 CHmRun        0.0876    11498\n12 CRuns         0.0234    11498\n13 CRBI          0.0242    11498\n14 CWalks        0.0250    11498\n15 LeagueN       0.0866    11498\n16 DivisionW    -6.23      11498\n17 PutOuts       0.0165    11498\n18 Assists       0.00262   11498\n19 Errors       -0.0206    11498\n20 NewLeagueN    0.303     11498\n\n\nNotice how the estimates are decreasing when the amount of penalty goes up. Look below at the parameter estimates for penalty = 705 and penalty = 50.\n\ntidy(ridge_fit, penalty = 705)\n\n# A tibble: 20 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)  54.4        705\n 2 AtBat         0.112      705\n 3 Hits          0.656      705\n 4 HmRun         1.18       705\n 5 Runs          0.937      705\n 6 RBI           0.847      705\n 7 Walks         1.32       705\n 8 Years         2.58       705\n 9 CAtBat        0.0108     705\n10 CHits         0.0468     705\n11 CHmRun        0.338      705\n12 CRuns         0.0937     705\n13 CRBI          0.0979     705\n14 CWalks        0.0718     705\n15 LeagueN      13.7        705\n16 DivisionW   -54.7        705\n17 PutOuts       0.119      705\n18 Assists       0.0161     705\n19 Errors       -0.704      705\n20 NewLeagueN    8.61       705\n\ntidy(ridge_fit, penalty = 50)\n\n# A tibble: 20 × 3\n   term          estimate penalty\n   <chr>            <dbl>   <dbl>\n 1 (Intercept)   48.2          50\n 2 AtBat         -0.354        50\n 3 Hits           1.95         50\n 4 HmRun         -1.29         50\n 5 Runs           1.16         50\n 6 RBI            0.809        50\n 7 Walks          2.71         50\n 8 Years         -6.20         50\n 9 CAtBat         0.00609      50\n10 CHits          0.107        50\n11 CHmRun         0.629        50\n12 CRuns          0.217        50\n13 CRBI           0.215        50\n14 CWalks        -0.149        50\n15 LeagueN       45.9          50\n16 DivisionW   -118.           50\n17 PutOuts        0.250        50\n18 Assists        0.121        50\n19 Errors        -3.28         50\n20 NewLeagueN    -9.42         50\n\n\nWe can visualize how the magnitude of the coefficients are being regularized towards zero as the penalty goes up.\n\nridge_fit %>%\n  autoplot()\n\n\n\n\nPrediction is done like normal, if we use predict() by itself, then penalty = 0 as we set in the model specification is used.\n\npredict(ridge_fit, new_data = Hitters)\n\n# A tibble: 263 × 1\n    .pred\n    <dbl>\n 1  442. \n 2  676. \n 3 1059. \n 4  521. \n 5  543. \n 6  218. \n 7   74.7\n 8   96.1\n 9  809. \n10  865. \n# … with 253 more rows\n\n\nbut we can also get predictions for other values of penalty by specifying it in predict()\n\npredict(ridge_fit, new_data = Hitters, penalty = 500)\n\n# A tibble: 263 × 1\n   .pred\n   <dbl>\n 1  525.\n 2  620.\n 3  895.\n 4  425.\n 5  589.\n 6  179.\n 7  147.\n 8  187.\n 9  841.\n10  840.\n# … with 253 more rows\n\n\nWe saw how we can fit a ridge model and make predictions for different values of penalty. But it would be nice if we could find the “best” value of the penalty. This is something we can use hyperparameter tuning for. Hyperparameter tuning is in its simplest form a way of fitting many models with different sets of hyperparameters trying to find one that performs “best”. The complexity in hyperparameter tuning can come from how you try different models. We will keep it simple for this lab and only look at grid search, only looking at evenly spaced parameter values. This is a fine enough approach if you have one or two tunable parameters but can become computationally infeasible. See the chapter on iterative search from Tidy Modeling with R for more information.\nWe start like normal by setting up a validation split. A K-fold cross-validation data set is created on the training data set with 10 folds.\n\nHitters_split <- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train <- training(Hitters_split)\nHitters_test <- testing(Hitters_split)\n\nHitters_fold <- vfold_cv(Hitters_train, v = 10)\n\nWe can use the tune_grid() function to perform hyperparameter tuning using a grid search. tune_grid() needs 3 different thing;\n\na workflow object containing the model and preprocessor,\na rset object containing the resamples the workflow should be fitted within, and\na tibble containing the parameter values to be evaluated.\n\nOptionally a metric set of performance metrics can be supplied for evaluation. If you don’t set one then a default set of performance metrics is used.\nWe already have a resample object created in Hitters_fold. Now we should create the workflow specification next.\nWe just used the data set as is when we fit the model earlier. But ridge regression is scale sensitive so we need to make sure that the variables are on the same scale. We can use step_normalize(). Secondly let us deal with the factor variables ourself using step_novel() and step_dummy().\n\nridge_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n\nThe model specification will look very similar to what we have seen earlier, but we will set penalty = tune(). This tells tune_grid() that the penalty parameter should be tuned.\n\nridge_spec <- \n  linear_reg(penalty = tune(), mixture = 0) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\")\n\nNow we combine to create a workflow object.\n\nridge_workflow <- workflow() %>% \n  add_recipe(ridge_recipe) %>% \n  add_model(ridge_spec)\n\nThe last thing we need is the values of penalty we are trying. This can be created using grid_regular() which creates a grid of evenly spaces parameter values. We use the penalty() function from the dials package to denote the parameter and set the range of the grid we are searching for. Note that this range is log-scaled.\n\npenalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\npenalty_grid\n\n# A tibble: 50 × 1\n     penalty\n       <dbl>\n 1 0.00001  \n 2 0.0000160\n 3 0.0000256\n 4 0.0000409\n 5 0.0000655\n 6 0.000105 \n 7 0.000168 \n 8 0.000268 \n 9 0.000429 \n10 0.000687 \n# … with 40 more rows\n\n\nUsing 50 levels for one parameter might seem overkill and in many applications it is. But remember that glmnet fits all the models in one go so adding more levels to penalty doesn’t affect the computational speed much.\nNow we have everything we need and we can fit all the models.\n\ntune_res <- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\ntune_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   <list>           <chr>  <list>             <list>          \n 1 <split [176/20]> Fold01 <tibble [100 × 5]> <tibble [0 × 3]>\n 2 <split [176/20]> Fold02 <tibble [100 × 5]> <tibble [0 × 3]>\n 3 <split [176/20]> Fold03 <tibble [100 × 5]> <tibble [0 × 3]>\n 4 <split [176/20]> Fold04 <tibble [100 × 5]> <tibble [0 × 3]>\n 5 <split [176/20]> Fold05 <tibble [100 × 5]> <tibble [0 × 3]>\n 6 <split [176/20]> Fold06 <tibble [100 × 5]> <tibble [0 × 3]>\n 7 <split [177/19]> Fold07 <tibble [100 × 5]> <tibble [0 × 3]>\n 8 <split [177/19]> Fold08 <tibble [100 × 5]> <tibble [0 × 3]>\n 9 <split [177/19]> Fold09 <tibble [100 × 5]> <tibble [0 × 3]>\n10 <split [177/19]> Fold10 <tibble [100 × 5]> <tibble [0 × 3]>\n\n\nThe output of tune_grid() can be hard to read by itself unprocessed. autoplot() creates a great visualization\n\nautoplot(tune_res)\n\n\n\n\nHere we see that the amount of regularization affects the performance metrics differently. Note how there are areas where the amount of regularization doesn’t have any meaningful influence on the coefficient estimates. We can also see the raw metrics that created this chart by calling collect_matrics().\n\ncollect_metrics(tune_res)\n\n# A tibble: 100 × 7\n     penalty .metric .estimator    mean     n std_err .config              \n       <dbl> <chr>   <chr>        <dbl> <int>   <dbl> <chr>                \n 1 0.00001   rmse    standard   342.       10 35.7    Preprocessor1_Model01\n 2 0.00001   rsq     standard     0.455    10  0.0500 Preprocessor1_Model01\n 3 0.0000160 rmse    standard   342.       10 35.7    Preprocessor1_Model02\n 4 0.0000160 rsq     standard     0.455    10  0.0500 Preprocessor1_Model02\n 5 0.0000256 rmse    standard   342.       10 35.7    Preprocessor1_Model03\n 6 0.0000256 rsq     standard     0.455    10  0.0500 Preprocessor1_Model03\n 7 0.0000409 rmse    standard   342.       10 35.7    Preprocessor1_Model04\n 8 0.0000409 rsq     standard     0.455    10  0.0500 Preprocessor1_Model04\n 9 0.0000655 rmse    standard   342.       10 35.7    Preprocessor1_Model05\n10 0.0000655 rsq     standard     0.455    10  0.0500 Preprocessor1_Model05\n# … with 90 more rows\n\n\nThe “best” values of this can be selected using select_best(), this function requires you to specify a matric that it should select against.\n\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00001 Preprocessor1_Model01\n\n\nThis value of penalty can then be used with finalize_workflow() to update/finalize the recipe by replacing tune() with the value of best_penalty. Now, this model should be fit again, this time using the whole training data set.\n\nridge_final <- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit <- fit(ridge_final, data = Hitters_train)\n\nThis final model can now be applied on our testing data set to validate the performance\n\naugment(ridge_final_fit, new_data = Hitters_test) %>%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.438\n\n\nAnd it performs fairly well given what we saw earlier."
  },
  {
    "objectID": "06-regularization.html#the-lasso",
    "href": "06-regularization.html#the-lasso",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.4 The Lasso",
    "text": "6.4 The Lasso\nWe will use the glmnet package to perform lasso regression. parsnip does not have a dedicated function to create a ridge regression model specification. You need to use linear_reg() and set mixture = 1 to specify a lasso model. The mixture argument specifies the amount of different types of regularization, mixture = 0 specifies only ridge regularization and mixture = 1 specifies only lasso regularization. Setting mixture to a value between 0 and 1 lets us use both.\nThe following procedure will be very similar to what we saw in the ridge regression section. The preprocessing needed is the same, but let us write it out one more time.\n\nlasso_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n\nNext, we finish the lasso regression workflow.\n\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nlasso_workflow <- workflow() %>% \n  add_recipe(lasso_recipe) %>% \n  add_model(lasso_spec)\n\nWhile we are doing a different kind of regularization we still use the same penalty argument. I have picked a different range for the values of penalty since I know it will be a good range. You would in practice have to cast a wide net at first and then narrow on the range of interest.\n\npenalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)\n\nAnd we can use tune_grid() again.\n\ntune_res <- tune_grid(\n  lasso_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nautoplot(tune_res)\n\n\n\n\nWe select the best value of penalty using select_best()\n\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\n\nAnd refit the using the whole training data set.\n\nlasso_final <- finalize_workflow(lasso_workflow, best_penalty)\n\nlasso_final_fit <- fit(lasso_final, data = Hitters_train)\n\nAnd we are done, by calculating the rsq value for the lasso model can we see that for this data ridge regression outperform lasso regression.\n\naugment(lasso_final_fit, new_data = Hitters_test) %>%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.519"
  },
  {
    "objectID": "06-regularization.html#principal-components-regression",
    "href": "06-regularization.html#principal-components-regression",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.5 Principal Components Regression",
    "text": "6.5 Principal Components Regression\nWe will talk more about principal components analysis in chapter 10. This section will show how principal components can be used as a dimensionality reduction preprocessing step.\nI will treat principal component regression as a linear model with PCA transformations in the preprocessing. But using the tidymodels framework then this is still mostly one model.\n\nlm_spec <- \n  linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\nThe preprocessing recipe will closely resemble the recipe we saw in the ridge and lasso sections. The main difference is that we end the recipe with step_pca() which will perform principal component analysis on all the predictors, and return the components that explain threshold percent of the variance. We have set threshold = tune() so we can treat the threshold as a hyperparameter to be tuned. By using workflows and tune together can be tune parameters in the preprocessing as well as parameters in the models.\n\npca_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), threshold = tune())\n\npca_workflow <- \n  workflow() %>% \n  add_recipe(pca_recipe) %>% \n  add_model(lm_spec)\n\nWe create a smaller grid for threshold and we don’t need to modify the range since [0, 1] is an acceptable range.\n\nthreshold_grid <- grid_regular(threshold(), levels = 10)\nthreshold_grid\n\n# A tibble: 10 × 1\n   threshold\n       <dbl>\n 1     0    \n 2     0.111\n 3     0.222\n 4     0.333\n 5     0.444\n 6     0.556\n 7     0.667\n 8     0.778\n 9     0.889\n10     1    \n\n\nAnd now we fit using tune_grid(). This time we will actually perform 100 fits since we need to fit a model for each value of threshold within each fold.\n\ntune_res <- tune_grid(\n  pca_workflow,\n  resamples = Hitters_fold, \n  grid = threshold_grid\n)\n\nThe results look a little shaky here.\n\nautoplot(tune_res)\n\n\n\n\nBut we can still select the best model.\n\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\n\nAnd fit the model much like have done a couple of times by now. The workflow is finalized using the value we selected with select_best(), and training using the full training data set.\n\npca_final <- finalize_workflow(pca_workflow, best_threshold)\n\npca_final_fit <- fit(pca_final, data = Hitters_train)"
  },
  {
    "objectID": "06-regularization.html#partial-least-squares",
    "href": "06-regularization.html#partial-least-squares",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.6 Partial Least Squares",
    "text": "6.6 Partial Least Squares\nLastly, we have a partial least squares model. We will treat this much like the PCA section and say that partial least squares calculations will be done as a preprocessing that we tune. The following code is almost identical to previous chapters and will be shown in full without many explanations to avoid repetition. If you skipped to this section, go back and read the previous sections for more commentary.\n\npls_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pls(all_predictors(), num_comp = tune(), outcome = \"Salary\")\n\nlm_spec <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") \n\npls_workflow <- workflow() %>% \n  add_recipe(pls_recipe) %>% \n  add_model(lm_spec) \n\nnum_comp_grid <- grid_regular(num_comp(c(1, 20)), levels = 10)\n\ntune_res <- tune_grid(\n  pls_workflow,\n  resamples = Hitters_fold, \n  grid = num_comp_grid\n)\n\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\n\npls_final <- finalize_workflow(pls_workflow, best_threshold)\n\npls_final_fit <- fit(pls_final, data = Hitters_train)"
  },
  {
    "objectID": "07-moving-beyond-linearity.html",
    "href": "07-moving-beyond-linearity.html",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "",
    "text": "This lab will look at the various ways we can introduce non-linearity into our model by doing preprocessing. Methods include: polynomials expansion, step functions, and splines.\nThe GAMs section is WIP since they are now supported in parsnip.\nThis chapter will use parsnip for model fitting and recipes and workflows to perform the transformations."
  },
  {
    "objectID": "07-moving-beyond-linearity.html#polynomial-regression-and-step-functions",
    "href": "07-moving-beyond-linearity.html#polynomial-regression-and-step-functions",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.1 Polynomial Regression and Step Functions",
    "text": "7.1 Polynomial Regression and Step Functions\nPolynomial regression can be thought of as doing polynomial expansion on a variable and passing that expansion into a linear regression model. We will be very explicit in this formulation in this chapter. step_poly() allows us to do a polynomial expansion on one or more variables.\nThe following step will take age and replace it with the variables age, age^2, age^3, and age^4 since we set degree = 4.\n\nrec_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\nThis recipe is combined with a linear regression specification and combined to create a workflow object.\n\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n\npoly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_poly)\n\nThis object can now be fit()\n\npoly_fit <- fit(poly_wf, data = Wage)\npoly_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)   age_poly_1   age_poly_2   age_poly_3   age_poly_4  \n     111.70       447.07      -478.32       125.52       -77.91  \n\n\nAnd we cal pull the coefficients using tidy()\n\ntidy(poly_fit)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    112.      0.729    153.   0       \n2 age_poly_1     447.     39.9       11.2  1.48e-28\n3 age_poly_2    -478.     39.9      -12.0  2.36e-32\n4 age_poly_3     126.     39.9        3.14 1.68e- 3\n5 age_poly_4     -77.9    39.9       -1.95 5.10e- 2\n\n\nI was lying when I said that step_poly() returned age, age^2, age^3, and age^4. What is happening is that it returns variables that are a basis of orthogonal polynomials, which means that each of the columns is a linear combination of the variables age, age^2, age^3, and age^4. We can see this by using poly() directly with raw = FALSE since it is the default\n\npoly(1:6, degree = 4, raw = FALSE)\n\n              1          2          3          4\n[1,] -0.5976143  0.5455447 -0.3726780  0.1889822\n[2,] -0.3585686 -0.1091089  0.5217492 -0.5669467\n[3,] -0.1195229 -0.4364358  0.2981424  0.3779645\n[4,]  0.1195229 -0.4364358 -0.2981424  0.3779645\n[5,]  0.3585686 -0.1091089 -0.5217492 -0.5669467\n[6,]  0.5976143  0.5455447  0.3726780  0.1889822\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 3.5 3.5 3.5 3.5\n\nattr(,\"coefs\")$norm2\n[1]  1.00000  6.00000 17.50000 37.33333 64.80000 82.28571\n\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nWe see that these variables don’t directly have a format we would have assumed. But this is still a well-reasoned transformation. We can get the raw polynomial transformation by setting raw = TRUE\n\npoly(1:6, degree = 4, raw = TRUE)\n\n     1  2   3    4\n[1,] 1  1   1    1\n[2,] 2  4   8   16\n[3,] 3  9  27   81\n[4,] 4 16  64  256\n[5,] 5 25 125  625\n[6,] 6 36 216 1296\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nThese transformations align with what we would expect. It is still recommended to stick with the default of raw = FALSE unless you have a reason not to do that. One of the benefits of using raw = FALSE is that the resulting variables are uncorrelated which is a desirable quality when using a linear regression model.\nYou can get the raw polynomials by setting options = list(raw = TRUE) in step_poly()\n\nrec_raw_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4, options = list(raw = TRUE))\n\nraw_poly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_raw_poly)\n\nraw_poly_fit <- fit(raw_poly_wf, data = Wage)\n\ntidy(raw_poly_fit)\n\n# A tibble: 5 × 5\n  term            estimate  std.error statistic  p.value\n  <chr>              <dbl>      <dbl>     <dbl>    <dbl>\n1 (Intercept) -184.        60.0           -3.07 0.00218 \n2 age_poly_1    21.2        5.89           3.61 0.000312\n3 age_poly_2    -0.564      0.206         -2.74 0.00626 \n4 age_poly_3     0.00681    0.00307        2.22 0.0264  \n5 age_poly_4    -0.0000320  0.0000164     -1.95 0.0510  \n\n\nLet us try something new and visualize the polynomial fit on our data. We can do this easily because we only have 1 predictor and 1 response. Starting with creating a tibble with different ranges of age. Then we take this tibble and predict with it, this will give us the repression curve. We are additionally adding confidence intervals by setting type = \"conf_int\" which we can do since we are using a linear regression model.\n\nage_range <- tibble(age = seq(min(Wage$age), max(Wage$age)))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = age_range),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\nregression_lines\n\n# A tibble: 63 × 4\n     age .pred .pred_lower .pred_upper\n   <int> <dbl>       <dbl>       <dbl>\n 1    18  51.9        41.5        62.3\n 2    19  58.5        49.9        67.1\n 3    20  64.6        57.5        71.6\n 4    21  70.2        64.4        76.0\n 5    22  75.4        70.5        80.2\n 6    23  80.1        76.0        84.2\n 7    24  84.5        80.9        88.1\n 8    25  88.5        85.2        91.7\n 9    26  92.1        89.1        95.2\n10    27  95.4        92.5        98.4\n# … with 53 more rows\n\n\nWe will then use ggplot2 to visualize the fitted line and confidence interval. The green line is the regression curve and the dashed blue lines are the confidence interval.\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n\n\n\n\nThe regression curve is now a curve instead of a line as we would have gotten with a simple linear regression model. Notice furthermore that the confidence bands are tighter when there is a lot of data and they wider towards the ends of the data.\nLet us take that one step further and see what happens to the regression line once we go past the domain it was trained on. the previous plot showed individuals within the age range 18-80. Let us see what happens once we push this to 18-100. This is not an impossible range but an unrealistic range.\n\nwide_age_range <- tibble(age = seq(18, 100))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = wide_age_range),\n  predict(poly_fit, new_data = wide_age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n\n\n\n\nAnd we see that the curve starts diverging once we get to 93 the predicted wage is negative. The confidence bands also get wider and wider as we get farther away from the data.\nWe can also think of this problem as a classification problem, and we will do that just now by setting us the task of predicting whether an individual earns more than $250000 per year. We will add a new factor value denoting this response.\n\nWage <- Wage %>%\n  mutate(high = factor(wage > 250, \n                       levels = c(TRUE, FALSE), \n                       labels = c(\"High\", \"Low\")))\n\nWe cannot use the polynomial expansion recipe rec_poly we created earlier since it had wage as the response and now we want to have high as the response. We also have to create a logistic regression specification that we will use as our classification model.\n\nrec_poly <- recipe(high ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nlr_poly_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_poly)\n\nThis polynomial logistic regression model workflow can now be fit and predicted with as usual.\n\nlr_poly_fit <- fit(lr_poly_wf, data = Wage)\n\npredict(lr_poly_fit, new_data = Wage)\n\n# A tibble: 3,000 × 1\n   .pred_class\n   <fct>      \n 1 Low        \n 2 Low        \n 3 Low        \n 4 Low        \n 5 Low        \n 6 Low        \n 7 Low        \n 8 Low        \n 9 Low        \n10 Low        \n# … with 2,990 more rows\n\n\nIf we want we can also get back the underlying probability predictions for the two classes, and their confidence intervals for these probability predictions by setting type = \"prob\" and type = \"conf_int\".\n\npredict(lr_poly_fit, new_data = Wage, type = \"prob\")\n\n# A tibble: 3,000 × 2\n      .pred_High .pred_Low\n           <dbl>     <dbl>\n 1 0.00000000983     1.00 \n 2 0.000120          1.00 \n 3 0.0307            0.969\n 4 0.0320            0.968\n 5 0.0305            0.970\n 6 0.0352            0.965\n 7 0.0313            0.969\n 8 0.00820           0.992\n 9 0.0334            0.967\n10 0.0323            0.968\n# … with 2,990 more rows\n\npredict(lr_poly_fit, new_data = Wage, type = \"conf_int\")\n\n# A tibble: 3,000 × 4\n   .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n              <dbl>            <dbl>           <dbl>           <dbl>\n 1         2.22e-16          0.00166           0.998           1    \n 2         1.82e- 6          0.00786           0.992           1.00 \n 3         2.19e- 2          0.0428            0.957           0.978\n 4         2.31e- 2          0.0442            0.956           0.977\n 5         2.13e- 2          0.0434            0.957           0.979\n 6         2.45e- 2          0.0503            0.950           0.975\n 7         2.25e- 2          0.0434            0.957           0.977\n 8         3.01e- 3          0.0222            0.978           0.997\n 9         2.39e- 2          0.0465            0.953           0.976\n10         2.26e- 2          0.0458            0.954           0.977\n# … with 2,990 more rows\n\n\nWe can use these to visualize the probability curve for the classification model.\n\nregression_lines <- bind_cols(\n  augment(lr_poly_fit, new_data = age_range, type = \"prob\"),\n  predict(lr_poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nregression_lines %>%\n  ggplot(aes(age)) +\n  ylim(c(0, 0.2)) +\n  geom_line(aes(y = .pred_High), color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower_High), color = \"blue\", linetype = \"dashed\") +\n  geom_line(aes(y = .pred_upper_High), color = \"blue\", linetype = \"dashed\") +\n  geom_jitter(aes(y = (high == \"High\") / 5), data = Wage, \n              shape = \"|\", height = 0, width = 0.2)\n\n\n\n\nNext, let us take a look at the step function and how to fit a model using it as a preprocessor. You can create step functions in a couple of different ways. step_discretize() will convert a numeric variable into a factor variable with n bins, n here is specified with num_breaks. These will have approximately the same number of points in them according to the training data set.\n\nrec_discretize <- recipe(high ~ age, data = Wage) %>%\n  step_discretize(age, num_breaks = 4)\n\ndiscretize_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_discretize)\n\ndiscretize_fit <- fit(discretize_wf, data = Wage)\ndiscretize_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_discretize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      agebin2      agebin3      agebin4  \n      5.004       -1.492       -1.665       -1.705  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:      730.5 \nResidual Deviance: 710.4    AIC: 718.4\n\n\nIf you already know where you want the step function to break then you can use step_cut() and supply the breaks manually.\n\nrec_cut <- recipe(high ~ age, data = Wage) %>%\n  step_cut(age, breaks = c(30, 50, 70))\n\ncut_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_cut)\n\ncut_fit <- fit(cut_wf, data = Wage)\ncut_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_cut()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)   age(30,50]   age(50,70]   age(70,80]  \n      6.256       -2.746       -3.038       10.310  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:      730.5 \nResidual Deviance: 704.3    AIC: 712.3"
  },
  {
    "objectID": "07-moving-beyond-linearity.html#splines",
    "href": "07-moving-beyond-linearity.html#splines",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.2 Splines",
    "text": "7.2 Splines\nIn order to fit regression splines, or in other words, use splines as preprocessors when fitting a linear model, we use step_bs() to construct the matrices of basis functions. The bs() function is used and arguments such as knots can be passed to bs() by using passing a named list to options.\n\nrec_spline <- recipe(wage ~ age, data = Wage) %>%\n  step_bs(age, options = list(knots = 25, 40, 60))\n\nWe already have the linear regression specification lm_spec so we can create the workflow, fit the model and predict with it like we have seen how to do in the previous chapters.\n\nspline_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spline)\n\nspline_fit <- fit(spline_wf, data = Wage)\n\npredict(spline_fit, new_data = Wage)\n\n# A tibble: 3,000 × 1\n   .pred\n   <dbl>\n 1  58.7\n 2  84.3\n 3 120. \n 4 120. \n 5 120. \n 6 119. \n 7 120. \n 8 102. \n 9 119. \n10 120. \n# … with 2,990 more rows\n\n\nLastly, we can plot the basic spline on top of the data.\n\nregression_lines <- bind_cols(\n  augment(spline_fit, new_data = age_range),\n  predict(spline_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")"
  },
  {
    "objectID": "07-moving-beyond-linearity.html#gams",
    "href": "07-moving-beyond-linearity.html#gams",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.3 GAMs",
    "text": "7.3 GAMs\nGAM section is WIP since they are now supported in parsnip."
  },
  {
    "objectID": "08-tree-based-methods.html",
    "href": "08-tree-based-methods.html",
    "title": "\n8  Tree-Based Methods\n",
    "section": "",
    "text": "This lab will take a look at different tree-based models, in doing so we will explore how changing the hyperparameters can help improve performance. This chapter will use parsnip for model fitting and recipes and workflows to perform the transformations, and tune and dials to tune the hyperparameters of the model. rpart.plot is used to visualize the decision trees created using the rpart package as engine, and vip is used to visualize variable importance for later models.\nThe Boston data set contain various statistics for 506 neighborhoods in Boston. We will build a regression model that related the median value of owner-occupied homes (medv) as the response with the remaining variables as predictors.\nWe will also use the Carseats data set from the ISLR package to demonstrate a classification model. We create a new variable High to denote if Sales <= 8, then the Sales predictor is removed as it is a perfect predictor of High."
  },
  {
    "objectID": "08-tree-based-methods.html#fitting-classification-trees",
    "href": "08-tree-based-methods.html#fitting-classification-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.1 Fitting Classification Trees",
    "text": "8.1 Fitting Classification Trees\nWe will both be fitting a classification and regression tree in this section, so we can save a little bit of typing by creating a general decision tree specification using rpart as the engine.\n\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\")\n\nThen this decision tree specification can be used to create a classification decision tree engine. This is a good example of how the flexible composition system created by parsnip can be used to create multiple model specifications.\n\nclass_tree_spec <- tree_spec %>%\n  set_mode(\"classification\")\n\nWith both a model specification and our data are we ready to fit the model.\n\nclass_tree_fit <- class_tree_spec %>%\n  fit(High ~ ., data = Carseats)\n\nWhen we look at the model output we see a quite informative summary of the model. It tries to give a written description of the tree that is created.\n\nclass_tree_fit\n\nparsnip model object\n\nn= 400 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 164 No (0.59000000 0.41000000)  \n    2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  \n      4) Price>=92.5 269  66 No (0.75464684 0.24535316)  \n        8) Advertising< 13.5 224  41 No (0.81696429 0.18303571)  \n         16) CompPrice< 124.5 96   6 No (0.93750000 0.06250000) *\n         17) CompPrice>=124.5 128  35 No (0.72656250 0.27343750)  \n           34) Price>=109.5 107  20 No (0.81308411 0.18691589)  \n             68) Price>=126.5 65   6 No (0.90769231 0.09230769) *\n             69) Price< 126.5 42  14 No (0.66666667 0.33333333)  \n              138) Age>=49.5 22   2 No (0.90909091 0.09090909) *\n              139) Age< 49.5 20   8 Yes (0.40000000 0.60000000) *\n           35) Price< 109.5 21   6 Yes (0.28571429 0.71428571) *\n        9) Advertising>=13.5 45  20 Yes (0.44444444 0.55555556)  \n         18) Age>=54.5 20   5 No (0.75000000 0.25000000) *\n         19) Age< 54.5 25   5 Yes (0.20000000 0.80000000) *\n      5) Price< 92.5 46  14 Yes (0.30434783 0.69565217)  \n       10) Income< 57 10   3 No (0.70000000 0.30000000) *\n       11) Income>=57 36   7 Yes (0.19444444 0.80555556) *\n    3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  \n      6) Price>=142.5 12   3 No (0.75000000 0.25000000) *\n      7) Price< 142.5 73  10 Yes (0.13698630 0.86301370) *\n\n\nOnce the tree gets more than a couple of nodes it can become hard to read the printed diagram. The rpart.plot package provides functions to let us easily visualize the decision tree. As the name implies, it only works with rpart trees.\n\nclass_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n\n\n\n\nWe can see that the most important variable to predict high sales appears to be shelving location as it forms the first node.\nThe training accuracy of this model is 85%\n\naugment(class_tree_fit, new_data = Carseats) %>%\n  accuracy(truth = High, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.848\n\n\nLet us take a look at the confusion matrix to see if the balance is there\n\naugment(class_tree_fit, new_data = Carseats) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  200  25\n       Yes  36 139\n\n\nAnd the model appears to work well overall. But this model was fit on the whole data set so we only get the training accuracy which could be misleading if the model is overfitting. Let us redo the fitting by creating a validation split and fit the model on the training data set.\n\nset.seed(1234)\nCarseats_split <- initial_split(Carseats)\n\nCarseats_train <- training(Carseats_split)\nCarseats_test <- testing(Carseats_split)\n\nNow we can fit the model on the training data set.\n\nclass_tree_fit <- fit(class_tree_spec, High ~ ., data = Carseats_train)\n\nLet us take a look at the confusion matrix for the training data set and testing data set.\n\naugment(class_tree_fit, new_data = Carseats_train) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  159  21\n       Yes  21  99\n\n\nThe training data set performs well as we would expect\n\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction No Yes\n       No  41   8\n       Yes 15  36\n\n\nbut the testing data set doesn’t perform just as well and get a smaller accuracy of 77%\n\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  accuracy(truth = High, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary          0.77\n\n\nLet us try to tune the cost_complexity of the decision tree to find a more optimal complexity. We use the class_tree_spec object and use the set_args() function to specify that we want to tune cost_complexity. This is then passed directly into the workflow object to avoid creating an intermediate object.\n\nclass_tree_wf <- workflow() %>%\n  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(High ~ .)\n\nTo be able to tune the variable we need 2 more objects. S resamples object, we will use a k-fold cross-validation data set, and a grid of values to try. Since we are only tuning 1 hyperparameter it is fine to stay with a regular grid.\n\nset.seed(1234)\nCarseats_fold <- vfold_cv(Carseats_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  class_tree_wf, \n  resamples = Carseats_fold, \n  grid = param_grid, \n  metrics = metric_set(accuracy)\n)\n\nusing autoplot() shows which values of cost_complexity appear to produce the highest accuracy\n\nautoplot(tune_res)\n\n\n\n\nWe can now select the best performing value with select_best(), finalize the workflow by updating the value of cost_complexity and fit the model on the full training data set.\n\nbest_complexity <- select_best(tune_res)\n\nclass_tree_final <- finalize_workflow(class_tree_wf, best_complexity)\n\nclass_tree_final_fit <- fit(class_tree_final, data = Carseats_train)\nclass_tree_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nHigh ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 300 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 300 120 No (0.6000000 0.4000000)  \n  2) ShelveLoc=Bad,Medium 242  73 No (0.6983471 0.3016529)  \n    4) Price>=92.5 213  51 No (0.7605634 0.2394366) *\n    5) Price< 92.5 29   7 Yes (0.2413793 0.7586207) *\n  3) ShelveLoc=Good 58  11 Yes (0.1896552 0.8103448) *\n\n\nAt last, we can visualize the model, and we see that the better-performing model is less complex than the original model we fit.\n\nclass_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()"
  },
  {
    "objectID": "08-tree-based-methods.html#fitting-regression-trees",
    "href": "08-tree-based-methods.html#fitting-regression-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.2 Fitting Regression Trees",
    "text": "8.2 Fitting Regression Trees\nWe will now show how we fit a regression tree. This is very similar to what we saw in the last section. The main difference here is that the response we are looking at will be continuous instead of categorical. We can reuse tree_spec as a base for the regression decision tree specification.\n\nreg_tree_spec <- tree_spec %>%\n  set_mode(\"regression\")\n\nWe are using the Boston data set here so we will do a validation split here.\n\nset.seed(1234)\nBoston_split <- initial_split(Boston)\n\nBoston_train <- training(Boston_split)\nBoston_test <- testing(Boston_split)\n\nfitting the model to the training data set\n\nreg_tree_fit <- fit(reg_tree_spec, medv ~ ., Boston_train)\nreg_tree_fit\n\nparsnip model object\n\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.9500 22.54802  \n   2) rm< 6.941 320 13602.3100 19.86281  \n     4) lstat>=14.395 129  2582.1090 14.51550  \n       8) nox>=0.607 80   984.7339 12.35875  \n        16) lstat>=19.34 47   388.6332 10.35957 *\n        17) lstat< 19.34 33   140.7188 15.20606 *\n       9) nox< 0.607 49   617.6939 18.03673 *\n     5) lstat< 14.395 191  4840.3640 23.47435  \n      10) rm< 6.543 151  2861.3990 22.21192  \n        20) dis>=1.68515 144  1179.5970 21.82083 *\n        21) dis< 1.68515 7  1206.6970 30.25714 *\n      11) rm>=6.543 40   829.8560 28.24000 *\n   3) rm>=6.941 59  4199.1020 37.11186  \n     6) rm< 7.437 35  1012.4100 32.08286 *\n     7) rm>=7.437 24  1010.6200 44.44583  \n      14) ptratio>=15.4 12   585.0767 40.71667 *\n      15) ptratio< 15.4 12    91.7825 48.17500 *\n\n\n\naugment(reg_tree_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        4.78\n\n\nand the rpart.plot() function works for the regression decision tree as well\n\nreg_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n\n\n\n\nNotice how the result is a numeric variable instead of a class.\nNow let us again try to tune the cost_complexity to find the best performing model.\n\nreg_tree_wf <- workflow() %>%\n  add_model(reg_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(medv ~ .)\n\nset.seed(1234)\nBoston_fold <- vfold_cv(Boston_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  reg_tree_wf, \n  resamples = Boston_fold, \n  grid = param_grid\n)\n\nAnd it appears that higher complexity works are to be preferred according to our cross-validation\n\nautoplot(tune_res)\n\n\n\n\nWe select the best-performing model according to \"rmse\" and fit the final model on the whole training data set.\n\nbest_complexity <- select_best(tune_res, metric = \"rmse\")\n\nreg_tree_final <- finalize_workflow(reg_tree_wf, best_complexity)\n\nreg_tree_final_fit <- fit(reg_tree_final, data = Boston_train)\nreg_tree_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmedv ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.95000 22.54802  \n   2) rm< 6.941 320 13602.31000 19.86281  \n     4) lstat>=14.395 129  2582.10900 14.51550  \n       8) nox>=0.607 80   984.73390 12.35875  \n        16) lstat>=19.34 47   388.63320 10.35957  \n          32) tax>=551.5 40   243.94980  9.67750 *\n          33) tax< 551.5 7    19.73714 14.25714 *\n        17) lstat< 19.34 33   140.71880 15.20606 *\n       9) nox< 0.607 49   617.69390 18.03673  \n        18) crim>=0.381565 25   313.20000 16.20000 *\n        19) crim< 0.381565 24   132.30000 19.95000 *\n     5) lstat< 14.395 191  4840.36400 23.47435  \n      10) rm< 6.543 151  2861.39900 22.21192  \n        20) dis>=1.68515 144  1179.59700 21.82083  \n          40) rm< 6.062 56   306.22860 20.28571 *\n          41) rm>=6.062 88   657.41950 22.79773  \n            82) lstat>=9.98 35    98.32686 21.02571 *\n            83) lstat< 9.98 53   376.61550 23.96792 *\n        21) dis< 1.68515 7  1206.69700 30.25714 *\n      11) rm>=6.543 40   829.85600 28.24000  \n        22) lstat>=4.44 33   274.06180 27.15455 *\n        23) lstat< 4.44 7   333.61710 33.35714 *\n   3) rm>=6.941 59  4199.10200 37.11186  \n     6) rm< 7.437 35  1012.41000 32.08286  \n      12) nox>=0.4885 14   673.46930 28.89286 *\n      13) nox< 0.4885 21   101.49810 34.20952 *\n     7) rm>=7.437 24  1010.62000 44.44583  \n      14) ptratio>=15.4 12   585.07670 40.71667 *\n      15) ptratio< 15.4 12    91.78250 48.17500 *\n\n\nVisualizing the model reveals a much more complex tree than what we saw in the last section.\n\nreg_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()"
  },
  {
    "objectID": "08-tree-based-methods.html#bagging-and-random-forests",
    "href": "08-tree-based-methods.html#bagging-and-random-forests",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.3 Bagging and Random Forests",
    "text": "8.3 Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data set. We will be using the randomForest package as the engine. A bagging model is the same as a random forest where mtry is equal to the number of predictors. We can specify the mtry to be .cols() which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and useable to many different data sets. .cols() is one of many descriptors in the parsnip package. We also set importance = TRUE in set_engine() to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the vip package later.\n\nbagging_spec <- rand_forest(mtry = .cols()) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\n\nWe fit the model like normal\n\nbagging_fit <- fit(bagging_spec, medv ~ ., data = Boston_train)\n\nand we take a look at the testing performance. Which we see is an improvement over the decision tree.\n\naugment(bagging_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.43\n\n\nWe can also create a quick scatterplot between the true and predicted value to see if we can make any diagnostics.\n\naugment(bagging_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nThere isn’t anything weird going on here so we are happy. Next, let us take a look at the variable importance\n\nvip(bagging_fit)\n\n\n\n\nNext, let us take a look at a random forest. By default, randomForest() p / 3 variables when building a random forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry = 6.\n\nrf_spec <- rand_forest(mtry = 6) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\n\nand fitting the model like normal\n\nrf_fit <- fit(rf_spec, medv ~ ., data = Boston_train)\n\nthis model has a slightly better performance than the bagging model\n\naugment(rf_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.26\n\n\nWe can likewise plot the true value against the predicted value\n\naugment(rf_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nit looks fine. No discernible difference between this chart and the one we created for the bagging model.\nThe variable importance plot is also quite similar to what we saw for the bagging model which isn’t surprising.\n\nvip(rf_fit)\n\n\n\n\nyou would normally want to perform hyperparameter tuning for the random forest model to get the best out of your forest. This exercise is left for the reader."
  },
  {
    "objectID": "08-tree-based-methods.html#boosting",
    "href": "08-tree-based-methods.html#boosting",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.4 Boosting",
    "text": "8.4 Boosting\nWe will now fit a boosted tree model. The xgboost packages give a good implementation of boosted trees. It has many parameters to tune and we know that setting trees too high can lead to overfitting. Nevertheless, let us try fitting a boosted tree. We set tree = 5000 to grow 5000 trees with a maximal depth of 4 by setting tree_depth = 4.\n\nboost_spec <- boost_tree(trees = 5000, tree_depth = 4) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nfitting the model like normal\n\nboost_fit <- fit(boost_spec, medv ~ ., data = Boston_train)\n\nand the rmse is a little high in this case which is properly because we didn’t tune any of the parameters.\n\naugment(boost_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.34\n\n\nWe can look at the scatterplot and we don’t see anything weird going on.\n\naugment(boost_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nYou would normally want to perform hyperparameter tuning for the boosted tree model to get the best out of your model. This exercise is left for the reader. Look at the Iterative search chapter of Tidy Modeling with R for inspiration."
  },
  {
    "objectID": "08-tree-based-methods.html#bayesian-additive-regression-trees",
    "href": "08-tree-based-methods.html#bayesian-additive-regression-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.5 Bayesian Additive Regression Trees",
    "text": "8.5 Bayesian Additive Regression Trees\nThis section is WIP."
  },
  {
    "objectID": "09-support-vector-machines.html",
    "href": "09-support-vector-machines.html",
    "title": "\n9  Support Vector Machines\n",
    "section": "",
    "text": "This lab will take a look at support vector machines, in doing so we will explore how changing the hyperparameters can help improve performance. This chapter will use parsnip for model fitting and recipes and workflows to perform the transformations, and tune and dials to tune the hyperparameters of the model."
  },
  {
    "objectID": "09-support-vector-machines.html#support-vector-classifier",
    "href": "09-support-vector-machines.html#support-vector-classifier",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.1 Support Vector Classifier",
    "text": "9.1 Support Vector Classifier\nLet us start by creating a synthetic data set. We will use some normally distributed data with an added offset to create 2 separate classes.\n\nset.seed(1)\nsim_data <- tibble(\n  x1 = rnorm(40),\n  x2 = rnorm(40),\n  y  = factor(rep(c(-1, 1), 20))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n\nPlotting it shows that we are having two slightly overlapping classes\n\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\n\n\n\n\nWe can then create a linear SVM specification by setting degree = 1 in a polynomial SVM model. We furthermore set scaled = FALSE in set_engine() to have the engine scale the data for us. Once we get to it later we can be performing this scaling in a recipe instead.\n\n\n\n\n\n\nNote\n\n\n\nset_engine() can be used to pass in additional arguments directly to the underlying engine. In this case, I’m passing in scaled = FALSE to kernlab::ksvm() which is the engine function.\n\n\n\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n\nTaking the specification, we can add a specific cost of 10 before fitting the model to the data. Using set_args() allows us to set the cost argument without modifying the model specification.\n\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 10 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 17 \n\nObjective Function Value : -152.0188 \nTraining error : 0.125 \nProbability model included. \n\n\nThe kernlab models can be visualized using the plot() function if you load the kernlab package.\n\nlibrary(kernlab)\nsvm_linear_fit %>%\n  extract_fit_engine() %>%\n  plot()\n\n\n\n\nwhat if we instead used a smaller value of the cost parameter?\n\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 0.1 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 25 \n\nObjective Function Value : -2.0376 \nTraining error : 0.15 \nProbability model included. \n\n\nNow that a smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider.\nLet us set up a tune_grid() section to find the value of cost that leads to the highest accuracy for the SVM model.\n\nsvm_linear_wf <- workflow() %>%\n  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\n\n\n\n\nusing the tune_res object and select_best() function allows us to find the value of cost that gives the best cross-validated accuracy. Finalize the workflow with finalize_workflow() and fit the new workflow on the data set.\n\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\n\nWe can now generate a different data set to act as the test data set. We will make sure that it is generated using the same model but with a different seed.\n\nset.seed(2)\nsim_data_test <- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n\nand accessing the model on this testing data set shows us that the model still performs very well.\n\naugment(svm_linear_fit, new_data = sim_data_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction -1 1\n        -1  8 3\n        1   2 7"
  },
  {
    "objectID": "09-support-vector-machines.html#support-vector-machine",
    "href": "09-support-vector-machines.html#support-vector-machine",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.2 Support Vector Machine",
    "text": "9.2 Support Vector Machine\nWe will now see how we can fit an SVM using a non-linear kernel. Let us start by generating some data, but this time generate with a non-linear class boundary.\n\nset.seed(1)\nsim_data2 <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nsim_data2 %>%\n  ggplot(aes(x1, x2, color = y)) +\n  geom_point()\n\n\n\n\nWe will try an SVM with a radial basis function. Such a kernel would allow us to capture the non-linearity in our data.\n\nsvm_rbf_spec <- svm_rbf() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\nfitting the model\n\nsvm_rbf_fit <- svm_rbf_spec %>%\n  fit(y ~ ., data = sim_data2)\n\nand plotting reveals that the model was able to separate the two classes, even though they were non-linearly separated.\n\nsvm_rbf_fit %>%\n  extract_fit_engine() %>%\n  plot()\n\n\n\n\nBut let us see how well this model generalizes to new data from the same generating process.\n\nset.seed(2)\nsim_data2_test <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nAnd it works well!\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction   1   2\n         1 137   7\n         2  13  43"
  },
  {
    "objectID": "09-support-vector-machines.html#roc-curves",
    "href": "09-support-vector-machines.html#roc-curves",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.3 ROC Curves",
    "text": "9.3 ROC Curves\nROC curves can easily be created using the roc_curve() function from the yardstick package. We use this function much the same way as we have done using the accuracy() function, but the main difference is that we pass the predicted class probability to estimate instead of passing the predicted class.\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1)\n\n# A tibble: 202 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1   -Inf          0            1    \n 2      0.104      0            1    \n 3      0.113      0.0200       1    \n 4      0.114      0.0400       1    \n 5      0.115      0.0600       1    \n 6      0.117      0.0800       1    \n 7      0.118      0.1          1    \n 8      0.119      0.12         1    \n 9      0.124      0.14         1    \n10      0.124      0.14         0.993\n# … with 192 more rows\n\n\nThis produces the different values of specificity and sensitivity for each threshold. We can get a quick visualization by passing the results of roc_curve() into autoplot()\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1) %>%\n  autoplot()\n\n\n\n\nA common metric is t calculate the area under this curve. This can be done using the roc_auc() function (_auc stands for area under curve).\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_auc(truth = y, estimate = .pred_1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.925"
  },
  {
    "objectID": "09-support-vector-machines.html#application-to-gene-expression-data",
    "href": "09-support-vector-machines.html#application-to-gene-expression-data",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.4 Application to Gene Expression Data",
    "text": "9.4 Application to Gene Expression Data\nWe now examine the Khan data set, which consists of several tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set comes in the Khan list which we will wrangle a little bit to create two tibbles, 1 for the training data and 1 for the testing data.\n\nKhan_train <- bind_cols(\n  y = factor(Khan$ytrain),\n  as_tibble(Khan$xtrain)\n)\n\nKhan_test <- bind_cols(\n  y = factor(Khan$ytest),\n  as_tibble(Khan$xtest)\n)\n\nlooking at the dimensions of the training data reveals that we have 63 observations with 20308 gene expression measurements.\n\ndim(Khan_train)\n\n[1]   63 2309\n\n\nThere is a very large number of predictors compared to the number of rows. This indicates that a linear kernel will be preferable, as the added flexibility we would get from a polynomial or radial kernel is unnecessary.\n\nkhan_fit <- svm_linear_spec %>%\n  set_args(cost = 10) %>%\n  fit(y ~ ., data = Khan_train)\n\nLet us take a look at the training confusion matrix. And look, we get a perfect confusion matrix. We are getting this because the hyperplane was able to fully separate the classes.\n\naugment(khan_fit, new_data = Khan_train) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction  1  2  3  4\n         1  8  0  0  0\n         2  0 23  0  0\n         3  0  0 12  0\n         4  0  0  0 20\n\n\nBut remember we don’t measure the performance by how well it performs on the training data set. We measure the performance of a model on how well it performs on the testing data set, so let us look at the testing confusion matrix\n\naugment(khan_fit, new_data = Khan_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction 1 2 3 4\n         1 3 0 0 0\n         2 0 6 2 0\n         3 0 0 4 0\n         4 0 0 0 5\n\n\nAnd it performs fairly well. A couple of misclassifications but nothing too bad."
  },
  {
    "objectID": "10-deep-learning.html",
    "href": "10-deep-learning.html",
    "title": "10  Deep learning",
    "section": "",
    "text": "There are no current plans to recreate this chapter using tidymodels as there isn’t any replacement for keras in tidymodels. If you would like something specific in this chapter please open an issue."
  },
  {
    "objectID": "11-survival-analysis.html",
    "href": "11-survival-analysis.html",
    "title": "\n11  Survival Analysis and Censored Data\n",
    "section": "",
    "text": "Survival analysis is being worked on in tidymodels, please check out censored as it is getting quite good, but still lacking a few key interactions with tidymodels to be included in this book."
  },
  {
    "objectID": "12-unsupervised-learning.html",
    "href": "12-unsupervised-learning.html",
    "title": "\n12  Unsupervised Learning\n",
    "section": "",
    "text": "This final chapter talks about unsupervised learning. This is broken into two parts. Dimensionality reduction and clustering. One downside at this moment is that clustering is not well integrated into tidymodels at this time. But we are still able to use some of the features in tidymodels."
  },
  {
    "objectID": "12-unsupervised-learning.html#principal-components-analysis",
    "href": "12-unsupervised-learning.html#principal-components-analysis",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.1 Principal Components Analysis",
    "text": "12.1 Principal Components Analysis\nThis section will be used to explore the USArrests data set using PCA. Before we move on, let is turn USArrests into a tibble and move the rownames into a column.\n\nUSArrests <- as_tibble(USArrests, rownames = \"state\")\nUSArrests\n\n# A tibble: 50 × 5\n   state       Murder Assault UrbanPop  Rape\n   <chr>        <dbl>   <int>    <int> <dbl>\n 1 Alabama       13.2     236       58  21.2\n 2 Alaska        10       263       48  44.5\n 3 Arizona        8.1     294       80  31  \n 4 Arkansas       8.8     190       50  19.5\n 5 California     9       276       91  40.6\n 6 Colorado       7.9     204       78  38.7\n 7 Connecticut    3.3     110       77  11.1\n 8 Delaware       5.9     238       72  15.8\n 9 Florida       15.4     335       80  31.9\n10 Georgia       17.4     211       60  25.8\n# … with 40 more rows\n\n\nNotice how the mean of each of the variables is quite different. if we were to apply PCA directly to the data set then Murder would have a very small influence.\n\nUSArrests %>%\n  select(-state) %>%\n  map_dfr(mean)\n\n# A tibble: 1 × 4\n  Murder Assault UrbanPop  Rape\n   <dbl>   <dbl>    <dbl> <dbl>\n1   7.79    171.     65.5  21.2\n\n\nWe will show how to perform PCA in two different ways in this section. Firstly, by using prcomp() directly, using broom::tidy() to extract the information we need, and secondly by using recipes. prcomp() takes 1 required argument x which much be a fully numeric data.frame or matrix. Then we pass that to prcomp(). We also set scale = TRUE in prcomp() which will perform the scaling we need.\n\nUSArrests_pca <- USArrests %>%\n  select(-state) %>%\n  prcomp(scale = TRUE)\n\nUSArrests_pca\n\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n\n\nNow we can use our favorite broom function to extract information from this prcomp object. We start with tidy(). tidy() can be used to extract a couple of different things, see ?broom:::tidy.prcomp() for more information. tidy() will by default extract the scores of a PCA object in long tidy format. The score is the location of the observation in PCA space. So we can\n\ntidy(USArrests_pca)\n\n# A tibble: 200 × 3\n     row    PC  value\n   <int> <dbl>  <dbl>\n 1     1     1 -0.976\n 2     1     2  1.12 \n 3     1     3 -0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2  1.06 \n 7     2     3  2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2 -0.738\n# … with 190 more rows\n\n\nWe can also explicitly say we want the scores by setting matrix = \"scores\".\n\ntidy(USArrests_pca, matrix = \"scores\")\n\n# A tibble: 200 × 3\n     row    PC  value\n   <int> <dbl>  <dbl>\n 1     1     1 -0.976\n 2     1     2  1.12 \n 3     1     3 -0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2  1.06 \n 7     2     3  2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2 -0.738\n# … with 190 more rows\n\n\nNext, we can get the loadings of the PCA.\n\ntidy(USArrests_pca, matrix = \"loadings\")\n\n# A tibble: 16 × 3\n   column      PC   value\n   <chr>    <dbl>   <dbl>\n 1 Murder       1 -0.536 \n 2 Murder       2  0.418 \n 3 Murder       3 -0.341 \n 4 Murder       4  0.649 \n 5 Assault      1 -0.583 \n 6 Assault      2  0.188 \n 7 Assault      3 -0.268 \n 8 Assault      4 -0.743 \n 9 UrbanPop     1 -0.278 \n10 UrbanPop     2 -0.873 \n11 UrbanPop     3 -0.378 \n12 UrbanPop     4  0.134 \n13 Rape         1 -0.543 \n14 Rape         2 -0.167 \n15 Rape         3  0.818 \n16 Rape         4  0.0890\n\n\nThis information tells us how each variable contributes to each principal component. If you don’t have too many principal components you can visualize the contribution without filtering\n\ntidy(USArrests_pca, matrix = \"loadings\") %>%\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col() +\n  scale_x_continuous(labels = scales::percent)\n\n\n\n\nLastly, we can set matrix = \"eigenvalues\" and get back the explained standard deviation for each PC including as a percent and cumulative which is quite handy for plotting.\n\ntidy(USArrests_pca, matrix = \"eigenvalues\")\n\n# A tibble: 4 × 4\n     PC std.dev percent cumulative\n  <dbl>   <dbl>   <dbl>      <dbl>\n1     1   1.57   0.620       0.620\n2     2   0.995  0.247       0.868\n3     3   0.597  0.0891      0.957\n4     4   0.416  0.0434      1    \n\n\nIf we want to see how the percent standard deviation explained drops off for each PC we can easily get that by using tidy() with matrix = \"eigenvalues\".\n\ntidy(USArrests_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_col()\n\n\n\n\nLastly, we have the augment() function which will give you back the fitted PC transformation if you apply it to the prcomp() object directly\n\naugment(USArrests_pca)\n\n# A tibble: 50 × 5\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4\n   <chr>          <dbl>      <dbl>      <dbl>      <dbl>\n 1 1            -0.976      1.12      -0.440     0.155  \n 2 2            -1.93       1.06       2.02     -0.434  \n 3 3            -1.75      -0.738      0.0542   -0.826  \n 4 4             0.140      1.11       0.113    -0.181  \n 5 5            -2.50      -1.53       0.593    -0.339  \n 6 6            -1.50      -0.978      1.08      0.00145\n 7 7             1.34      -1.08      -0.637    -0.117  \n 8 8            -0.0472    -0.322     -0.711    -0.873  \n 9 9            -2.98       0.0388    -0.571    -0.0953 \n10 10           -1.62       1.27      -0.339     1.07   \n# … with 40 more rows\n\n\nand will apply this transformation to new data by passing the new data to newdata\n\naugment(USArrests_pca, newdata = USArrests[1:5, ])\n\n# A tibble: 5 × 10\n  .rownames state Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3\n  <chr>     <chr>  <dbl>   <int>    <int> <dbl>      <dbl>      <dbl>      <dbl>\n1 1         Alab…   13.2     236       58  21.2     -0.976      1.12     -0.440 \n2 2         Alas…   10       263       48  44.5     -1.93       1.06      2.02  \n3 3         Ariz…    8.1     294       80  31       -1.75      -0.738     0.0542\n4 4         Arka…    8.8     190       50  19.5      0.140      1.11      0.113 \n5 5         Cali…    9       276       91  40.6     -2.50      -1.53      0.593 \n# … with 1 more variable: .fittedPC4 <dbl>\n\n\nIf you are using PCA as a preprocessing method I recommend you use recipes to apply the PCA transformation. This is a good way of doing it since recipe will correctly apply the same transformation to new data that the recipe is used on.\nWe step_normalize() to make sure all the variables are on the same scale. By using all_numeric() we are able to apply PCA on the variables we want without having to remove state. We are also setting an id for step_pca() to make it easier to tidy() later.\n\npca_rec <- recipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), id = \"pca\") %>%\n  prep()\n\nBy calling bake(new_data = NULL) we can get the fitted PC transformation of our numerical variables\n\npca_rec %>%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 5\n   state           PC1     PC2     PC3      PC4\n   <fct>         <dbl>   <dbl>   <dbl>    <dbl>\n 1 Alabama     -0.976   1.12   -0.440   0.155  \n 2 Alaska      -1.93    1.06    2.02   -0.434  \n 3 Arizona     -1.75   -0.738   0.0542 -0.826  \n 4 Arkansas     0.140   1.11    0.113  -0.181  \n 5 California  -2.50   -1.53    0.593  -0.339  \n 6 Colorado    -1.50   -0.978   1.08    0.00145\n 7 Connecticut  1.34   -1.08   -0.637  -0.117  \n 8 Delaware    -0.0472 -0.322  -0.711  -0.873  \n 9 Florida     -2.98    0.0388 -0.571  -0.0953 \n10 Georgia     -1.62    1.27   -0.339   1.07   \n# … with 40 more rows\n\n\nbut we can also supply our own data to new_data.\n\npca_rec %>%\n  bake(new_data = USArrests[40:45, ])\n\n# A tibble: 6 × 5\n  state             PC1    PC2    PC3     PC4\n  <fct>           <dbl>  <dbl>  <dbl>   <dbl>\n1 South Carolina -1.31   1.91  -0.298 -0.130 \n2 South Dakota    1.97   0.815  0.385 -0.108 \n3 Tennessee      -0.990  0.852  0.186  0.646 \n4 Texas          -1.34  -0.408 -0.487  0.637 \n5 Utah            0.545 -1.46   0.291 -0.0815\n6 Vermont         2.77   1.39   0.833 -0.143 \n\n\nWe can get back the same information as we could for prcomp() but we have to specify the slightly different inside tidy(). Here id = \"pca\" refers to the second step of pca_rec. We get the scores with type = \"coef\".\n\ntidy(pca_rec, id = \"pca\", type = \"coef\")\n\n# A tibble: 16 × 4\n   terms      value component id   \n   <chr>      <dbl> <chr>     <chr>\n 1 Murder   -0.536  PC1       pca  \n 2 Assault  -0.583  PC1       pca  \n 3 UrbanPop -0.278  PC1       pca  \n 4 Rape     -0.543  PC1       pca  \n 5 Murder    0.418  PC2       pca  \n 6 Assault   0.188  PC2       pca  \n 7 UrbanPop -0.873  PC2       pca  \n 8 Rape     -0.167  PC2       pca  \n 9 Murder   -0.341  PC3       pca  \n10 Assault  -0.268  PC3       pca  \n11 UrbanPop -0.378  PC3       pca  \n12 Rape      0.818  PC3       pca  \n13 Murder    0.649  PC4       pca  \n14 Assault  -0.743  PC4       pca  \n15 UrbanPop  0.134  PC4       pca  \n16 Rape      0.0890 PC4       pca  \n\n\nAnd the eigenvalues with type = \"variance\".\n\ntidy(pca_rec, id = \"pca\", type = \"variance\")\n\n# A tibble: 16 × 4\n   terms                         value component id   \n   <chr>                         <dbl>     <int> <chr>\n 1 variance                      2.48          1 pca  \n 2 variance                      0.990         2 pca  \n 3 variance                      0.357         3 pca  \n 4 variance                      0.173         4 pca  \n 5 cumulative variance           2.48          1 pca  \n 6 cumulative variance           3.47          2 pca  \n 7 cumulative variance           3.83          3 pca  \n 8 cumulative variance           4             4 pca  \n 9 percent variance             62.0           1 pca  \n10 percent variance             24.7           2 pca  \n11 percent variance              8.91          3 pca  \n12 percent variance              4.34          4 pca  \n13 cumulative percent variance  62.0           1 pca  \n14 cumulative percent variance  86.8           2 pca  \n15 cumulative percent variance  95.7           3 pca  \n16 cumulative percent variance 100             4 pca  \n\n\nSometimes you don’t want to get back all the principal components of the data. We can either specify how many components we want with num_comp (or rank. in prcomp())\n\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), num_comp = 3) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 4\n   state           PC1     PC2     PC3\n   <fct>         <dbl>   <dbl>   <dbl>\n 1 Alabama     -0.976   1.12   -0.440 \n 2 Alaska      -1.93    1.06    2.02  \n 3 Arizona     -1.75   -0.738   0.0542\n 4 Arkansas     0.140   1.11    0.113 \n 5 California  -2.50   -1.53    0.593 \n 6 Colorado    -1.50   -0.978   1.08  \n 7 Connecticut  1.34   -1.08   -0.637 \n 8 Delaware    -0.0472 -0.322  -0.711 \n 9 Florida     -2.98    0.0388 -0.571 \n10 Georgia     -1.62    1.27   -0.339 \n# … with 40 more rows\n\n\nor using a threshold to specify how many components to keep by the variance explained. So by setting threshold = 0.7, step_pca() will generate enough principal components to explain 70% of the variance.\n\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), threshold = 0.7) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 3\n   state           PC1     PC2\n   <fct>         <dbl>   <dbl>\n 1 Alabama     -0.976   1.12  \n 2 Alaska      -1.93    1.06  \n 3 Arizona     -1.75   -0.738 \n 4 Arkansas     0.140   1.11  \n 5 California  -2.50   -1.53  \n 6 Colorado    -1.50   -0.978 \n 7 Connecticut  1.34   -1.08  \n 8 Delaware    -0.0472 -0.322 \n 9 Florida     -2.98    0.0388\n10 Georgia     -1.62    1.27  \n# … with 40 more rows"
  },
  {
    "objectID": "12-unsupervised-learning.html#matrix-completion",
    "href": "12-unsupervised-learning.html#matrix-completion",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.2 Matrix Completion",
    "text": "12.2 Matrix Completion\nThis section is WIP."
  },
  {
    "objectID": "12-unsupervised-learning.html#kmeans-clustering",
    "href": "12-unsupervised-learning.html#kmeans-clustering",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.3 Kmeans Clustering",
    "text": "12.3 Kmeans Clustering\nThe kmeans() function can be used to perform K-means clustering in R. But before we get to that let us create a synthetic data set that we know has groups.\n\nset.seed(2)\n\nx_df <- tibble(\n  V1 = rnorm(n = 50, mean = rep(c(0, 3), each = 25)),\n  V2 = rnorm(n = 50, mean = rep(c(0, -4), each = 25))\n)\n\nAnd we can plot it with ggplot2 to see that the groups are really there. Note that we didn’t include this grouping information in x_df as we are trying to emulate a situation where we don’t know of the possible underlying clusters.\n\nx_df %>%\n  ggplot(aes(V1, V2, color = rep(c(\"A\", \"B\"), each = 25))) +\n  geom_point() +\n  labs(color = \"groups\")\n\n\n\n\nThe kmeans() function takes a matrix or data.frame and centers which is the number of clusters we want kmeans() to find. We also set nstart = 20, this allows the algorithm to have multiple initial starting positions, which we use in the hope of finding global maxima instead of local maxima.\n\nset.seed(1234)\nres_kmeans <- kmeans(x_df, centers = 3, nstart = 20)\n\nThis fitted model has a lot of different kinds of information.\n\nres_kmeans\n\nK-means clustering with 3 clusters of sizes 11, 23, 16\n\nCluster means:\n         V1          V2\n1 2.5355362 -2.48605364\n2 0.2339095  0.04414551\n3 2.8241300 -5.01221675\n\nClustering vector:\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 3 1 1 1 3 1 3 3 3 3 1 3 3\n[39] 3 1 1 1 3 3 3 3 1 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 14.56698 54.84869 26.98215\n (between_SS / total_SS =  76.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nAnd we can use broom functions to extract information in tidy formats. The tidy() function returns information for each cluster, including their position, size and within-cluster sum-of-squares.\n\ntidy(res_kmeans)\n\n# A tibble: 3 × 5\n     V1      V2  size withinss cluster\n  <dbl>   <dbl> <int>    <dbl> <fct>  \n1 2.54  -2.49      11     14.6 1      \n2 0.234  0.0441    23     54.8 2      \n3 2.82  -5.01      16     27.0 3      \n\n\nThe glance() function returns model wise metrics. One of these is tot.withinss which is the total within-cluster sum-of-squares that we seek to minimize when we perform K-means clustering.\n\nglance(res_kmeans)\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  416.         96.4      320.     2\n\n\nLastly, we can see what cluster each observation belongs to by using augment() which “predicts” which cluster a given observation belongs to.\n\naugment(res_kmeans, data = x_df)\n\n# A tibble: 50 × 3\n        V1     V2 .cluster\n     <dbl>  <dbl> <fct>   \n 1 -0.897  -0.838 2       \n 2  0.185   2.07  2       \n 3  1.59   -0.562 2       \n 4 -1.13    1.28  2       \n 5 -0.0803 -1.05  2       \n 6  0.132  -1.97  2       \n 7  0.708  -0.323 2       \n 8 -0.240   0.936 2       \n 9  1.98    1.14  2       \n10 -0.139   1.67  2       \n# … with 40 more rows\n\n\nWe can visualize the result of augment() to see how well the clustering performed.\n\naugment(res_kmeans, data = x_df) %>%\n  ggplot(aes(V1, V2, color = .cluster)) +\n  geom_point()\n\n\n\n\nThis is all well and good, but it would be nice if we could try out a number of different clusters and then find the best one. We will use the mutate() and map() combo to fit multiple models and extract information from them. We remember to set a seed to ensure reproducibility.\n\nset.seed(1234)\nmulti_kmeans <- tibble(k = 1:10) %>%\n  mutate(\n    model = purrr::map(k, ~ kmeans(x_df, centers = .x, nstart = 20)),\n    tot.withinss = purrr::map_dbl(model, ~ glance(.x)$tot.withinss)\n  )\n\nmulti_kmeans\n\n# A tibble: 10 × 3\n       k model    tot.withinss\n   <int> <list>          <dbl>\n 1     1 <kmeans>        416. \n 2     2 <kmeans>        127. \n 3     3 <kmeans>         96.4\n 4     4 <kmeans>         73.4\n 5     5 <kmeans>         57.4\n 6     6 <kmeans>         42.4\n 7     7 <kmeans>         32.4\n 8     8 <kmeans>         27.9\n 9     9 <kmeans>         23.5\n10    10 <kmeans>         20.3\n\n\nNow that we have the total within-cluster sum-of-squares we can plot them against k so we can use the elbow method to find the optimal number of clusters.\n\nmulti_kmeans %>%\n  ggplot(aes(k, tot.withinss)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(1,10))\n\n\n\n\nWe see an elbow at k = 2 which makes us happy since the data set is specifically created to have 2 clusters. We can now extract the model where k = 2 from multi_kmeans.\n\nfinal_kmeans <- multi_kmeans %>%\n  filter(k == 2) %>%\n  pull(model) %>%\n  pluck(1)\n\nAnd we can finish by visualizing the clusters it found.\n\naugment(final_kmeans, data = x_df) %>%\n  ggplot(aes(V1, V2, color = .cluster)) +\n  geom_point()"
  },
  {
    "objectID": "12-unsupervised-learning.html#hierarchical-clustering",
    "href": "12-unsupervised-learning.html#hierarchical-clustering",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.4 Hierarchical Clustering",
    "text": "12.4 Hierarchical Clustering\nThe hclust() function is one way to perform hierarchical clustering in R. It only needs one input and that is a dissimilarity structure as produced by dist(). Furthermore, we can specify a couple of things, including the agglomeration method. Let us cluster this data in a couple of different ways to see how the choice of agglomeration method changes the clustering.\n\nres_hclust_complete <- x_df %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\nres_hclust_average <- x_df %>%\n  dist() %>%\n  hclust(method = \"average\")\n\nres_hclust_single <- x_df %>%\n  dist() %>%\n  hclust(method = \"single\")\n\nThe factoextra package provides functions (fviz_dend()) to visualize the clustering created using hclust(). We use fviz_dend() to show the dendrogram.\n\nfviz_dend(res_hclust_complete, main = \"complete\", k = 2)\n\n\n\n\n\nfviz_dend(res_hclust_average, main = \"average\", k = 2)\n\n\n\n\n\nfviz_dend(res_hclust_single, main = \"single\", k = 2)\n\n\n\n\nIf we don’t know the importance of the different predictors in data set it could be beneficial to scale the data such that each variable has the same influence. We can perform scaling by using scale() before dist().\n\nx_df %>%\n  scale() %>%\n  dist() %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend(k = 2)\n\n\n\n\nAnother way of calculating distances is based on correlation. This only makes sense if the data set has 3 or more variables.\n\n# correlation based distance\nset.seed(2)\nx <- matrix(rnorm(30 * 3), ncol = 3)\n\nx %>%\n  proxy::dist(method = \"correlation\") %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend()"
  },
  {
    "objectID": "12-unsupervised-learning.html#pca-on-the-nci60-data",
    "href": "12-unsupervised-learning.html#pca-on-the-nci60-data",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.5 PCA on the NCI60 Data",
    "text": "12.5 PCA on the NCI60 Data\nWe will now explore the NCI60 data set. It is genomic data set, containing cancer cell line microarray data, which consists of 6830 gene expression measurements on 64 cancer cell lines. The data comes as a list containing a matrix and its labels. We do a little work to turn the data into a tibble we will use for the rest of the chapter.\n\ndata(NCI60, package = \"ISLR\")\nnci60 <- NCI60$data %>%\n  as_tibble(.name_repair = ~ paste0(\"V_\", .x)) %>%\n  mutate(label = factor(NCI60$labs)) %>%\n  relocate(label)\n\nWe do not expect to use the label variable doing the analysis since we are emulating an unsupervised analysis. Since we are an exploratory task we will be fine with using prcomp() since we don’t need to apply these transformations to anything else. We remove label and remember to set scale = TRUE to perform scaling of all the variables.\n\nnci60_pca <- nci60 %>%\n  select(-label) %>%\n  prcomp(scale = TRUE)\n\nFor visualization purposes, we will now join up the labels into the result of augment(nci60_pca) so we can visualize how close similar labeled points are to each other.\n\nnci60_pcs <- bind_cols(\n  augment(nci60_pca),\n  nci60 %>% select(label)\n)\n\nWe have 14 different labels, so we will make use of the \"Polychrome 36\" palette to help us better differentiate between the labels.\n\ncolors <- unname(palette.colors(n = 14, palette = \"Polychrome 36\"))\n\nOr we can plot the different PCs against each other. It is a good idea to compare the first PCs against each other since they carry the most information. We will just compare the pairs 1-2 and 1-3 but you can do more yourself. It tends to be a good idea to stop once interesting things appear in the plots.\n\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n\n\n\n\nWe see there is some local clustering of the different cancer types which is promising, it is not perfect but let us see what happens when we compare PC1 against PC3 now.\n\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC3, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n\n\n\n\nLastly, we will plot the variance explained of each principal component. We can use tidy() with matrix = \"eigenvalues\" to accomplish this easily, so we start with the percentage of each PC\n\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 60, by = 5)) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\nwith the first PC having a little more than 10% and a fairly fast drop.\nAnd we can get the cumulative variance explained just the same.\n\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, cumulative)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "12-unsupervised-learning.html#clustering-on-nci60-dataset",
    "href": "12-unsupervised-learning.html#clustering-on-nci60-dataset",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.6 Clustering on nci60 dataset",
    "text": "12.6 Clustering on nci60 dataset\nLet us now see what happens if we perform clustering on the nci60 data set. Before we start it would be good if we create a scaled version of this data set. We can use the recipes package to perform those transformations.\n\nnci60_scaled <- recipe(~ ., data = nci60) %>%\n  step_rm(label) %>%\n  step_normalize(all_predictors()) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nNow we start by fitting multiple hierarchical clustering models using different agglomeration methods.\n\nnci60_complete <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"complete\")\n\nnci60_average <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"average\")\n\nnci60_single <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"single\")\n\nWe then visualize them to see if any of them have some good natural separations.\n\nfviz_dend(nci60_complete, main = \"Complete\")\n\n\n\n\n\nfviz_dend(nci60_average, main = \"Average\")\n\n\n\n\n\nfviz_dend(nci60_single, main = \"Single\")\n\n\n\n\nWe now color according to k = 4 and we get the following separations.\n\nnci60_complete %>%\n  fviz_dend(k = 4, main = \"hclust(complete) on nci60\")\n\n\n\n\nWe now take the clustering id extracted with cutree and calculate which label is the most common one within each cluster.\n\ntibble(\n  label = nci60$label,\n  cluster_id = cutree(nci60_complete, k = 4)\n) %>%\n  count(label, cluster_id) %>%\n  group_by(cluster_id) %>%\n  mutate(prop = n / sum(n)) %>%\n  slice_max(n = 1, order_by = prop) %>%\n  ungroup()\n\n# A tibble: 6 × 4\n  label    cluster_id     n  prop\n  <fct>         <int> <int> <dbl>\n1 MELANOMA          1     8 0.2  \n2 NSCLC             1     8 0.2  \n3 RENAL             1     8 0.2  \n4 BREAST            2     3 0.429\n5 LEUKEMIA          3     6 0.75 \n6 COLON             4     5 0.556\n\n\nWe can also see what happens if we try to fit a K-means clustering. We liked 4 clusters from earlier so let’s stick with that.\n\nset.seed(2)\nres_kmeans_scaled <- kmeans(nci60_scaled, centers = 4, nstart = 50)\n\nWe can again use tidy() to extract cluster information, note that we only look at cluster, size, and withinss as there are thousands of other variables denoting the location of the cluster.\n\ntidy(res_kmeans_scaled) %>%\n  select(cluster, size, withinss)\n\n# A tibble: 4 × 3\n  cluster  size withinss\n  <fct>   <int>    <dbl>\n1 1           8   44071.\n2 2          20  108801.\n3 3          27  154545.\n4 4           9   37150.\n\n\nLastly, let us see how the two different methods we used compare against each other. Let us save the cluster ids in cluster_kmeans and cluster_hclust and then use conf_mat() in a different way to quickly generate a heatmap between the two methods.\n\ncluster_kmeans <- res_kmeans_scaled$cluster\ncluster_hclust <- cutree(nci60_complete, k = 4)\n\ntibble(\n  kmeans = factor(cluster_kmeans),\n  hclust = factor(cluster_hclust)\n) %>%\n  conf_mat(kmeans, hclust) %>%\n  autoplot(type = \"heatmap\")\n\n\n\n\nThere is not a lot of agreement between labels which makes sense, since the labels themselves are arbitrarily added. What is important is that they tend to agree quite a lot (the confusion matrix is sparse).\nOne last thing is that it is sometimes useful to perform dimensionality reduction before using the clustering method. Let us use the recipes package to calculate the PCA of nci60 and keep the 5 first components (we could have started with nci60 too if we added step_rm() and step_normalize()).\n\nnci60_pca <- recipe(~., nci60_scaled) %>%\n  step_pca(all_predictors(), num_comp = 5) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nWe can now use hclust() on this reduced data set, and sometimes we get quite good results since the clustering method doesn’t have to work in high dimensions.\n\nnci60_pca %>%\n  dist() %>%\n  hclust() %>%\n  fviz_dend(k = 4, main = \"hclust on first five PCs\")"
  },
  {
    "objectID": "13-multiple-testing.html",
    "href": "13-multiple-testing.html",
    "title": "\n13  Multiple Testing\n",
    "section": "",
    "text": "This chapter is WIP. If you would like something specific in this chapter please open an issue.\nThis section is being worked on to include code using the infer package."
  }
]