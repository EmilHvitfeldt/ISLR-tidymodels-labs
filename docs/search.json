[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"book aims complement 2nd version Introduction Statistical Learning book translations labs using tidymodels set packages.labs mirrored quite closely stay true original material.","code":""},{"path":"index.html","id":"version-differences","chapter":"1 Introduction","heading":"Version Differences","text":"listed changes relative 1st version.Naive Bayes added chapter 4 ClassificationPoisson Regression added chapter 4 Classification“Application Caravan Insurance Data” section longer treated section now part K-Nearest Neighbors sectionBayesian Additive Regression Trees added chapter 8 Tree-Based Methodschapter Unsupervised Learning renumbered chapter 12 instead 10Matrix Completion added chapter 12 Unsupervised Learningchapter Deep learning added chapter 10chapter Survival Analysis Censored Data added chapter 11chapter Multiple Testing added chapter 13","code":""},{"path":"index.html","id":"colophon","chapter":"1 Introduction","heading":"Colophon","text":"book written RStudio using bookdown. website hosted via GitHub Pages, complete source available GitHub.version book built R version 4.1.0 (2021-05-18) following packages:","code":""},{"path":"statistical-learning.html","id":"statistical-learning","chapter":"2 Statistical learning","heading":"2 Statistical learning","text":"original labs introduce basics R. repeat endeavor done well. notice labs book look slightly different since following tidyverse style guide.primary purpose book rewrite labs ISLR using tidymodels packages. great introduction tidymodels can found tidymodels website.Proper introductions various tidymodels packages present labs since aim mirror labs ISLR. getting started page good introductions toparsniprecipes workflowsrsampletuneThe charts whenever possible created using ggplot2, unfamiliar ggplot2 recommend start reading Data Visualisation chapter R Data Science book.tidymodels packages can installed whole using install.packages(\"tidymodels\") ISLR package contains many data set using can installed using install.packages(\"ISLR\").","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"lab go perform linear regression. include simple linear regression multiple linear regression addition can apply transformations predictors. chapter use parsnip model fitting recipes workflows perform transformations.","code":""},{"path":"linear-regression.html","id":"libraries","chapter":"3 Linear Regression","heading":"3.1 Libraries","text":"load tidymodels ISLR MASS data sets.","code":"\nlibrary(MASS) # For Boston data set\nlibrary(tidymodels)\nlibrary(ISLR)"},{"path":"linear-regression.html","id":"simple-linear-regression","chapter":"3 Linear Regression","heading":"3.2 Simple linear regression","text":"Boston data set contain various statistics 506 neighborhoods Boston. build simple linear regression model related median value owner-occupied homes (medv) response variable indicating percentage population belongs lower status (lstat) predictor.\nBoston data set quite outdated contains really unfortunate variables.\nstart creating parsnip specification linear regression model.unnecessary set mode linear regression since can regression, continue labs explicit.specification doesn’t perform calculations . just specification want .specification can fit supplying formula expression data want fit model .\nformula written form y ~ x y name response x name predictors.\nnames used formula match names variables data set passed data.result fit parsnip model object. object contains underlying fit well parsnip-specific information. want look underlying fit object can access lm_fit$fit withThe lm object nice summary() method shows information fit, including parameter estimates lack--fit statistics.can use packages broom package extract key information model objects tidy formats.tidy() function returns parameter estimates lm objectand glance() can used extract model statistics.Suppose like model fit want generate predictions, typically use predict() function like :produces error used parsnip model object. happening need explicitly supply data set predictions performed via new_data argumentNotice predictions returned tibble. always case parsnip models, matter engine used. useful since consistency allows us combine data sets easily.can also return types predicts specifying type argument. Setting type = \"conf_int\" return 95% confidence interval.\nengines can return types predictions.\nwant evaluate performance model, might want compare observed value predicted value data set. YouYou can get results using augment() function little bit typing","code":"\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\nlm_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat, data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  5ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       34.55        -0.95\nlm_fit %>% \n  pluck(\"fit\")## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       34.55        -0.95\nlm_fit %>% \n  pluck(\"fit\") %>%\n  summary()## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\ntidy(lm_fit)## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   34.6      0.563       61.4 3.74e-236\n## 2 lstat         -0.950    0.0387     -24.5 5.08e- 88\nglance(lm_fit)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.544         0.543  6.22      602. 5.08e-88     1 -1641. 3289. 3302.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npredict(lm_fit)## Error in predict_numeric(object = object, new_data = new_data, ...): argument \"new_data\" is missing, with no default\npredict(lm_fit, new_data = Boston)## # A tibble: 506 × 1\n##    .pred\n##    <dbl>\n##  1 29.8 \n##  2 25.9 \n##  3 30.7 \n##  4 31.8 \n##  5 29.5 \n##  6 29.6 \n##  7 22.7 \n##  8 16.4 \n##  9  6.12\n## 10 18.3 \n## # … with 496 more rows\npredict(lm_fit, new_data = Boston, type = \"conf_int\")## # A tibble: 506 × 2\n##    .pred_lower .pred_upper\n##          <dbl>       <dbl>\n##  1       29.0        30.6 \n##  2       25.3        26.5 \n##  3       29.9        31.6 \n##  4       30.8        32.7 \n##  5       28.7        30.3 \n##  6       28.8        30.4 \n##  7       22.2        23.3 \n##  8       15.6        17.1 \n##  9        4.70        7.54\n## 10       17.7        18.9 \n## # … with 496 more rows\nbind_cols(\n  predict(lm_fit, new_data = Boston),\n  Boston\n) %>%\n  select(medv, .pred)## # A tibble: 506 × 2\n##     medv .pred\n##    <dbl> <dbl>\n##  1  24   29.8 \n##  2  21.6 25.9 \n##  3  34.7 30.7 \n##  4  33.4 31.8 \n##  5  36.2 29.5 \n##  6  28.7 29.6 \n##  7  22.9 22.7 \n##  8  27.1 16.4 \n##  9  16.5  6.12\n## 10  18.9 18.3 \n## # … with 496 more rows\naugment(lm_fit, new_data = Boston) %>% \n  select(medv, .pred)## # A tibble: 506 × 2\n##     medv .pred\n##    <dbl> <dbl>\n##  1  24   29.8 \n##  2  21.6 25.9 \n##  3  34.7 30.7 \n##  4  33.4 31.8 \n##  5  36.2 29.5 \n##  6  28.7 29.6 \n##  7  22.9 22.7 \n##  8  27.1 16.4 \n##  9  16.5  6.12\n## 10  18.9 18.3 \n## # … with 496 more rows"},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"3 Linear Regression","heading":"3.3 Multiple linear regression","text":"multiple linear regression model can fit much way simple linear regression model. difference specify predictors. using formula expression y ~ x, can specify multiple values separating +s.Everything else works . extracting parameter estimatesto predicting new valuesA shortcut using formulas use form y ~ . means; set y response set remaining variables predictors. useful lot variables don’t want type .formula syntax look ?formula.","code":"\nlm_fit2 <- lm_spec %>% \n  fit(medv ~ lstat + age, data = Boston)\n\nlm_fit2## parsnip model object\n## \n## Fit time:  2ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat + age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  \n##    33.22276     -1.03207      0.03454\ntidy(lm_fit2)## # A tibble: 3 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)  33.2       0.731      45.5  2.94e-180\n## 2 lstat        -1.03      0.0482    -21.4  8.42e- 73\n## 3 age           0.0345    0.0122      2.83 4.91e-  3\npredict(lm_fit2, new_data = Boston)## # A tibble: 506 × 1\n##    .pred\n##    <dbl>\n##  1 30.3 \n##  2 26.5 \n##  3 31.2 \n##  4 31.8 \n##  5 29.6 \n##  6 29.9 \n##  7 22.7 \n##  8 16.8 \n##  9  5.79\n## 10 18.5 \n## # … with 496 more rows\nlm_fit3 <- lm_spec %>% \n  fit(medv ~ ., data = Boston)\n\nlm_fit3## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = medv ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)         crim           zn        indus         chas          nox  \n##   3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  \n##          rm          age          dis          rad          tax      ptratio  \n##   3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  \n##       black        lstat  \n##   9.312e-03   -5.248e-01"},{"path":"linear-regression.html","id":"interaction-terms","chapter":"3 Linear Regression","heading":"3.4 Interaction terms","text":"Adding interaction terms quite easy using formula expressions. However, syntax used describe isn’t accepted engines go include interaction terms using recipes well.two ways including interaction term; x:y x * yx:y include interaction x y,x * y include interaction x y, x, y, e.. short x:y + x + y.way let expand lm_fit2 adding interaction termnote interaction term named lstat:age.Sometimes want perform transformations, want transformations applied, part model fit pre-processing step. use recipes package task.use step_interact() specify interaction term. Next, create workflow object combine linear regression model specification lm_spec pre-processing specification rec_spec_interact can fitted much like parsnip model specification.Notice since specified variables recipe don’t need specify fitting workflow object. Furthermore, take note name interaction term. step_interact() tries avoid special characters variables.","code":"\nlm_fit4 <- lm_spec %>%\n  fit(medv ~ lstat * age, data = Boston)\n\nlm_fit4## parsnip model object\n## \n## Fit time:  2ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat * age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age    lstat:age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560\nrec_spec_interact <- recipe(medv ~ lstat + age, data = Boston) %>%\n  step_interact(~ lstat:age)\n\nlm_wf_interact <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_interact)\n\nlm_wf_interact %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  lstat_x_age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560"},{"path":"linear-regression.html","id":"non-linear-transformations-of-the-predictors","chapter":"3 Linear Regression","heading":"3.5 Non-linear transformations of the predictors","text":"Much like use recipes create interaction terms values able apply transformations individual variables well. familiar dplyr package know mutate() works much way using step_mutate().want keep much pre-processing inside recipes transformation applied consistently new data.don’t hand-craft every type linear transformation since recipes bunch created already step_log() take logarithms variables.","code":"\nrec_spec_pow2 <- recipe(medv ~ lstat, data = Boston) %>%\n  step_mutate(lstat2 = lstat ^ 2)\n\nlm_wf_pow2 <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_pow2)\n\nlm_wf_pow2 %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_mutate()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat       lstat2  \n##    42.86201     -2.33282      0.04355\nrec_spec_log <- recipe(medv ~ lstat, data = Boston) %>%\n  step_log(lstat)\n\nlm_wf_log <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_log)\n\nlm_wf_log %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_log()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       52.12       -12.48"},{"path":"linear-regression.html","id":"qualitative-predictors","chapter":"3 Linear Regression","heading":"3.6 Qualitative predictors","text":"now turn attention Carseats data set. attempt predict Sales child car seats 400 locations based number predictors. One variables ShelveLoc qualitative predictor indicates quality shelving location. ShelveLoc takes three possible valuesBadMediumGoodIf pass variable lm() read generate dummy variables automatically using following convention.problems including qualitative predictors using lm engine.however, many things, can always guarantee underlying engine knows deal qualitative variables. recipes can used handle well. step_dummy() perform transformation turning 1 qualitative C levels C-1 indicator variables.\nmight seem unnecessary right now, engines, later , handle qualitative variables step necessary. also using all_nominal_predictors() selector select character factor predictor variables. allows us select type rather type names.","code":"\nCarseats %>%\n  pull(ShelveLoc) %>%\n  contrasts()##        Good Medium\n## Bad       0      0\n## Good      1      0\n## Medium    0      1\nlm_spec %>% \n  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = Sales ~ . + Income:Advertising + Price:Age, \n##     data = data)\n## \n## Coefficients:\n##        (Intercept)           CompPrice              Income         Advertising  \n##          6.5755654           0.0929371           0.0108940           0.0702462  \n##         Population               Price       ShelveLocGood     ShelveLocMedium  \n##          0.0001592          -0.1008064           4.8486762           1.9532620  \n##                Age           Education            UrbanYes               USYes  \n##         -0.0579466          -0.0208525           0.1401597          -0.1575571  \n## Income:Advertising           Price:Age  \n##          0.0007510           0.0001068\nrec_spec <- recipe(Sales ~ ., data = Carseats) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~ Income:Advertising + Price:Age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Carseats)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_dummy()\n## • step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##          (Intercept)             CompPrice                Income  \n##            6.5755654             0.0929371             0.0108940  \n##          Advertising            Population                 Price  \n##            0.0702462             0.0001592            -0.1008064  \n##                  Age             Education        ShelveLoc_Good  \n##           -0.0579466            -0.0208525             4.8486762  \n##     ShelveLoc_Medium             Urban_Yes                US_Yes  \n##            1.9532620             0.1401597            -0.1575571  \n## Income_x_Advertising           Price_x_Age  \n##            0.0007510             0.0001068"},{"path":"linear-regression.html","id":"writing-functions","chapter":"3 Linear Regression","heading":"3.7 Writing functions","text":"book talk write functions R. still wants know write functions recommend functions R Data Science.","code":""},{"path":"classification.html","id":"classification","chapter":"4 Classification","heading":"4 Classification","text":"lab first experience classification models. models differ regression model saw last chapter fact response variable qualitative variable instead continuous variable.\nchapter use parsnip model fitting recipes workflows perform transformations.","code":""},{"path":"classification.html","id":"the-stock-market-data","chapter":"4 Classification","heading":"4.1 The Stock Market Data","text":"load tidymodels modeling functions, ISLR ISLR2 data sets, discrim give us access discriminant analysis models LDA QDA well Naive Bayes model poissonreg Poisson Regression.examining Smarket data set lab. contains number numeric variables plus variable called Direction two labels \"\" \"\". modeling, let us take look correlation variables.look correlation, use corrr package. correlate() function calculate correlation matrix variables fed. therefore remove Direction numeric.\npass rplot() quickly visualize correlation matrix. also changed colours argument better see going .see variables less uncorrelated . pair Year Volume little correlated.want create heatmap styled correlation chart can also create manually.plot Year Volume see upwards trend Volume time.","code":"\nlibrary(tidymodels)\nlibrary(ISLR) # For the Smarket data set\nlibrary(ISLR2) # For the Bikeshare data set\nlibrary(discrim)\nlibrary(poissonreg)\nlibrary(corrr)\ncor_Smarket <- Smarket %>%\n  select(-Direction) %>%\n  correlate()## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))## Don't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\nlibrary(paletteer)\ncor_Smarket %>%\n  stretch() %>%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1)\nggplot(Smarket, aes(Year, Volume)) +\n  geom_jitter(height = 0)"},{"path":"classification.html","id":"logistic-regression","chapter":"4 Classification","heading":"4.2 Logistic Regression","text":"Now fit logistic regression model. use parsnip package, use logistic_reg() create logistic regression model specification.Notice set engine mode, just restating defaults.can now fit model like normal. want model Direction stock market based percentage return 5 previous days plus volume shares traded.\nfitting classification parsnip requires response variable factor. case Smarket data set don’t need adjustments.fit done using glm() function, comes handy summary() method well.lets us see couple different things ; parameter estimates, standard errors, p-values, model fit statistics. can use tidy() function extract model attributes analysis presentation.Predictions done much way. use model predict data trained .result tibble single column .pred_class factor variable labels original training data set.can also get back probability predictions, specifying type = \"prob\".note get back column classes. little reductive since easily calculate inverse, get multi-classification models becomes quite handy.Using augment() can add predictions data.frame use look model performance metrics. calculate metrics directly, find useful look confusion matrix. show well predictive model performing given table predicted values true value.good performing model ideally high numbers along diagonal (-left -right) small numbers -diagonal. see model isn’t great, tends predict \"\" \"\" often .want visual representation confusion matrix can pipe result conf_mat() autoplot() generate ggplot2 chart.can also calculate various performance metrics. One common metrics accuracy, often model predicted correctly percentage.see accuracy isn’t great either obvious already looking confusion matrix.just fit model evaluated data. doesn’t give us much information model performance. Let us instead split data, train evaluate part data. Since working data time component, natural fit model using first year’s worth data evaluate last year. closely match model used real life.Now split data Smarket_train Smarket_test can fit logistic regression model Smarket_train evaluate Smarket_test see well model generalizes.evaluate testing data set.see model likely predict \"\" rather \"\". Also, note model performs worse last model. expected since evaluating new data.recall logistic regression model underwhelming p-values. Let us see happens remove variables appear helpful might achieve predictive model since variables relationship response cause increase variance without decrease bias.see increase performance. model still perfect starting perform better.Suppose want predict returns associated particular values Lag1 Lag2. particular, want predict Direction day Lag1 Lag2 equal 1.2 1.1, respectively, day equal 1.5 −0.8.start creating tibble corresponding scenarios want predict forAnd use predict()","code":"\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket\n    )\n\nlr_fit## parsnip model object\n## \n## Fit time:  10ms \n## \n## Call:  stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Coefficients:\n## (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  \n##   -0.126000    -0.073074    -0.042301     0.011085     0.009359     0.010313  \n##      Volume  \n##    0.135441  \n## \n## Degrees of Freedom: 1249 Total (i.e. Null);  1243 Residual\n## Null Deviance:       1731 \n## Residual Deviance: 1728  AIC: 1742\nlr_fit %>%\n  pluck(\"fit\") %>%\n  summary()## \n## Call:\n## stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.446  -1.203   1.065   1.145   1.326  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.126000   0.240736  -0.523    0.601\n## Lag1        -0.073074   0.050167  -1.457    0.145\n## Lag2        -0.042301   0.050086  -0.845    0.398\n## Lag3         0.011085   0.049939   0.222    0.824\n## Lag4         0.009359   0.049974   0.187    0.851\n## Lag5         0.010313   0.049511   0.208    0.835\n## Volume       0.135441   0.158360   0.855    0.392\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1731.2  on 1249  degrees of freedom\n## Residual deviance: 1727.6  on 1243  degrees of freedom\n## AIC: 1741.6\n## \n## Number of Fisher Scoring iterations: 3\ntidy(lr_fit)## # A tibble: 7 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) -0.126      0.241     -0.523   0.601\n## 2 Lag1        -0.0731     0.0502    -1.46    0.145\n## 3 Lag2        -0.0423     0.0501    -0.845   0.398\n## 4 Lag3         0.0111     0.0499     0.222   0.824\n## 5 Lag4         0.00936    0.0500     0.187   0.851\n## 6 Lag5         0.0103     0.0495     0.208   0.835\n## 7 Volume       0.135      0.158      0.855   0.392\npredict(lr_fit, new_data = Smarket)## # A tibble: 1,250 × 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Down       \n##  3 Down       \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Down       \n##  8 Up         \n##  9 Up         \n## 10 Down       \n## # … with 1,240 more rows\npredict(lr_fit, new_data = Smarket, type = \"prob\")## # A tibble: 1,250 × 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.493    0.507\n##  2      0.519    0.481\n##  3      0.519    0.481\n##  4      0.485    0.515\n##  5      0.489    0.511\n##  6      0.493    0.507\n##  7      0.507    0.493\n##  8      0.491    0.509\n##  9      0.482    0.518\n## 10      0.511    0.489\n## # … with 1,240 more rows\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class)##           Truth\n## Prediction Down  Up\n##       Down  145 141\n##       Up    457 507\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\naugment(lr_fit, new_data = Smarket) %>%\n  accuracy(truth = Direction, estimate = .pred_class)## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.522\nSmarket_train <- Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- Smarket %>%\n  filter(Year == 2005)\nlr_fit2 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket_train\n    )\naugment(lr_fit2, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   77 97\n##       Up     34 44\naugment(lr_fit2, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.480\nlr_fit3 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2,\n    data = Smarket_train\n    )\n\naugment(lr_fit3, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lr_fit3, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560\nSmarket_new <- tibble(\n  Lag1 = c(1.2, 1.5), \n  Lag2 = c(1.1, -0.8)\n)\npredict(\n  lr_fit3,\n  new_data = Smarket_new, \n  type = \"prob\"\n)## # A tibble: 2 × 2\n##   .pred_Down .pred_Up\n##        <dbl>    <dbl>\n## 1      0.521    0.479\n## 2      0.504    0.496"},{"path":"classification.html","id":"linear-discriminant-analysis","chapter":"4 Classification","heading":"4.3 Linear Discriminant Analysis","text":"Now perform LDA Smarket data. use discrim_linear() function create LDA specification. continue use 2 predictors easy comparison.One things look LDA output group means. see slight difference means two groups. suggest tendency previous 2 days’ returns negative days market increases, tendency previous day’ returns positive days market declines.Predictions done just logistic regressionAnd can take look performance.see markedly difference performance model logistic regression model.","code":"\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nlda_fit <- lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nlda_fit## parsnip model object\n## \n## Fit time:  4ms \n## Call:\n## lda(Direction ~ Lag1 + Lag2, data = data)\n## \n## Prior probabilities of groups:\n##     Down       Up \n## 0.491984 0.508016 \n## \n## Group means:\n##             Lag1        Lag2\n## Down  0.04279022  0.03389409\n## Up   -0.03954635 -0.03132544\n## \n## Coefficients of linear discriminants:\n##             LD1\n## Lag1 -0.6420190\n## Lag2 -0.5135293\npredict(lda_fit, new_data = Smarket_test)## # A tibble: 252 × 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Up         \n##  3 Up         \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Up         \n##  8 Up         \n##  9 Up         \n## 10 Up         \n## # … with 242 more rows\npredict(lda_fit, new_data = Smarket_test, type = \"prob\")## # A tibble: 252 × 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.490    0.510\n##  2      0.479    0.521\n##  3      0.467    0.533\n##  4      0.474    0.526\n##  5      0.493    0.507\n##  6      0.494    0.506\n##  7      0.495    0.505\n##  8      0.487    0.513\n##  9      0.491    0.509\n## 10      0.484    0.516\n## # … with 242 more rows\naugment(lda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560"},{"path":"classification.html","id":"quadratic-discriminant-analysis","chapter":"4 Classification","heading":"4.4 Quadratic Discriminant Analysis","text":"now fit QDA model. discrim_quad() function used .model specification fitting model just like .seeing another increase accuracy. However model still rarely predicts \"'. make appear quadratic form assumed QDA captures relationship clearly.","code":"\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nqda_fit <- qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\naugment(qda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   30  20\n##       Up     81 121\naugment(qda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.599"},{"path":"classification.html","id":"naive-bayes","chapter":"4 Classification","heading":"4.5 Naive Bayes","text":"now fit Naive Bayes model Smarket data. , using naive_bayes() function create specification also set usekernel argument FALSE. means assuming predictors Lag1 Lag2 drawn Gaussian distributions.model specified, fitting process exactly like beforeOnce model fit, can create confusion matrix based testing data also assess model accuracy.accuracy Naive Bayes similar QDA model. seems reasonable since scatter plot shows apparent relationship Lag1 vs Lag2 thus Naive Bayes’ assumption independently distributed predictors unreasonable.","code":"\nnb_spec <- naive_Bayes() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"klaR\") %>% \n  set_args(usekernel = FALSE)  \n\nnb_fit <- nb_spec %>% \n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\naugment(nb_fit, new_data = Smarket_test) %>% \n  conf_mat(truth = Direction, estimate = .pred_class)##           Truth\n## Prediction Down  Up\n##       Down   28  20\n##       Up     83 121\naugment(nb_fit, new_data = Smarket_test) %>% \n  accuracy(truth = Direction, estimate = .pred_class)## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.591\nggplot(Smarket, aes(Lag1, Lag2)) +\n  geom_point(alpha = 0.1, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"No apparent correlation between Lag1 and Lag2\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"classification.html","id":"k-nearest-neighbors","chapter":"4 Classification","heading":"4.6 K-Nearest Neighbors","text":"Lastly let us take look K-Nearest Neighbors model. first model looked hyperparameter need specify. set 3 neighbors = 3. Fitting done like normal.evaluation done wayIt appears model performing well.try using K-nearest neighbors model application caravan insurance data. data set includes 85 predictors measure demographic characteristics 5822 individuals. response variable Purchase, indicates whether given individual purchases caravan insurance policy. data set, 6% people purchased caravan insurance.want build predictive model uses demographic characteristics predict whether individual going purchase caravan insurance. go , split data set training data set testing data set. (proper way done. See next chapter correct way.)Since using K-nearest neighbor model, importance variables centered scaled make sure variables uniform influence. can accomplish transformation step_normalize(), centering scaling one go.trying different values K see number neighbors affect model performance. workflow object created, just recipe added.Next create general KNN model specification.can use model specification along Caravan_wf create 3 full workflow objects K = 1,3,5.workflow specification can fit models one one.can calculate confusion matricies.appears model performance doesn’t change much changing 1 5.","code":"\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\nknn_fit <- knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nknn_fit## parsnip model object\n## \n## Fit time:  36ms \n## \n## Call:\n## kknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n## \n## Type of response variable: nominal\n## Minimal misclassification: 0.492986\n## Best kernel: optimal\n## Best k: 3\naugment(knn_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   43 58\n##       Up     68 83\naugment(knn_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary           0.5\nCaravan_test <- Caravan[seq_len(1000), ]\nCaravan_train <- Caravan[-seq_len(1000), ]\nrec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%\n  step_normalize(all_numeric_predictors())\nCaravan_wf <- workflow() %>%\n  add_recipe(rec_spec)\nknn_spec <- nearest_neighbor() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\nknn1_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 1))\n\nknn3_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 3))\n\nknn5_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 5))\nknn1_fit <- fit(knn1_wf, data = Caravan_train)\nknn3_fit <- fit(knn3_wf, data = Caravan_train)\nknn5_fit <- fit(knn5_wf, data = Caravan_train)\naugment(knn1_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9\naugment(knn3_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  875  50\n##        Yes  66   9\naugment(knn5_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9"},{"path":"classification.html","id":"poisson-regression","chapter":"4 Classification","heading":"4.7 Poisson Regression","text":"far using Smarket data set predict stock price movement. now shift new data set, Bikeshare, look number bike rentals per hour Washington, D.C.variable interest, number bike rentals per hour can take non-negative integer values. makes Poisson Regression suitable candidate model .start specifying model using poisson_reg() function.predicting bikers using following predictors:mnth - month year, coded factormnth - month year, coded factorhr - hour day, coded factor 0 23hr - hour day, coded factor 0 23workingday - workday? Already coded dummy variable Yes = 1, = 0workingday - workday? Already coded dummy variable Yes = 1, = 0temp - normalized temperature Celsiustemp - normalized temperature Celsiusweathersit - weather condition, coded factor following levels:\nclear\ncloudy/misty\nlight rain/snow\nheavy rain/snow\nweathersit - weather condition, coded factor following levels:clearcloudy/mistylight rain/snowheavy rain/snowAs can see, apart temp predictors categorical nature. Thus, first create recipe convert dummy variables bundle model spec recipe using workflow.workflow place, follow pattern fit model look predictions.can also look model coefficients get feel working model comparing understanding.Looking coefficients corresponding mnth variable, note lower winter months higher summer months. seems logical expect number bike rentals higher summertime.can similarly also look coefficients corresponding hr variable. peaks occur 8:00 5:00 PM, .e. normal office start end times.","code":"\npois_spec <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\npois_rec_spec <- recipe(\n  bikers ~ mnth + hr + workingday + temp + weathersit,\n  data = Bikeshare\n) %>% \n  step_dummy(all_nominal_predictors())\npois_wf <- workflow() %>% \n  add_recipe(pois_rec_spec) %>% \n  add_model(pois_spec)\npois_fit <- pois_wf %>% fit(data = Bikeshare)\n\naugment(pois_fit, new_data = Bikeshare, type.predict = \"response\") %>% \n  ggplot(aes(bikers, .pred)) +\n  geom_point(alpha = 0.1) +\n  geom_abline(slope = 1, size = 1, color = \"grey40\") +\n  labs(title = \"Predicting the number of bikers per hour using Poission Regression\",\n       x = \"Actual\", y = \"Predicted\")\npois_fit_coef_mnths <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^mnth\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"mnth_\", \"\"),\n    term = forcats::fct_inorder(term)\n  ) \n\npois_fit_coef_mnths %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"Month\", y = \"Coefficient\")\npois_fit_coef_hr <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^hr\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"hr_X\", \"\"),\n    term = forcats::fct_inorder(term)\n  )\n\npois_fit_coef_hr %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"Month\", y = \"Coefficient\")"},{"path":"classification.html","id":"extra---comparing-multiple-models","chapter":"4 Classification","heading":"4.8 Extra - comparing multiple models","text":"section new part ISLR. fitted lot different models lab. able calculate performance metrics one one, ideal want compare different models. example can conveniently calculate performance metrics multiple models time.Start creating named list fitted models want evaluate. made sure include models fitted parameters make easier compare .Next use imap_dfr() purrr package apply augment() models using testing data set. .id = \"model\" creates column named \"model\" added resulting tibble using names models.seen use accuracy() lot times now, metric use classification, yardstick provides many .\ncan combine multiple different metrics together metric_set()resulting function can applied calculate multiple metrics time. yardstick works grouped tibbles calling group_by(model) can calculate metrics models one go.technique can used create ROC curves.can’t see LDA lies perfectly logistic regression.","code":"\nmodels <- list(\"logistic regression\" = lr_fit3,\n               \"LDA\" = lda_fit,\n               \"QDA\" = qda_fit,\n               \"KNN\" = knn_fit)\npreds <- imap_dfr(models, augment, \n                  new_data = Smarket_test, .id = \"model\")\n\npreds %>%\n  select(model, Direction, .pred_class, .pred_Down, .pred_Up)## # A tibble: 1,008 × 5\n##    model               Direction .pred_class .pred_Down .pred_Up\n##    <chr>               <fct>     <fct>            <dbl>    <dbl>\n##  1 logistic regression Down      Up               0.490    0.510\n##  2 logistic regression Down      Up               0.479    0.521\n##  3 logistic regression Down      Up               0.467    0.533\n##  4 logistic regression Up        Up               0.474    0.526\n##  5 logistic regression Down      Up               0.493    0.507\n##  6 logistic regression Up        Up               0.494    0.506\n##  7 logistic regression Down      Up               0.495    0.505\n##  8 logistic regression Up        Up               0.487    0.513\n##  9 logistic regression Down      Up               0.491    0.509\n## 10 logistic regression Up        Up               0.484    0.516\n## # … with 998 more rows\nmulti_metric <- metric_set(accuracy, sensitivity, specificity)\npreds %>%\n  group_by(model) %>%\n  multi_metric(truth = Direction, estimate = .pred_class)## # A tibble: 12 × 4\n##    model               .metric  .estimator .estimate\n##    <chr>               <chr>    <chr>          <dbl>\n##  1 KNN                 accuracy binary         0.5  \n##  2 LDA                 accuracy binary         0.560\n##  3 logistic regression accuracy binary         0.560\n##  4 QDA                 accuracy binary         0.599\n##  5 KNN                 sens     binary         0.387\n##  6 LDA                 sens     binary         0.315\n##  7 logistic regression sens     binary         0.315\n##  8 QDA                 sens     binary         0.270\n##  9 KNN                 spec     binary         0.589\n## 10 LDA                 spec     binary         0.752\n## 11 logistic regression spec     binary         0.752\n## 12 QDA                 spec     binary         0.858\npreds %>%\n  group_by(model) %>%\n  roc_curve(Direction, .pred_Down) %>%\n  autoplot()"},{"path":"resampling-methods.html","id":"resampling-methods","chapter":"5 Resampling Methods","heading":"5 Resampling Methods","text":"lab show us perform different resampling techniques. tasks quite general useful many different areas. bootstrap example. chapter introduces lot new packages.\nchapter bring rsample view creating resampled data frames well yardstick calculate performance metrics. Lastly, also use tune fit models within said resamples. also see use dials used together tune select hyperparameter tuning values.","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\n\nAuto <- tibble(Auto)\nPortfolio <- tibble(Portfolio)"},{"path":"resampling-methods.html","id":"the-validation-set-approach","chapter":"5 Resampling Methods","heading":"5.1 The Validation Set Approach","text":"fitting model often desired able calculate performance metric quantify well model fits data. model evaluated data fit quite likely get -optimistic results. therefore split data testing training. way can fit model data evaluate similar.Splitting data done using random sampling, advised set seed splitting assure can reproduce results.\ninitial_split() function takes data.frame returns rsplit object. object contains information observations belong data set, testing, training. normally set proportion data used training much used evaluation. set using prop argument set 0.5 closely match happened ISLR. ’m also setting strata argument. argument makes sure sides split roughly distribution value strata. numeric variable passed strata binned distributions matched within bins.testing training data sets can materialized using testing() training() functions respectively.looking Auto_train Auto_test see lengths match expect.Now train-test split let us fit models evaluate performance. move important reiterate use testing data set ! looked performance testing data set modify models. might overfit model due data leakage.modeling goal predict mpg horsepower using simple linear regression model, polynomial regression model.\nFirst, set linear regression specification.fit like normal. Note fitting using Auto_train.can now use augment() function extract prediction rmse() calculate root mean squared error. testing RMSE since evaluating Auto_test.get RMSE 5.0583165. particular value going vary depending seed number picked since random sampling used splitting data set slightly different.Using framework makes easy us calculate training RMSEComparing two values can give us look generalizable model data hasn’t seen . expect training RMSE lower testing RMSE see large difference indication overfitting shift training data set testing data set. don’t expect shift since data sets created random sampling.Next fit polynomial regression model. can use linear model specification lm_spec add preprocessing unit recipe() step_poly() create polynomial expansion horsepower. can combine two workflow() create workflow object.can now fit model. remember fit training data set Auto_train.testing RMSE calculated asWhich little bit lower. appear just , polynomial regression model better fit. Note making decisions using testing performance metrics, training performance metrics.Lastly, show changing seed results slightly different estimate.","code":"\nset.seed(1)\nAuto_split <- initial_split(Auto, strata = mpg, prop = 0.5)\nAuto_split## <Analysis/Assess/Total>\n## <194/198/392>\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\nAuto_train## # A tibble: 194 × 9\n##      mpg cylinders displacement horsepower weight acceleration  year origin\n##    <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>  <dbl>\n##  1    15         8          350        165   3693         11.5    70      1\n##  2    16         8          304        150   3433         12      70      1\n##  3    14         8          440        215   4312          8.5    70      1\n##  4    14         8          455        225   4425         10      70      1\n##  5    10         8          307        200   4376         15      70      1\n##  6    17         6          250        100   3329         15.5    71      1\n##  7    14         8          400        175   4464         11.5    71      1\n##  8    14         8          351        153   4154         13.5    71      1\n##  9    14         8          318        150   4096         13      71      1\n## 10    13         8          400        170   4746         12      71      1\n## # … with 184 more rows, and 1 more variable: name <fct>\nAuto_test## # A tibble: 198 × 9\n##      mpg cylinders displacement horsepower weight acceleration  year origin\n##    <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>  <dbl>\n##  1    18         8          318        150   3436         11      70      1\n##  2    17         8          302        140   3449         10.5    70      1\n##  3    15         8          429        198   4341         10      70      1\n##  4    14         8          454        220   4354          9      70      1\n##  5    15         8          390        190   3850          8.5    70      1\n##  6    15         8          383        170   3563         10      70      1\n##  7    14         8          340        160   3609          8      70      1\n##  8    15         8          400        150   3761          9.5    70      1\n##  9    14         8          455        225   3086         10      70      1\n## 10    22         6          198         95   2833         15.5    70      1\n## # … with 188 more rows, and 1 more variable: name <fct>\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\nlm_fit <- lm_spec %>% \n  fit(mpg ~ horsepower, data = Auto_train)\naugment(lm_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        5.06\naugment(lm_fit, new_data = Auto_train) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.74\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = 2)\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\npoly_wf## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\npoly_fit <- fit(poly_wf, data = Auto_train)\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.37\nset.seed(2)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\n\npoly_fit <- fit(poly_wf, data = Auto_train)\n\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.35"},{"path":"resampling-methods.html","id":"leave-one-out-cross-validation","chapter":"5 Resampling Methods","heading":"5.2 Leave-One-Out Cross-Validation","text":"Leave-One-Cross-Validation integrated broader tidymodels framework. information read .","code":""},{"path":"resampling-methods.html","id":"k-fold-cross-validation","chapter":"5 Resampling Methods","heading":"5.3 k-Fold Cross-Validation","text":"Earlier set degree = 2 create second-degree polynomial regression model. suppose want find best value degree yields “closest” fit. known hyperparameter tuning case can use k-Fold Cross-Validation. use k-Fold Cross-Validation using tune package, need 3 things get working:parsnip/workflow object one arguments marked tuning,vfold_cv rsample object cross-validation resamples,tibble denoting values hyperparameter values explored.hyperparameter tuning just one parameter, namely degree argument step_poly(). Creating new recipe degree = tune() indicated intend degree tuned.means able fit workflow right now value degree unspecified, try get error:next thing need create k-Fold data set. can done using vfold_cv() function. Note function uses v instead k terminology ISLR. set v = 10 common choice k.result tibble vfold_splits quite similar rsplit object saw earlier.last thing need tibble possible values want explore. tunable parameters tidymodels associated function dials package. need use degree() function , extend range max 10. dials function passed grid_regular() create regular grid values.Using grid_regular() little overkill application since following code provide result. multiple parameters want tune makes sure everything check properly named.Now necessary objects created can pass tune_grid() fit models within fold value specified degree_grid.can helpful add control = control_grid(verbose = TRUE), print progress. Especially helpful models take fit. tune_res isn’t easily readable. Luckily tune provides handful helper functions.autoplot() gives visual overview performance different hyperparameter pairs.appears biggest jump performance comes going degree = 2. Afterward, might little bit improvement isn’t obvious.number used plotting can extracted directly collect_metrics(). also get estimate standard error performance metric. get since 10 different estimates, one fold.can also use show_best() show best performing models.see performance plateaued degree = 2. couple function select models sophisticated rules. select_by_one_std_err() select_by_pct_loss(). use select_by_one_std_err() selects simple model within one standard error numerically optimal results. need specify degree tell select_by_one_std_err() direction simple.want touse desc(you_model_parameter) larger values lead simpler modeluse you_model_parameter smaller values lead simpler modellower polynomials models simpler ditch desc().selected degree = 2. use value since simpler models sometimes can beneficial. Especially want explain happens .selected value can now used specify previous unspecified degree argument poly_wf using finalize_workflow().workflow can now fitted. want make sure fit full training data set.","code":"\npoly_tuned_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = tune())\n\npoly_tuned_wf <- workflow() %>%\n  add_recipe(poly_tuned_rec) %>%\n  add_model(lm_spec)\nfit(poly_tuned_wf, data = Auto_train)## Error: You cannot `prep()` a tuneable recipe. Argument(s) with `tune()`: 'degree'. Do you want to use a tuning function such as `tune_grid()`?\nAuto_folds <- vfold_cv(Auto_train, v = 10)\nAuto_folds## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##    splits           id    \n##    <list>           <chr> \n##  1 <split [264/30]> Fold01\n##  2 <split [264/30]> Fold02\n##  3 <split [264/30]> Fold03\n##  4 <split [264/30]> Fold04\n##  5 <split [265/29]> Fold05\n##  6 <split [265/29]> Fold06\n##  7 <split [265/29]> Fold07\n##  8 <split [265/29]> Fold08\n##  9 <split [265/29]> Fold09\n## 10 <split [265/29]> Fold10\ndegree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)\ndegree_grid <- tibble(degree = seq(1, 10))\ntune_res <- tune_grid(\n  object = poly_tuned_wf, \n  resamples = Auto_folds, \n  grid = degree_grid\n)\nautoplot(tune_res)\ncollect_metrics(tune_res)## # A tibble: 20 × 7\n##    degree .metric .estimator  mean     n std_err .config              \n##     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1      1 rmse    standard   4.81     10  0.172  Preprocessor01_Model1\n##  2      1 rsq     standard   0.621    10  0.0316 Preprocessor01_Model1\n##  3      2 rmse    standard   4.37     10  0.209  Preprocessor02_Model1\n##  4      2 rsq     standard   0.677    10  0.0436 Preprocessor02_Model1\n##  5      3 rmse    standard   4.40     10  0.217  Preprocessor03_Model1\n##  6      3 rsq     standard   0.675    10  0.0446 Preprocessor03_Model1\n##  7      4 rmse    standard   4.43     10  0.218  Preprocessor04_Model1\n##  8      4 rsq     standard   0.670    10  0.0453 Preprocessor04_Model1\n##  9      5 rmse    standard   4.42     10  0.203  Preprocessor05_Model1\n## 10      5 rsq     standard   0.674    10  0.0436 Preprocessor05_Model1\n## 11      6 rmse    standard   4.41     10  0.189  Preprocessor06_Model1\n## 12      6 rsq     standard   0.670    10  0.0423 Preprocessor06_Model1\n## 13      7 rmse    standard   4.40     10  0.176  Preprocessor07_Model1\n## 14      7 rsq     standard   0.670    10  0.0420 Preprocessor07_Model1\n## 15      8 rmse    standard   4.41     10  0.175  Preprocessor08_Model1\n## 16      8 rsq     standard   0.670    10  0.0420 Preprocessor08_Model1\n## 17      9 rmse    standard   4.47     10  0.207  Preprocessor09_Model1\n## 18      9 rsq     standard   0.663    10  0.0445 Preprocessor09_Model1\n## 19     10 rmse    standard   4.50     10  0.227  Preprocessor10_Model1\n## 20     10 rsq     standard   0.658    10  0.0465 Preprocessor10_Model1\nshow_best(tune_res, metric = \"rmse\")## # A tibble: 5 × 7\n##   degree .metric .estimator  mean     n std_err .config              \n##    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n## 1      2 rmse    standard    4.37    10   0.209 Preprocessor02_Model1\n## 2      3 rmse    standard    4.40    10   0.217 Preprocessor03_Model1\n## 3      7 rmse    standard    4.40    10   0.176 Preprocessor07_Model1\n## 4      6 rmse    standard    4.41    10   0.189 Preprocessor06_Model1\n## 5      8 rmse    standard    4.41    10   0.175 Preprocessor08_Model1\nselect_by_one_std_err(tune_res, degree, metric = \"rmse\")## # A tibble: 1 × 9\n##   degree .metric .estimator  mean     n std_err .config             .best .bound\n##    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               <dbl>  <dbl>\n## 1      2 rmse    standard    4.37    10   0.209 Preprocessor02_Mod…  4.37   4.58\nbest_degree <- select_by_one_std_err(tune_res, degree, metric = \"rmse\")\nfinal_wf <- finalize_workflow(poly_wf, best_degree)\n\nfinal_wf## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nfinal_fit <- fit(final_wf, Auto_train)\n\nfinal_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##       (Intercept)  horsepower_poly_1  horsepower_poly_2  \n##             23.34            -104.85              34.39"},{"path":"resampling-methods.html","id":"the-bootstrap","chapter":"5 Resampling Methods","heading":"5.4 The Bootstrap","text":"section illustrates use bootstrap simple Section 5.2 ISLR, well example involving estimating accuracy linear regression model Auto data set.First, want look accuracy statistic interest. statistic justified ISLR. want calculate metric within many different bootstraps. start calculating 1000 bootstraps Portfolio data set.result tibble boot_split objects. rsample constructed splits way 1000 bootstraps take way less 1000 times space Portfolio.Next, create function takes boot_split object returns calculated metric.Now can use mutate() map_dbl() dplyr purrr respectively apply alpha.fn bootstraps.now bootstrap sample values. can now analyzed.next example want study variability slope intercept estimate linear regression model. follows structure. First, create bootstraps data. create function takes split returns values. function return tibble bootstrap.use mutate() map() apply function bootstraps.can now unnest() use group_by() summarise() get estimate variability slope intercept linear regression model.","code":"\nPortfolio_boots <- bootstraps(Portfolio, times = 1000)\nPortfolio_boots## # Bootstrap sampling \n## # A tibble: 1,000 × 2\n##    splits           id           \n##    <list>           <chr>        \n##  1 <split [100/36]> Bootstrap0001\n##  2 <split [100/39]> Bootstrap0002\n##  3 <split [100/39]> Bootstrap0003\n##  4 <split [100/33]> Bootstrap0004\n##  5 <split [100/39]> Bootstrap0005\n##  6 <split [100/34]> Bootstrap0006\n##  7 <split [100/40]> Bootstrap0007\n##  8 <split [100/38]> Bootstrap0008\n##  9 <split [100/36]> Bootstrap0009\n## 10 <split [100/41]> Bootstrap0010\n## # … with 990 more rows\nalpha.fn <- function(split) {\n  data <- analysis(split)\n  X <- data$X\n  Y <- data$Y\n  \n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\nalpha_res <- Portfolio_boots %>%\n  mutate(alpha = map_dbl(splits, alpha.fn))\n\nalpha_res## # Bootstrap sampling \n## # A tibble: 1,000 × 3\n##    splits           id            alpha\n##    <list>           <chr>         <dbl>\n##  1 <split [100/36]> Bootstrap0001 0.516\n##  2 <split [100/39]> Bootstrap0002 0.687\n##  3 <split [100/39]> Bootstrap0003 0.599\n##  4 <split [100/33]> Bootstrap0004 0.556\n##  5 <split [100/39]> Bootstrap0005 0.549\n##  6 <split [100/34]> Bootstrap0006 0.619\n##  7 <split [100/40]> Bootstrap0007 0.387\n##  8 <split [100/38]> Bootstrap0008 0.675\n##  9 <split [100/36]> Bootstrap0009 0.538\n## 10 <split [100/41]> Bootstrap0010 0.407\n## # … with 990 more rows\nAuto_boots <- bootstraps(Auto)\n\nboot.fn <- function(split) {\n  lm_fit <- lm_spec %>% fit(mpg ~ horsepower, data = analysis(split))\n  tidy(lm_fit)\n}\nboot_res <- Auto_boots %>%\n  mutate(models = map(splits, boot.fn))\nboot_res %>%\n  unnest(cols = c(models)) %>%\n  group_by(term) %>%\n  summarise(mean = mean(estimate),\n            sd = sd(estimate))## # A tibble: 2 × 3\n##   term          mean      sd\n##   <chr>        <dbl>   <dbl>\n## 1 (Intercept) 39.8   0.759  \n## 2 horsepower  -0.156 0.00593"},{"path":"linear-model-selection-and-regularization.html","id":"linear-model-selection-and-regularization","chapter":"6 Linear Model Selection and Regularization","heading":"6 Linear Model Selection and Regularization","text":"lab take look regularization models hyperparameter tuning. models related models saw chapter 3 4, difference contain regularization term.\nchapter use parsnip model fitting recipes workflows perform transformations, tune dials tune hyperparameters model.using Hitters data set ISLR package. wish predict baseball players Salary based several different characteristics included data set. Since wish predict Salary, need remove missing data column. Otherwise, won’t able run models.","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\n\nHitters <- as_tibble(Hitters) %>%\n  filter(!is.na(Salary))"},{"path":"linear-model-selection-and-regularization.html","id":"best-subset-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.1 Best Subset Selection","text":"tidymodels currently support subset selection methods, unlikely include near future.","code":""},{"path":"linear-model-selection-and-regularization.html","id":"forward-and-backward-stepwise-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.2 Forward and Backward Stepwise Selection","text":"tidymodels currently support forward backward stepwise selection methods, unlikely include near future.","code":""},{"path":"linear-model-selection-and-regularization.html","id":"ridge-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.3 Ridge Regression","text":"use glmnet package perform ridge regression. parsnip dedicated function create ridge regression model specification. need use linear_reg() set mixture = 0 specify ridge model. mixture argument specifies amount different types regularization, mixture = 0 specifies ridge regularization mixture = 1 specifies lasso regularization. Setting mixture value 0 1 lets us use . using glmnet engine also need set penalty able fit model. set value 0 now, best value, look select best value little bit.specification created can fit data. use predictors.glmnet package fit model values penalty , let us see parameter estimate model now penalty = 0.Let us instead see estimates penalty 11498.Notice estimates decreasing amount penalty goes . Look parameter estimates penalty = 705 penalty = 50.can visualize magnitude coefficients regularized towards zero penalty goes .Prediction done like normal, use predict() , penalty = 0 set model specification used.can also get predictions values penalty specifying predict()saw can fit ridge model make predictions different values penalty. nice find “best” value penalty. something can use hyperparameter tuning . Hyperparameter tuning simplest form way fitting many models different sets hyperparameters trying find one performs “best.” complexity hyperparameter tuning can come try different models. keep simple lab look grid search, looking evenly spaced parameter values. fine enough approach one two tunable parameters can become computationally infeasible. See chapter iterative search Tidy Modeling R information.start like normal setting validation split. K-fold cross-validation data set created training data set 10 folds.can use tune_grid() function perform hyperparameter tuning using grid search. tune_grid() needs 3 different thing;workflow object containing model preprocessor,rset object containing resamples workflow fitted within, anda tibble containing parameter values evaluated.Optionally metric set performance metrics can supplied evaluation. don’t set one default set performance metrics used.already resample object created Hitters_fold. Now create workflow specification next.just used data set fit model earlier. ridge regression scale sensitive need make sure variables scale. can use step_normalize(). Secondly let us deal factor variables ourself using step_novel() step_dummy().model specification look similar seen earlier, set penalty = tune(). tells tune_grid() penalty parameter tuned.Now combine create workflow object.last thing need values penalty trying. can created using grid_regular() creates grid evenly spaces parameter values. use penalty() function dials package denote parameter set range grid searching . Note range log-scaled.Using 50 levels one parameter might seem overkill many applications . remember glmnet fits models one go adding levels penalty doesn’t affect computational speed much.Now everything need can fit models.output tune_grid() can hard read unprocessed. autoplot() creates great visualization\nsee amount regularization affects performance metrics differently. Note areas amount regularization doesn’t meaningful influence coefficient estimates. can also see raw metrics created chart calling collect_matrics().“best” values can selected using select_best(), function requires specify matric select .value penalty can used finalize_workflow() update/finalize recipe replacing tune() value best_penalty. Now, model fit , time using whole training data set.final model can now applied testing data set validate performanceAnd performs fairly well given saw earlier.","code":"\nridge_spec <- linear_reg(mixture = 0, penalty = 0) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\")\nridge_fit <- fit(ridge_spec, Salary ~ ., data = Hitters)\ntidy(ridge_fit)## # A tibble: 20 × 3\n##    term          estimate penalty\n##    <chr>            <dbl>   <dbl>\n##  1 (Intercept)   81.1           0\n##  2 AtBat         -0.682         0\n##  3 Hits           2.77          0\n##  4 HmRun         -1.37          0\n##  5 Runs           1.01          0\n##  6 RBI            0.713         0\n##  7 Walks          3.38          0\n##  8 Years         -9.07          0\n##  9 CAtBat        -0.00120       0\n## 10 CHits          0.136         0\n## 11 CHmRun         0.698         0\n## 12 CRuns          0.296         0\n## 13 CRBI           0.257         0\n## 14 CWalks        -0.279         0\n## 15 LeagueN       53.2           0\n## 16 DivisionW   -123.            0\n## 17 PutOuts        0.264         0\n## 18 Assists        0.170         0\n## 19 Errors        -3.69          0\n## 20 NewLeagueN   -18.1           0\ntidy(ridge_fit, penalty = 11498)## # A tibble: 20 × 3\n##    term         estimate penalty\n##    <chr>           <dbl>   <dbl>\n##  1 (Intercept) 407.        11498\n##  2 AtBat         0.0370    11498\n##  3 Hits          0.138     11498\n##  4 HmRun         0.525     11498\n##  5 Runs          0.231     11498\n##  6 RBI           0.240     11498\n##  7 Walks         0.290     11498\n##  8 Years         1.11      11498\n##  9 CAtBat        0.00314   11498\n## 10 CHits         0.0117    11498\n## 11 CHmRun        0.0876    11498\n## 12 CRuns         0.0234    11498\n## 13 CRBI          0.0242    11498\n## 14 CWalks        0.0250    11498\n## 15 LeagueN       0.0866    11498\n## 16 DivisionW    -6.23      11498\n## 17 PutOuts       0.0165    11498\n## 18 Assists       0.00262   11498\n## 19 Errors       -0.0206    11498\n## 20 NewLeagueN    0.303     11498\ntidy(ridge_fit, penalty = 705)## # A tibble: 20 × 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)  54.4        705\n##  2 AtBat         0.112      705\n##  3 Hits          0.656      705\n##  4 HmRun         1.18       705\n##  5 Runs          0.937      705\n##  6 RBI           0.847      705\n##  7 Walks         1.32       705\n##  8 Years         2.58       705\n##  9 CAtBat        0.0108     705\n## 10 CHits         0.0468     705\n## 11 CHmRun        0.338      705\n## 12 CRuns         0.0937     705\n## 13 CRBI          0.0979     705\n## 14 CWalks        0.0718     705\n## 15 LeagueN      13.7        705\n## 16 DivisionW   -54.7        705\n## 17 PutOuts       0.119      705\n## 18 Assists       0.0161     705\n## 19 Errors       -0.704      705\n## 20 NewLeagueN    8.61       705\ntidy(ridge_fit, penalty = 50)## # A tibble: 20 × 3\n##    term          estimate penalty\n##    <chr>            <dbl>   <dbl>\n##  1 (Intercept)   48.2          50\n##  2 AtBat         -0.354        50\n##  3 Hits           1.95         50\n##  4 HmRun         -1.29         50\n##  5 Runs           1.16         50\n##  6 RBI            0.809        50\n##  7 Walks          2.71         50\n##  8 Years         -6.20         50\n##  9 CAtBat         0.00609      50\n## 10 CHits          0.107        50\n## 11 CHmRun         0.629        50\n## 12 CRuns          0.217        50\n## 13 CRBI           0.215        50\n## 14 CWalks        -0.149        50\n## 15 LeagueN       45.9          50\n## 16 DivisionW   -118.           50\n## 17 PutOuts        0.250        50\n## 18 Assists        0.121        50\n## 19 Errors        -3.28         50\n## 20 NewLeagueN    -9.42         50\nridge_fit %>%\n  extract_fit_engine() %>%\n  plot(xvar = \"lambda\")\npredict(ridge_fit, new_data = Hitters)## # A tibble: 263 × 1\n##     .pred\n##     <dbl>\n##  1  442. \n##  2  676. \n##  3 1059. \n##  4  521. \n##  5  543. \n##  6  218. \n##  7   74.7\n##  8   96.1\n##  9  809. \n## 10  865. \n## # … with 253 more rows\npredict(ridge_fit, new_data = Hitters, penalty = 500)## # A tibble: 263 × 1\n##    .pred\n##    <dbl>\n##  1  525.\n##  2  620.\n##  3  895.\n##  4  425.\n##  5  589.\n##  6  179.\n##  7  147.\n##  8  187.\n##  9  841.\n## 10  840.\n## # … with 253 more rows\nHitters_split <- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train <- training(Hitters_split)\nHitters_test <- testing(Hitters_split)\n\nHitters_fold <- vfold_cv(Hitters_train, v = 10)\nridge_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\nridge_spec <- \n  linear_reg(penalty = tune(), mixture = 0) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\")\nridge_workflow <- workflow() %>% \n  add_recipe(ridge_recipe) %>% \n  add_model(ridge_spec)\npenalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\npenalty_grid## # A tibble: 50 × 1\n##      penalty\n##        <dbl>\n##  1 0.00001  \n##  2 0.0000160\n##  3 0.0000256\n##  4 0.0000409\n##  5 0.0000655\n##  6 0.000105 \n##  7 0.000168 \n##  8 0.000268 \n##  9 0.000429 \n## 10 0.000687 \n## # … with 40 more rows\ntune_res <- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\ntune_res## # Tuning results\n## # 10-fold cross-validation \n## # A tibble: 10 × 4\n##    splits           id     .metrics           .notes          \n##    <list>           <chr>  <list>             <list>          \n##  1 <split [176/20]> Fold01 <tibble [100 × 5]> <tibble [0 × 1]>\n##  2 <split [176/20]> Fold02 <tibble [100 × 5]> <tibble [0 × 1]>\n##  3 <split [176/20]> Fold03 <tibble [100 × 5]> <tibble [0 × 1]>\n##  4 <split [176/20]> Fold04 <tibble [100 × 5]> <tibble [0 × 1]>\n##  5 <split [176/20]> Fold05 <tibble [100 × 5]> <tibble [0 × 1]>\n##  6 <split [176/20]> Fold06 <tibble [100 × 5]> <tibble [0 × 1]>\n##  7 <split [177/19]> Fold07 <tibble [100 × 5]> <tibble [0 × 1]>\n##  8 <split [177/19]> Fold08 <tibble [100 × 5]> <tibble [0 × 1]>\n##  9 <split [177/19]> Fold09 <tibble [100 × 5]> <tibble [0 × 1]>\n## 10 <split [177/19]> Fold10 <tibble [100 × 5]> <tibble [0 × 1]>\nautoplot(tune_res)\ncollect_metrics(tune_res)## # A tibble: 100 × 7\n##      penalty .metric .estimator    mean     n std_err .config              \n##        <dbl> <chr>   <chr>        <dbl> <int>   <dbl> <chr>                \n##  1 0.00001   rmse    standard   340.       10 32.7    Preprocessor1_Model01\n##  2 0.00001   rsq     standard     0.473    10  0.0525 Preprocessor1_Model01\n##  3 0.0000160 rmse    standard   340.       10 32.7    Preprocessor1_Model02\n##  4 0.0000160 rsq     standard     0.473    10  0.0525 Preprocessor1_Model02\n##  5 0.0000256 rmse    standard   340.       10 32.7    Preprocessor1_Model03\n##  6 0.0000256 rsq     standard     0.473    10  0.0525 Preprocessor1_Model03\n##  7 0.0000409 rmse    standard   340.       10 32.7    Preprocessor1_Model04\n##  8 0.0000409 rsq     standard     0.473    10  0.0525 Preprocessor1_Model04\n##  9 0.0000655 rmse    standard   340.       10 32.7    Preprocessor1_Model05\n## 10 0.0000655 rsq     standard     0.473    10  0.0525 Preprocessor1_Model05\n## # … with 90 more rows\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nbest_penalty## # A tibble: 1 × 2\n##   penalty .config              \n##     <dbl> <chr>                \n## 1    569. Preprocessor1_Model39\nridge_final <- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit <- fit(ridge_final, data = Hitters_train)\naugment(ridge_final_fit, new_data = Hitters_test) %>%\n  rsq(truth = Salary, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.488"},{"path":"linear-model-selection-and-regularization.html","id":"the-lasso","chapter":"6 Linear Model Selection and Regularization","heading":"6.4 The Lasso","text":"use glmnet package perform lasso regression. parsnip dedicated function create ridge regression model specification. need use linear_reg() set mixture = 1 specify lasso model. mixture argument specifies amount different types regularization, mixture = 0 specifies ridge regularization mixture = 1 specifies lasso regularization. Setting mixture value 0 1 lets us use .following procedure similar saw ridge regression section. preprocessing needed , let us write one time.Next, finish lasso regression workflow.different kind regularization still use penalty argument. picked different range values penalty since know good range. practice cast wide net first narrow range interest.can use tune_grid() .select best value penalty using select_best()refit using whole training data set.done, calculating rsq value lasso model can see data doesn’t make much difference kind regularization use similar performance.","code":"\nlasso_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nlasso_workflow <- workflow() %>% \n  add_recipe(lasso_recipe) %>% \n  add_model(lasso_spec)\npenalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)\ntune_res <- tune_grid(\n  lasso_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nautoplot(tune_res)\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nlasso_final <- finalize_workflow(lasso_workflow, best_penalty)\n\nlasso_final_fit <- fit(lasso_final, data = Hitters_train)\naugment(ridge_final_fit, new_data = Hitters_test) %>%\n  rsq(truth = Salary, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.488"},{"path":"linear-model-selection-and-regularization.html","id":"principal-components-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.5 Principal Components Regression","text":"talk principal components analysis chapter 10. section show principal components can used dimensionality reduction preprocessing step.treat principal component regression linear model PCA transformations preprocessing. using tidymodels framework still mostly one model.preprocessing recipe closely resemble recipe saw ridge lasso sections. main difference end recipe step_pca() perform principal component analysis predictors, return components explain threshold percent variance. set threshold = tune() can treat threshold hyperparameter tuned. using workflows tune together can tune parameters preprocessing well parameters models.create smaller grid threshold don’t need modify range since [0, 1] acceptable range.now fit using tune_grid(). time actually perform 100 fits since need fit model value threshold within fold.results look little shaky .can still select best model.fit model much like done couple times now. workflow finalized using value selected select_best(), training using full training data set.","code":"\nlm_spec <- \n  linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\npca_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), threshold = tune())\n\npca_workflow <- \n  workflow() %>% \n  add_recipe(pca_recipe) %>% \n  add_model(lm_spec)\nthreshold_grid <- grid_regular(threshold(), levels = 10)\nthreshold_grid## # A tibble: 10 × 1\n##    threshold\n##        <dbl>\n##  1     0    \n##  2     0.111\n##  3     0.222\n##  4     0.333\n##  5     0.444\n##  6     0.556\n##  7     0.667\n##  8     0.778\n##  9     0.889\n## 10     1\ntune_res <- tune_grid(\n  pca_workflow,\n  resamples = Hitters_fold, \n  grid = threshold_grid\n)\nautoplot(tune_res)\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\npca_final <- finalize_workflow(pca_workflow, best_threshold)\n\npca_final_fit <- fit(pca_final, data = Hitters_train)"},{"path":"linear-model-selection-and-regularization.html","id":"partial-least-squares","chapter":"6 Linear Model Selection and Regularization","heading":"6.6 Partial Least Squares","text":"Lastly, partial least squares model. treat much like PCA section say partial least squares calculations done preprocessing tune. following code almost identical previous chapters shown full without many explanations avoid repetition. skipped section, go back read previous sections commentary.","code":"\npls_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pls(all_predictors(), num_comp = tune(), outcome = \"Salary\")\n\nlm_spec <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") \n\npls_workflow <- workflow() %>% \n  add_recipe(pls_recipe) %>% \n  add_model(lm_spec) \n\nnum_comp_grid <- grid_regular(num_comp(c(1, 20)), levels = 10)\n\ntune_res <- tune_grid(\n  pls_workflow,\n  resamples = Hitters_fold, \n  grid = num_comp_grid\n)\n\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\n\npls_final <- finalize_workflow(pls_workflow, best_threshold)\n\npls_final_fit <- fit(pls_final, data = Hitters_train)"},{"path":"moving-beyond-linearity.html","id":"moving-beyond-linearity","chapter":"7 Moving Beyond Linearity","heading":"7 Moving Beyond Linearity","text":"lab look various ways can introduce non-linearity model preprocessing. Methods include; polynomials expansion, step functions, splines.GAM section WIP since now supported parsnip.chapter use parsnip model fitting recipes workflows perform transformations.","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\n\nWage <- as_tibble(Wage)"},{"path":"moving-beyond-linearity.html","id":"polynomial-regression-and-step-functions","chapter":"7 Moving Beyond Linearity","heading":"7.1 Polynomial Regression and Step Functions","text":"Polynomial regression can thought polynomial expansion variable passing expansion linear regression model. explicit formulation chapter. step_poly() allows us polynomial expansion one variables.following step take age replace variables age, age^2, age^3, age^4 since set degree = 4.recipe combined linear regression specification combined create workflow object.object can now fit()cal pull coefficients using tidy()lying said step_poly() returned age, age^2, age^3, age^4. happening returns variables basis orthogonal polynomials, means columns linear combination variables age, age^2, age^3, age^4. can see using poly() directly raw = FALSE since defaultWe see variables don’t directly format assumed. still well-reasoned transformation.\ncan get raw polynomial transformation setting raw = TRUEThese transformations align expect. still recommended stick default raw = FALSE unless reason .\nOne benefits using raw = FALSE resulting variables uncorrelated desirable quality using linear regression model.can get raw polynomials setting options = list(raw = TRUE) step_poly()Let us try something new visualize polynomial fit data. can easily 1 predictor 1 response. Starting creating tibble different ranges age. take tibble predict , give us repression curve. additionally adding confidence intervals setting type = \"conf_int\" can since using linear regression model.use ggplot2 visualize fitted line confidence interval. green line regression curve dashed blue lines confidence interval.regression curve now curve instead line gotten simple linear regression model. Notice furthermore confidence bands tighter lot data wider towards ends data.Let us take one step see happens regression line go past domain trained . previous plot showed individuals within age range 18-80. Let us see happens push 18-100. impossible range unrealistic range.see curve starts diverging get 93 predicted wage negative. confidence bands also get wider wider get farther away data.can also think problem classification problem, just now setting us task predicting whether individual earns $250000 per year. add new factor value denoting response.use polynomial expansion recipe rec_poly created earlier since wage response now want high response.\nalso create logistic regression specification use classification model.polynomial logistic regression model workflow can now fit predicted usualIf want can also get back underlying probability predictions two classes, confidence intervals probability predictions setting type = \"prob\" type = \"conf_int\".can use visualize probability curve classification model.Next, let us take look step function fit model using preprocessor. can create step functions couple different ways. step_discretize() convert numeric variable factor variable n bins, n specified num_breaks. approximately number points according training data set.already know want step function break can use step_cut() supply breaks manually.","code":"\nrec_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n\npoly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_poly)\npoly_fit <- fit(poly_wf, data = Wage)\npoly_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)   age_poly_1   age_poly_2   age_poly_3   age_poly_4  \n##      111.70       447.07      -478.32       125.52       -77.91\ntidy(poly_fit)## # A tibble: 5 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    112.      0.729    153.   0       \n## 2 age_poly_1     447.     39.9       11.2  1.48e-28\n## 3 age_poly_2    -478.     39.9      -12.0  2.36e-32\n## 4 age_poly_3     126.     39.9        3.14 1.68e- 3\n## 5 age_poly_4     -77.9    39.9       -1.95 5.10e- 2\npoly(1:6, degree = 4, raw = FALSE)##               1          2          3          4\n## [1,] -0.5976143  0.5455447 -0.3726780  0.1889822\n## [2,] -0.3585686 -0.1091089  0.5217492 -0.5669467\n## [3,] -0.1195229 -0.4364358  0.2981424  0.3779645\n## [4,]  0.1195229 -0.4364358 -0.2981424  0.3779645\n## [5,]  0.3585686 -0.1091089 -0.5217492 -0.5669467\n## [6,]  0.5976143  0.5455447  0.3726780  0.1889822\n## attr(,\"coefs\")\n## attr(,\"coefs\")$alpha\n## [1] 3.5 3.5 3.5 3.5\n## \n## attr(,\"coefs\")$norm2\n## [1]  1.00000  6.00000 17.50000 37.33333 64.80000 82.28571\n## \n## attr(,\"degree\")\n## [1] 1 2 3 4\n## attr(,\"class\")\n## [1] \"poly\"   \"matrix\"\npoly(1:6, degree = 4, raw = TRUE)##      1  2   3    4\n## [1,] 1  1   1    1\n## [2,] 2  4   8   16\n## [3,] 3  9  27   81\n## [4,] 4 16  64  256\n## [5,] 5 25 125  625\n## [6,] 6 36 216 1296\n## attr(,\"degree\")\n## [1] 1 2 3 4\n## attr(,\"class\")\n## [1] \"poly\"   \"matrix\"\nrec_raw_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4, options = list(raw = TRUE))\n\nraw_poly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_raw_poly)\n\nraw_poly_fit <- fit(raw_poly_wf, data = Wage)\n\ntidy(raw_poly_fit)## # A tibble: 5 × 5\n##   term            estimate  std.error statistic  p.value\n##   <chr>              <dbl>      <dbl>     <dbl>    <dbl>\n## 1 (Intercept) -184.        60.0           -3.07 0.00218 \n## 2 age_poly_1    21.2        5.89           3.61 0.000312\n## 3 age_poly_2    -0.564      0.206         -2.74 0.00626 \n## 4 age_poly_3     0.00681    0.00307        2.22 0.0264  \n## 5 age_poly_4    -0.0000320  0.0000164     -1.95 0.0510\nage_range <- tibble(age = seq(min(Wage$age), max(Wage$age)))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = age_range),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\nregression_lines## # A tibble: 63 × 4\n##      age .pred .pred_lower .pred_upper\n##    <int> <dbl>       <dbl>       <dbl>\n##  1    18  51.9        41.5        62.3\n##  2    19  58.5        49.9        67.1\n##  3    20  64.6        57.5        71.6\n##  4    21  70.2        64.4        76.0\n##  5    22  75.4        70.5        80.2\n##  6    23  80.1        76.0        84.2\n##  7    24  84.5        80.9        88.1\n##  8    25  88.5        85.2        91.7\n##  9    26  92.1        89.1        95.2\n## 10    27  95.4        92.5        98.4\n## # … with 53 more rows\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\nwide_age_range <- tibble(age = seq(18, 100))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = wide_age_range),\n  predict(poly_fit, new_data = wide_age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\nWage <- Wage %>%\n  mutate(high = factor(wage > 250, \n                       levels = c(TRUE, FALSE), \n                       labels = c(\"High\", \"Low\")))\nrec_poly <- recipe(high ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nlr_poly_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_poly)\nlr_poly_fit <- fit(lr_poly_wf, data = Wage)\n\npredict(lr_poly_fit, new_data = Wage)## # A tibble: 3,000 × 1\n##    .pred_class\n##    <fct>      \n##  1 Low        \n##  2 Low        \n##  3 Low        \n##  4 Low        \n##  5 Low        \n##  6 Low        \n##  7 Low        \n##  8 Low        \n##  9 Low        \n## 10 Low        \n## # … with 2,990 more rows\npredict(lr_poly_fit, new_data = Wage, type = \"prob\")## # A tibble: 3,000 × 2\n##       .pred_High .pred_Low\n##            <dbl>     <dbl>\n##  1 0.00000000983     1.00 \n##  2 0.000120          1.00 \n##  3 0.0307            0.969\n##  4 0.0320            0.968\n##  5 0.0305            0.970\n##  6 0.0352            0.965\n##  7 0.0313            0.969\n##  8 0.00820           0.992\n##  9 0.0334            0.967\n## 10 0.0323            0.968\n## # … with 2,990 more rows\npredict(lr_poly_fit, new_data = Wage, type = \"conf_int\")## # A tibble: 3,000 × 4\n##    .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n##               <dbl>            <dbl>           <dbl>           <dbl>\n##  1         2.22e-16          0.00166           0.998           1    \n##  2         1.82e- 6          0.00786           0.992           1.00 \n##  3         2.19e- 2          0.0428            0.957           0.978\n##  4         2.31e- 2          0.0442            0.956           0.977\n##  5         2.13e- 2          0.0434            0.957           0.979\n##  6         2.45e- 2          0.0503            0.950           0.975\n##  7         2.25e- 2          0.0434            0.957           0.977\n##  8         3.01e- 3          0.0222            0.978           0.997\n##  9         2.39e- 2          0.0465            0.953           0.976\n## 10         2.26e- 2          0.0458            0.954           0.977\n## # … with 2,990 more rows\nregression_lines <- bind_cols(\n  augment(lr_poly_fit, new_data = age_range, type = \"prob\"),\n  predict(lr_poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nregression_lines %>%\n  ggplot(aes(age)) +\n  ylim(c(0, 0.2)) +\n  geom_line(aes(y = .pred_High), color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower_High), color = \"blue\", linetype = \"dashed\") +\n  geom_line(aes(y = .pred_upper_High), color = \"blue\", linetype = \"dashed\") +\n  geom_jitter(aes(y = (high == \"High\") / 5), data = Wage, \n              shape = \"|\", height = 0, width = 0.2)## Warning: Removed 8 row(s) containing missing values (geom_path).\nrec_discretize <- recipe(high ~ age, data = Wage) %>%\n  step_discretize(age, num_breaks = 4)\n\ndiscretize_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_discretize)\n\ndiscretize_fit <- fit(discretize_wf, data = Wage)\ndiscretize_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_discretize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n## \n## Coefficients:\n## (Intercept)      agebin2      agebin3      agebin4  \n##       5.004       -1.492       -1.665       -1.705  \n## \n## Degrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\n## Null Deviance:       730.5 \n## Residual Deviance: 710.4     AIC: 718.4\nrec_cut <- recipe(high ~ age, data = Wage) %>%\n  step_cut(age, breaks = c(30, 50, 70))\n\ncut_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_cut)\n\ncut_fit <- fit(cut_wf, data = Wage)\ncut_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_cut()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n## \n## Coefficients:\n## (Intercept)   age(30,50]   age(50,70]   age(70,80]  \n##       6.256       -2.746       -3.038       10.310  \n## \n## Degrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\n## Null Deviance:       730.5 \n## Residual Deviance: 704.3     AIC: 712.3"},{"path":"moving-beyond-linearity.html","id":"splines","chapter":"7 Moving Beyond Linearity","heading":"7.2 Splines","text":"order fit regression splines, words, use splines preprocessors fitting linear model, use step_bs() construct matrices basis functions. bs() function used arguments knots can passed bs() using passing named list options.already linear regression specification lm_spec can create workflow, fit model predict like seen previous chapters.Lastly, can plot basic spline top data.","code":"\nrec_spline <- recipe(wage ~ age, data = Wage) %>%\n  step_bs(age, options = list(knots = 25, 40, 60))\nspline_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spline)\n\nspline_fit <- fit(spline_wf, data = Wage)\n\npredict(spline_fit, new_data = Wage)## # A tibble: 3,000 × 1\n##    .pred\n##    <dbl>\n##  1  58.7\n##  2  84.3\n##  3 120. \n##  4 120. \n##  5 120. \n##  6 119. \n##  7 120. \n##  8 102. \n##  9 119. \n## 10 120. \n## # … with 2,990 more rows\nregression_lines <- bind_cols(\n  augment(spline_fit, new_data = age_range),\n  predict(spline_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"blue\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")"},{"path":"moving-beyond-linearity.html","id":"gams","chapter":"7 Moving Beyond Linearity","heading":"7.3 GAMs","text":"GAM section WIP since now supported parsnip.","code":""},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"8 Tree-Based Methods","heading":"8 Tree-Based Methods","text":"lab take look different tree-based models, explore changing hyperparameters can help improve performance.\nchapter use parsnip model fitting recipes workflows perform transformations, tune dials tune hyperparameters model. rpart.plot used visualize decision trees created using rpart package engine, vip used visualize variable importance later models.Boston data set contain various statistics 506 neighborhoods Boston. build regression model related median value owner-occupied homes (medv) response remaining variables predictors.\nBoston data set quite outdated contains really unfortunate variables.\nalso use Carseats data set ISLR package demonstrate classification model. create new variable High denote Sales <= 8, Sales predictor removed perfect predictor High.","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\nlibrary(rpart.plot)\nlibrary(vip)\n\ndata(\"Boston\", package = \"MASS\")\n\nBoston <- as_tibble(Boston)\nCarseats <- as_tibble(Carseats) %>%\n  mutate(High = factor(if_else(Sales <= 8, \"No\", \"Yes\"))) %>%\n  select(-Sales)"},{"path":"tree-based-methods.html","id":"fitting-classification-trees","chapter":"8 Tree-Based Methods","heading":"8.1 Fitting Classification Trees","text":"fitting classification regression tree section, can save little bit typing creating general decision tree specification using rpart engine.decision tree specification can used create classification decision tree engine. good example flexible composition system created parsnip can used create multiple model specifications.model specification data ready fit model.look model output see quite informative summary model. tries give written description tree created.tree gets couple nodes can become hard read printed diagram. rpart.plot package provides functions let us easily visualize decision tree. name implies, works rpart trees.can see important variable predict high sales appears shelving location forms first node.training accuracy model 85%Let us take look confusion matrix see balance thereAnd model appears work well overall. model fit whole data set get training accuracy misleading model overfitting. Let us redo fitting creating validation split fit model training data set.Now can fit model training data set.Let us take look confusion matrix training data set testing data set.training data set performs well expectbut testing data set doesn’t perform just well get smaller accuracy 77%Let us try tune cost_complexity decision tree find optimal complexity. use class_tree_spec object use set_args() function specify want tune cost_complexity. passed directly workflow object avoid creating intermediate object.able tune variable need 2 objects. S resamples object, use k-fold cross-validation data set, grid values try. Since tuning 1 hyperparameter fine stay regular grid.using autoplot() shows values cost_complexity appear produce highest accuracyWe can now select best performing value select_best(), finalize workflow updating value cost_complexity fit model full training data set.last, can visualize model, see better-performing model less complex original model fit.","code":"\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\")\nclass_tree_spec <- tree_spec %>%\n  set_mode(\"classification\")\nclass_tree_fit <- class_tree_spec %>%\n  fit(High ~ ., data = Carseats)\nclass_tree_fit## parsnip model object\n## \n## Fit time:  17ms \n## n= 400 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 400 164 No (0.59000000 0.41000000)  \n##     2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  \n##       4) Price>=92.5 269  66 No (0.75464684 0.24535316)  \n##         8) Advertising< 13.5 224  41 No (0.81696429 0.18303571)  \n##          16) CompPrice< 124.5 96   6 No (0.93750000 0.06250000) *\n##          17) CompPrice>=124.5 128  35 No (0.72656250 0.27343750)  \n##            34) Price>=109.5 107  20 No (0.81308411 0.18691589)  \n##              68) Price>=126.5 65   6 No (0.90769231 0.09230769) *\n##              69) Price< 126.5 42  14 No (0.66666667 0.33333333)  \n##               138) Age>=49.5 22   2 No (0.90909091 0.09090909) *\n##               139) Age< 49.5 20   8 Yes (0.40000000 0.60000000) *\n##            35) Price< 109.5 21   6 Yes (0.28571429 0.71428571) *\n##         9) Advertising>=13.5 45  20 Yes (0.44444444 0.55555556)  \n##          18) Age>=54.5 20   5 No (0.75000000 0.25000000) *\n##          19) Age< 54.5 25   5 Yes (0.20000000 0.80000000) *\n##       5) Price< 92.5 46  14 Yes (0.30434783 0.69565217)  \n##        10) Income< 57 10   3 No (0.70000000 0.30000000) *\n##        11) Income>=57 36   7 Yes (0.19444444 0.80555556) *\n##     3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  \n##       6) Price>=142.5 12   3 No (0.75000000 0.25000000) *\n##       7) Price< 142.5 73  10 Yes (0.13698630 0.86301370) *\nclass_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\naugment(class_tree_fit, new_data = Carseats) %>%\n  accuracy(truth = High, estimate = .pred_class)## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.848\naugment(class_tree_fit, new_data = Carseats) %>%\n  conf_mat(truth = High, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  200  25\n##        Yes  36 139\nset.seed(1234)\nCarseats_split <- initial_split(Carseats)\n\nCarseats_train <- training(Carseats_split)\nCarseats_test <- testing(Carseats_split)\nclass_tree_fit <- fit(class_tree_spec, High ~ ., data = Carseats_train)\naugment(class_tree_fit, new_data = Carseats_train) %>%\n  conf_mat(truth = High, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  159  21\n##        Yes  21  99\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  conf_mat(truth = High, estimate = .pred_class)##           Truth\n## Prediction No Yes\n##        No  41   8\n##        Yes 15  36\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  accuracy(truth = High, estimate = .pred_class)## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary          0.77\nclass_tree_wf <- workflow() %>%\n  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(High ~ .)\nset.seed(1234)\nCarseats_fold <- vfold_cv(Carseats_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  class_tree_wf, \n  resamples = Carseats_fold, \n  grid = param_grid, \n  metrics = metric_set(accuracy)\n)\nautoplot(tune_res)\nbest_complexity <- select_best(tune_res)\n\nclass_tree_final <- finalize_workflow(class_tree_wf, best_complexity)\n\nclass_tree_final_fit <- fit(class_tree_final, data = Carseats_train)\nclass_tree_final_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## High ~ .\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 300 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n## 1) root 300 120 No (0.6000000 0.4000000)  \n##   2) ShelveLoc=Bad,Medium 242  73 No (0.6983471 0.3016529)  \n##     4) Price>=92.5 213  51 No (0.7605634 0.2394366) *\n##     5) Price< 92.5 29   7 Yes (0.2413793 0.7586207) *\n##   3) ShelveLoc=Good 58  11 Yes (0.1896552 0.8103448) *\nclass_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()"},{"path":"tree-based-methods.html","id":"fitting-regression-trees","chapter":"8 Tree-Based Methods","heading":"8.2 Fitting Regression Trees","text":"now show fit regression tree. similar saw last section. main difference response looking continuous instead categorical. can reuse tree_spec base regression decision tree specification.using Boston data set validation split .fitting model training data setand rpart.plot() function works regression decision tree wellNotice result numeric variable instead class.Now let us try tune cost_complexity find best performing model.appears higher complexity works preferred according cross-validationWe select best-performing model according \"rmse\" fit final model whole training data set.Visualizing model reveals much complex tree saw last section.","code":"\nreg_tree_spec <- tree_spec %>%\n  set_mode(\"regression\")\nset.seed(1234)\nBoston_split <- initial_split(Boston)\n\nBoston_train <- training(Boston_split)\nBoston_test <- testing(Boston_split)\nreg_tree_fit <- fit(reg_tree_spec, medv ~ ., Boston_train)\nreg_tree_fit## parsnip model object\n## \n## Fit time:  13ms \n## n= 379 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 379 32622.9500 22.54802  \n##    2) rm< 6.941 320 13602.3100 19.86281  \n##      4) lstat>=14.395 129  2582.1090 14.51550  \n##        8) nox>=0.607 80   984.7339 12.35875  \n##         16) lstat>=19.34 47   388.6332 10.35957 *\n##         17) lstat< 19.34 33   140.7188 15.20606 *\n##        9) nox< 0.607 49   617.6939 18.03673 *\n##      5) lstat< 14.395 191  4840.3640 23.47435  \n##       10) rm< 6.543 151  2861.3990 22.21192  \n##         20) dis>=1.68515 144  1179.5970 21.82083 *\n##         21) dis< 1.68515 7  1206.6970 30.25714 *\n##       11) rm>=6.543 40   829.8560 28.24000 *\n##    3) rm>=6.941 59  4199.1020 37.11186  \n##      6) rm< 7.437 35  1012.4100 32.08286 *\n##      7) rm>=7.437 24  1010.6200 44.44583  \n##       14) ptratio>=15.4 12   585.0767 40.71667 *\n##       15) ptratio< 15.4 12    91.7825 48.17500 *\naugment(reg_tree_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.78\nreg_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\nreg_tree_wf <- workflow() %>%\n  add_model(reg_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(medv ~ .)\n\nset.seed(1234)\nBoston_fold <- vfold_cv(Boston_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  reg_tree_wf, \n  resamples = Boston_fold, \n  grid = param_grid\n)\nautoplot(tune_res)\nbest_complexity <- select_best(tune_res, metric = \"rmse\")\n\nreg_tree_final <- finalize_workflow(reg_tree_wf, best_complexity)\n\nreg_tree_final_fit <- fit(reg_tree_final, data = Boston_train)\nreg_tree_final_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## medv ~ .\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 379 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 379 32622.95000 22.54802  \n##    2) rm< 6.941 320 13602.31000 19.86281  \n##      4) lstat>=14.395 129  2582.10900 14.51550  \n##        8) nox>=0.607 80   984.73390 12.35875  \n##         16) lstat>=19.34 47   388.63320 10.35957  \n##           32) tax>=551.5 40   243.94980  9.67750 *\n##           33) tax< 551.5 7    19.73714 14.25714 *\n##         17) lstat< 19.34 33   140.71880 15.20606 *\n##        9) nox< 0.607 49   617.69390 18.03673  \n##         18) crim>=0.381565 25   313.20000 16.20000 *\n##         19) crim< 0.381565 24   132.30000 19.95000 *\n##      5) lstat< 14.395 191  4840.36400 23.47435  \n##       10) rm< 6.543 151  2861.39900 22.21192  \n##         20) dis>=1.68515 144  1179.59700 21.82083  \n##           40) rm< 6.062 56   306.22860 20.28571 *\n##           41) rm>=6.062 88   657.41950 22.79773  \n##             82) lstat>=9.98 35    98.32686 21.02571 *\n##             83) lstat< 9.98 53   376.61550 23.96792 *\n##         21) dis< 1.68515 7  1206.69700 30.25714 *\n##       11) rm>=6.543 40   829.85600 28.24000  \n##         22) lstat>=4.44 33   274.06180 27.15455 *\n##         23) lstat< 4.44 7   333.61710 33.35714 *\n##    3) rm>=6.941 59  4199.10200 37.11186  \n##      6) rm< 7.437 35  1012.41000 32.08286  \n##       12) nox>=0.4885 14   673.46930 28.89286 *\n##       13) nox< 0.4885 21   101.49810 34.20952 *\n##      7) rm>=7.437 24  1010.62000 44.44583  \n##       14) ptratio>=15.4 12   585.07670 40.71667 *\n##       15) ptratio< 15.4 12    91.78250 48.17500 *\nreg_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()"},{"path":"tree-based-methods.html","id":"bagging-and-random-forests","chapter":"8 Tree-Based Methods","heading":"8.3 Bagging and Random Forests","text":"apply bagging random forests Boston data set. using randomForest package engine. bagging model random forest mtry equal number predictors. can specify mtry .cols() means number columns predictor matrix used. useful want make specification general useable many different data sets. .cols() one many descriptors parsnip package.\nalso set importance = TRUE set_engine() tell engine save information regarding variable importance. needed engine want use vip package later.fit model like normaland take look testing performance. see improvement decision tree.can also create quick scatterplot true predicted value see can make diagnostics.isn’t anything weird going happy. Next, let us take look variable importanceNext, let us take look random forest. default, randomForest() p / 3 variables building random forest regression trees, sqrt(p) variables building random forest classification trees. use mtry = 6.fitting model like normalthis model slightly better performance bagging modelWe can likewise plot true value predicted valueit looks fine. discernable difference chart one created bagging model.variable importance plot also quite similar saw bagging model isn’t surprising.normally want perform hyperparameter tuning random forest model get best forest. exercise left reader.","code":"\nbagging_spec <- rand_forest(mtry = .cols()) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\nbagging_fit <- fit(bagging_spec, medv ~ ., data = Boston_train)\naugment(bagging_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.43\naugment(bagging_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\nvip(bagging_fit)\nrf_spec <- rand_forest(mtry = 6) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\nrf_fit <- fit(rf_spec, medv ~ ., data = Boston_train)\naugment(rf_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.26\naugment(rf_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\nvip(rf_fit)"},{"path":"tree-based-methods.html","id":"boosting","chapter":"8 Tree-Based Methods","heading":"8.4 Boosting","text":"now fit boosted tree model. xgboost packages give good implementation boosted trees. many parameters tune know setting trees high can lead overfitting. Nevertheless, let us try fitting boosted tree. set tree = 5000 grow 5000 trees maximal depth 4 setting tree_depth = 4.fitting model like normaland rmse little high case properly didn’t tune parameters.can look scatterplot don’t see anything weird going .normally want perform hyperparameter tuning boosted tree model get best model. exercise left reader. Look Iterative search chapter Tidy Modeling R inspiration.","code":"\nboost_spec <- boost_tree(trees = 5000, tree_depth = 4) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\nboost_fit <- fit(boost_spec, medv ~ ., data = Boston_train)\naugment(boost_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.34\naugment(boost_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)"},{"path":"tree-based-methods.html","id":"bayesian-additive-regression-trees","chapter":"8 Tree-Based Methods","heading":"8.5 Bayesian Additive Regression Trees","text":"section WIP.","code":""},{"path":"support-vector-machines.html","id":"support-vector-machines","chapter":"9 Support Vector Machines","heading":"9 Support Vector Machines","text":"lab take look support vector machines, explore changing hyperparameters can help improve performance.\nchapter use parsnip model fitting recipes workflows perform transformations, tune dials tune hyperparameters model.","code":"\nlibrary(tidymodels)\nlibrary(ISLR)"},{"path":"support-vector-machines.html","id":"support-vector-classifier","chapter":"9 Support Vector Machines","heading":"9.1 Support Vector Classifier","text":"Let us start creating synthetic data set. use normally distributed data added offset create 2 separate classes.PLotting shows two slightly overlapping classesWe can create linear SVM specification setting degree = 1 polynomial SVM model. furthermore set scaled = FALSE set_engine() engine scale data us. get later can performing scaling recipe instead.Taking specification, can add specific cost 10 fitting model data. Using set_args() allows us set cost argument without modifying model specification.kernlab models can visualized using plot() function load kernlab package.instead used smaller value cost parameter?Now smaller value cost parameter used, obtain larger number support vectors, margin now wider.Let us set tune_grid() section find value cost leads highest accuracy SVM model.using tune_res object select_best() function allows us find value cost gives best cross-validated accuracy. Finalize workflow finalize_workflow() fit new workflow data set.can now generate different data set act test data set. make sure generated using model different seed.accessing model testing data set shows us model still performs well.","code":"\nset.seed(1)\nsim_data <- tibble(\n  x1 = rnorm(40),\n  x2 = rnorm(40),\n  y  = factor(rep(c(-1, 1), 20))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit## parsnip model object\n## \n## Fit time:  776ms \n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 10 \n## \n## Polynomial kernel function. \n##  Hyperparameters : degree =  1  scale =  1  offset =  1 \n## \n## Number of Support Vectors : 17 \n## \n## Objective Function Value : -152.0188 \n## Training error : 0.125 \n## Probability model included.\nlibrary(kernlab)\nsvm_linear_fit %>%\n  extract_fit_engine() %>%\n  plot()\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit## parsnip model object\n## \n## Fit time:  28ms \n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 0.1 \n## \n## Polynomial kernel function. \n##  Hyperparameters : degree =  1  scale =  1  offset =  1 \n## \n## Number of Support Vectors : 25 \n## \n## Objective Function Value : -2.0376 \n## Training error : 0.15 \n## Probability model included.\nsvm_linear_wf <- workflow() %>%\n  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\nset.seed(2)\nsim_data_test <- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\naugment(svm_linear_fit, new_data = sim_data_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)##           Truth\n## Prediction -1 1\n##         -1  8 3\n##         1   2 7"},{"path":"support-vector-machines.html","id":"support-vector-machine","chapter":"9 Support Vector Machines","heading":"9.2 Support Vector Machine","text":"now see can fit SVM using non-linear kernel. Let us start generating data, time generate non-linear class boundary.try SVM radial basis function. kernel allow us capture non-linearity data.fitting modeland plotting reveals model able o separate two classes, even though non-linearly separated.let us see well model generalizes new data generating process.works well!","code":"\nset.seed(1)\nsim_data2 <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nsim_data2 %>%\n  ggplot(aes(x1, x2, color = y)) +\n  geom_point()\nsvm_rbf_spec <- svm_rbf() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\nsvm_rbf_fit <- svm_rbf_spec %>%\n  fit(y ~ ., data = sim_data2)\nsvm_rbf_fit %>%\n  extract_fit_engine() %>%\n  plot()\nset.seed(2)\nsim_data2_test <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)##           Truth\n## Prediction   1   2\n##          1 137   7\n##          2  13  43"},{"path":"support-vector-machines.html","id":"roc-curves","chapter":"9 Support Vector Machines","heading":"9.3 ROC Curves","text":"ROC curves can easily created using roc_curve() yardstick package. use function much way done using accuracy() function, main difference pass predicted class probability estimate instead passing predicted class.produces different values specificity sensitivity threshold. can get quick visualization passing results roc_curve() autoplot()common metric t calculate area curve. can done using roc_auc() function (_auc stands area curve).","code":"\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1)## # A tibble: 202 × 3\n##    .threshold specificity sensitivity\n##         <dbl>       <dbl>       <dbl>\n##  1   -Inf            0          1    \n##  2      0.104        0          1    \n##  3      0.113        0.02       1    \n##  4      0.114        0.04       1    \n##  5      0.115        0.06       1    \n##  6      0.117        0.08       1    \n##  7      0.118        0.1        1    \n##  8      0.119        0.12       1    \n##  9      0.124        0.14       1    \n## 10      0.124        0.14       0.993\n## # … with 192 more rows\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1) %>%\n  autoplot()\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_auc(truth = y, estimate = .pred_1)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.925"},{"path":"support-vector-machines.html","id":"application-to-gene-expression-data","chapter":"9 Support Vector Machines","heading":"9.4 Application to Gene Expression Data","text":"now examine Khan data set, consists several tissue samples corresponding four distinct types small round blue cell tumors. tissue sample, gene expression measurements available. data set comes Khan list wrangle little bit create two tibbles, 1 training data 1 testing data.looking dimensions training data reveals 63 observations 20308 gene expression measurements.large number predictors compared number rows. indicates linear kernel added flexibility get polynomial radial kernel unnecessary.Let us take look training confusion matrix. look, get perfect confusion matrix. getting hyperplane able fully separate classes.remember don’t measure performance well performs training data set. measure performance model well performs testing data set, let us look testing confusion matrixAnd performs fairly well. couple misclassification nothing bad.","code":"\nKhan_train <- bind_cols(\n  y = factor(Khan$ytrain),\n  as_tibble(Khan$xtrain)\n)## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\n## Using compatibility `.name_repair`.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nKhan_test <- bind_cols(\n  y = factor(Khan$ytest),\n  as_tibble(Khan$xtest)\n)\ndim(Khan_train)## [1]   63 2309\nkhan_fit <- svm_linear_spec %>%\n  set_args(cost = 10) %>%\n  fit(y ~ ., data = Khan_train)\naugment(khan_fit, new_data = Khan_train) %>%\n  conf_mat(truth = y, estimate = .pred_class)##           Truth\n## Prediction  1  2  3  4\n##          1  8  0  0  0\n##          2  0 23  0  0\n##          3  0  0 12  0\n##          4  0  0  0 20\naugment(khan_fit, new_data = Khan_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)##           Truth\n## Prediction 1 2 3 4\n##          1 3 0 0 0\n##          2 0 6 2 0\n##          3 0 0 4 0\n##          4 0 0 0 5"},{"path":"deep-learning.html","id":"deep-learning","chapter":"10 Deep learning","heading":"10 Deep learning","text":"current plans recreate chapter using tidymodels isn’t replacement keras tidymodels. like something specific chapter please open issue.","code":""},{"path":"survival-analysis-and-censored-data.html","id":"survival-analysis-and-censored-data","chapter":"11 Survival Analysis and Censored Data","heading":"11 Survival Analysis and Censored Data","text":"https://github.com/tidymodels/planning/tree/master/survival-analysis worked tidymodels yet ready included book yet.","code":""},{"path":"unsupervised-learning.html","id":"unsupervised-learning","chapter":"12 Unsupervised Learning","heading":"12 Unsupervised Learning","text":"final chapter talks unsupervised learning. broken two parts. Dimensionality reduction clustering. One downside moment clustering well integrated tidymodels time. still able use features tidymodels.","code":"\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(factoextra)\nlibrary(patchwork)\nlibrary(proxy)\nlibrary(ISLR)"},{"path":"unsupervised-learning.html","id":"principal-components-analysis","chapter":"12 Unsupervised Learning","heading":"12.1 Principal Components Analysis","text":"section used explore USArrests data set using PCA. move , let turn USArrests tibble move rownames column.Notice mean variables quite different. apply PCA directly data set Murder small influence.show perform PCA two different ways section. First, using prcomp() directly, using broom extract information need, secondly using recipes.\nprcomp() Takes 1 required argument x much fully numeric data.frame matrix. pass prcomp(). also set scale = TRUE prcomp() perform scaling need.now can use favorite broom function extract information prcomp object.\nstart tidy(). tidy() can used extract couple different things, see ?broom:::tidy.prcomp() information. tidy() default extract scores PCA object long tidy format. score location observation PCA space. canWe can also explicitly say want scores setting matrix = \"scores\".Next, can get loadings PCA.information tells us variable contributes principal component. don’t many principal components can visualize contribution without filteringLastly, can set matrix = \"eigenvalues\" get back explained standard deviation PC including percent cumulative quite handy plotting.want see percent standard deviation explained drops PC can easily get using tidy() matrix = \"eigenvalues\".Lastly, augment() function give back fitted PC transformation apply prcomp() object directlyand apply transformation new data passing new data newdataIf using PCA preprocessing method recommend use recipes apply PCA transformation. good way since recipe correctly apply transformation new data recipe used .step_normalize() make sure variables scale. using all_numeric() able apply PCA variables want without remove state. also setting id step_pca() make easier tidy() later.calling bake(new_data = NULL) can get fitted PC transformation numerical variablesbut can also supply data new_data.can get back information prcomp() specify slightly different inside tidy(). id = \"pca\" refers second step pca_rec. get scores type = \"coef\"eigenvalues type = \"variance\".Sometimes don’t want get back principal components data. can either specify many components want num_comp (rank. prcomp())using threshold specify many components keep variance explained. setting threshold = 0.7 step_pca() generate enough principal components explain 70% variance.","code":"\nUSArrests <- as_tibble(USArrests, rownames = \"state\")\nUSArrests## # A tibble: 50 × 5\n##    state       Murder Assault UrbanPop  Rape\n##    <chr>        <dbl>   <int>    <int> <dbl>\n##  1 Alabama       13.2     236       58  21.2\n##  2 Alaska        10       263       48  44.5\n##  3 Arizona        8.1     294       80  31  \n##  4 Arkansas       8.8     190       50  19.5\n##  5 California     9       276       91  40.6\n##  6 Colorado       7.9     204       78  38.7\n##  7 Connecticut    3.3     110       77  11.1\n##  8 Delaware       5.9     238       72  15.8\n##  9 Florida       15.4     335       80  31.9\n## 10 Georgia       17.4     211       60  25.8\n## # … with 40 more rows\nUSArrests %>%\n  select(-state) %>%\n  map_dfr(mean)## # A tibble: 1 × 4\n##   Murder Assault UrbanPop  Rape\n##    <dbl>   <dbl>    <dbl> <dbl>\n## 1   7.79    171.     65.5  21.2\nUSArrests_pca <- USArrests %>%\n  select(-state) %>%\n  prcomp(scale = TRUE)\n\nUSArrests_pca## Standard deviations (1, .., p=4):\n## [1] 1.5748783 0.9948694 0.5971291 0.4164494\n## \n## Rotation (n x k) = (4 x 4):\n##                 PC1        PC2        PC3         PC4\n## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780\n## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748\n## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\n## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432\ntidy(USArrests_pca)## # A tibble: 200 × 3\n##      row    PC  value\n##    <int> <dbl>  <dbl>\n##  1     1     1 -0.976\n##  2     1     2  1.12 \n##  3     1     3 -0.440\n##  4     1     4  0.155\n##  5     2     1 -1.93 \n##  6     2     2  1.06 \n##  7     2     3  2.02 \n##  8     2     4 -0.434\n##  9     3     1 -1.75 \n## 10     3     2 -0.738\n## # … with 190 more rows\ntidy(USArrests_pca, matrix = \"scores\")## # A tibble: 200 × 3\n##      row    PC  value\n##    <int> <dbl>  <dbl>\n##  1     1     1 -0.976\n##  2     1     2  1.12 \n##  3     1     3 -0.440\n##  4     1     4  0.155\n##  5     2     1 -1.93 \n##  6     2     2  1.06 \n##  7     2     3  2.02 \n##  8     2     4 -0.434\n##  9     3     1 -1.75 \n## 10     3     2 -0.738\n## # … with 190 more rows\ntidy(USArrests_pca, matrix = \"loadings\")## # A tibble: 16 × 3\n##    column      PC   value\n##    <chr>    <dbl>   <dbl>\n##  1 Murder       1 -0.536 \n##  2 Murder       2  0.418 \n##  3 Murder       3 -0.341 \n##  4 Murder       4  0.649 \n##  5 Assault      1 -0.583 \n##  6 Assault      2  0.188 \n##  7 Assault      3 -0.268 \n##  8 Assault      4 -0.743 \n##  9 UrbanPop     1 -0.278 \n## 10 UrbanPop     2 -0.873 \n## 11 UrbanPop     3 -0.378 \n## 12 UrbanPop     4  0.134 \n## 13 Rape         1 -0.543 \n## 14 Rape         2 -0.167 \n## 15 Rape         3  0.818 \n## 16 Rape         4  0.0890\ntidy(USArrests_pca, matrix = \"loadings\") %>%\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col()\ntidy(USArrests_pca, matrix = \"eigenvalues\")## # A tibble: 4 × 4\n##      PC std.dev percent cumulative\n##   <dbl>   <dbl>   <dbl>      <dbl>\n## 1     1   1.57   0.620       0.620\n## 2     2   0.995  0.247       0.868\n## 3     3   0.597  0.0891      0.957\n## 4     4   0.416  0.0434      1\ntidy(USArrests_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_col()\naugment(USArrests_pca)## # A tibble: 50 × 5\n##    .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4\n##    <chr>          <dbl>      <dbl>      <dbl>      <dbl>\n##  1 1            -0.976      1.12      -0.440     0.155  \n##  2 2            -1.93       1.06       2.02     -0.434  \n##  3 3            -1.75      -0.738      0.0542   -0.826  \n##  4 4             0.140      1.11       0.113    -0.181  \n##  5 5            -2.50      -1.53       0.593    -0.339  \n##  6 6            -1.50      -0.978      1.08      0.00145\n##  7 7             1.34      -1.08      -0.637    -0.117  \n##  8 8            -0.0472    -0.322     -0.711    -0.873  \n##  9 9            -2.98       0.0388    -0.571    -0.0953 \n## 10 10           -1.62       1.27      -0.339     1.07   \n## # … with 40 more rows\naugment(USArrests_pca, newdata = USArrests[1:5, ])## # A tibble: 5 × 10\n##   .rownames state Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3\n##   <chr>     <chr>  <dbl>   <int>    <int> <dbl>      <dbl>      <dbl>      <dbl>\n## 1 1         Alab…   13.2     236       58  21.2     -0.976      1.12     -0.440 \n## 2 2         Alas…   10       263       48  44.5     -1.93       1.06      2.02  \n## 3 3         Ariz…    8.1     294       80  31       -1.75      -0.738     0.0542\n## 4 4         Arka…    8.8     190       50  19.5      0.140      1.11      0.113 \n## 5 5         Cali…    9       276       91  40.6     -2.50      -1.53      0.593 \n## # … with 1 more variable: .fittedPC4 <dbl>\npca_rec <- recipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), id = \"pca\") %>%\n  prep()\npca_rec %>%\n  bake(new_data = NULL)## # A tibble: 50 × 5\n##    state           PC1     PC2     PC3      PC4\n##    <fct>         <dbl>   <dbl>   <dbl>    <dbl>\n##  1 Alabama     -0.976   1.12   -0.440   0.155  \n##  2 Alaska      -1.93    1.06    2.02   -0.434  \n##  3 Arizona     -1.75   -0.738   0.0542 -0.826  \n##  4 Arkansas     0.140   1.11    0.113  -0.181  \n##  5 California  -2.50   -1.53    0.593  -0.339  \n##  6 Colorado    -1.50   -0.978   1.08    0.00145\n##  7 Connecticut  1.34   -1.08   -0.637  -0.117  \n##  8 Delaware    -0.0472 -0.322  -0.711  -0.873  \n##  9 Florida     -2.98    0.0388 -0.571  -0.0953 \n## 10 Georgia     -1.62    1.27   -0.339   1.07   \n## # … with 40 more rows\npca_rec %>%\n  bake(new_data = USArrests[40:45, ])## # A tibble: 6 × 5\n##   state             PC1    PC2    PC3     PC4\n##   <fct>           <dbl>  <dbl>  <dbl>   <dbl>\n## 1 South Carolina -1.31   1.91  -0.298 -0.130 \n## 2 South Dakota    1.97   0.815  0.385 -0.108 \n## 3 Tennessee      -0.990  0.852  0.186  0.646 \n## 4 Texas          -1.34  -0.408 -0.487  0.637 \n## 5 Utah            0.545 -1.46   0.291 -0.0815\n## 6 Vermont         2.77   1.39   0.833 -0.143\ntidy(pca_rec, id = \"pca\", type = \"coef\")## # A tibble: 16 × 4\n##    terms      value component id   \n##    <chr>      <dbl> <chr>     <chr>\n##  1 Murder   -0.536  PC1       pca  \n##  2 Assault  -0.583  PC1       pca  \n##  3 UrbanPop -0.278  PC1       pca  \n##  4 Rape     -0.543  PC1       pca  \n##  5 Murder    0.418  PC2       pca  \n##  6 Assault   0.188  PC2       pca  \n##  7 UrbanPop -0.873  PC2       pca  \n##  8 Rape     -0.167  PC2       pca  \n##  9 Murder   -0.341  PC3       pca  \n## 10 Assault  -0.268  PC3       pca  \n## 11 UrbanPop -0.378  PC3       pca  \n## 12 Rape      0.818  PC3       pca  \n## 13 Murder    0.649  PC4       pca  \n## 14 Assault  -0.743  PC4       pca  \n## 15 UrbanPop  0.134  PC4       pca  \n## 16 Rape      0.0890 PC4       pca\ntidy(pca_rec, id = \"pca\", type = \"variance\")## # A tibble: 16 × 4\n##    terms                         value component id   \n##    <chr>                         <dbl>     <int> <chr>\n##  1 variance                      2.48          1 pca  \n##  2 variance                      0.990         2 pca  \n##  3 variance                      0.357         3 pca  \n##  4 variance                      0.173         4 pca  \n##  5 cumulative variance           2.48          1 pca  \n##  6 cumulative variance           3.47          2 pca  \n##  7 cumulative variance           3.83          3 pca  \n##  8 cumulative variance           4             4 pca  \n##  9 percent variance             62.0           1 pca  \n## 10 percent variance             24.7           2 pca  \n## 11 percent variance              8.91          3 pca  \n## 12 percent variance              4.34          4 pca  \n## 13 cumulative percent variance  62.0           1 pca  \n## 14 cumulative percent variance  86.8           2 pca  \n## 15 cumulative percent variance  95.7           3 pca  \n## 16 cumulative percent variance 100             4 pca\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), num_comp = 3) %>%\n  prep() %>%\n  bake(new_data = NULL)## # A tibble: 50 × 4\n##    state           PC1     PC2     PC3\n##    <fct>         <dbl>   <dbl>   <dbl>\n##  1 Alabama     -0.976   1.12   -0.440 \n##  2 Alaska      -1.93    1.06    2.02  \n##  3 Arizona     -1.75   -0.738   0.0542\n##  4 Arkansas     0.140   1.11    0.113 \n##  5 California  -2.50   -1.53    0.593 \n##  6 Colorado    -1.50   -0.978   1.08  \n##  7 Connecticut  1.34   -1.08   -0.637 \n##  8 Delaware    -0.0472 -0.322  -0.711 \n##  9 Florida     -2.98    0.0388 -0.571 \n## 10 Georgia     -1.62    1.27   -0.339 \n## # … with 40 more rows\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), threshold = 0.7) %>%\n  prep() %>%\n  bake(new_data = NULL)## # A tibble: 50 × 3\n##    state           PC1     PC2\n##    <fct>         <dbl>   <dbl>\n##  1 Alabama     -0.976   1.12  \n##  2 Alaska      -1.93    1.06  \n##  3 Arizona     -1.75   -0.738 \n##  4 Arkansas     0.140   1.11  \n##  5 California  -2.50   -1.53  \n##  6 Colorado    -1.50   -0.978 \n##  7 Connecticut  1.34   -1.08  \n##  8 Delaware    -0.0472 -0.322 \n##  9 Florida     -2.98    0.0388\n## 10 Georgia     -1.62    1.27  \n## # … with 40 more rows"},{"path":"unsupervised-learning.html","id":"matrix-completion","chapter":"12 Unsupervised Learning","heading":"12.2 Matrix Completion","text":"section WIP.","code":""},{"path":"unsupervised-learning.html","id":"kmeans-clustering","chapter":"12 Unsupervised Learning","heading":"12.3 Kmeans Clustering","text":"kmeans() function can used perform K-means clustering R. get let us create synthetic data set know groups.can plot ggplot2 see groups really . Note didn’t include grouping information x_df trying emulate situation don’t know possible underlying clusters.kmeans() functions takes matrix data.frame centers number clusters want kmeans() find. also set nstart = 20, allows algorithm multiple initial starting positions, use hope finding global maxima instead local maxima.fitted model lot different kinds information.can use broom functions extract information tidy formats. tidy() function returns information cluster, including position, size within-cluster sum--squares.glance() function returns model wise metrics. One tot.withinss total within-cluster sum--squares seek minimize perform K-means clustering.Lastly, can see cluster observation belongs using augment() “predict” cluster given observation belongs .can visualize result augment() see well clustering performed.well good, nice try number different clusters find best one. use mutate() map() combo fit multiple models extract information . remember set seed ensure reproducibility.Now total within-cluster sum--squares can plot k can use elbow method find optimal number clusters.see elbow k = 2 makes us happy since data set specifically created 2 clusters. can now extract model k = 2 multi_kmeans.can finish visualizing clusters found.","code":"\nset.seed(2)\n\nx_df <- tibble(\n  V1 = rnorm(n = 50, mean = rep(c(0, 3), each = 25)),\n  V2 = rnorm(n = 50, mean = rep(c(0, -4), each = 25))\n)\nx_df %>%\n  ggplot(aes(V1, V2, color = rep(c(\"A\", \"B\"), each = 25))) +\n  geom_point()\nset.seed(1234)\nres_kmeans <- kmeans(x_df, centers = 3, nstart = 20)\nres_kmeans## K-means clustering with 3 clusters of sizes 11, 23, 16\n## \n## Cluster means:\n##          V1          V2\n## 1 2.5355362 -2.48605364\n## 2 0.2339095  0.04414551\n## 3 2.8241300 -5.01221675\n## \n## Clustering vector:\n##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 3 1 1 1 3 1 3 3 3 3 1 3 3\n## [39] 3 1 1 1 3 3 3 3 1 3 3 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 14.56698 54.84869 26.98215\n##  (between_SS / total_SS =  76.8 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\ntidy(res_kmeans)## # A tibble: 3 × 5\n##      V1      V2  size withinss cluster\n##   <dbl>   <dbl> <int>    <dbl> <fct>  \n## 1 2.54  -2.49      11     14.6 1      \n## 2 0.234  0.0441    23     54.8 2      \n## 3 2.82  -5.01      16     27.0 3\nglance(res_kmeans)## # A tibble: 1 × 4\n##   totss tot.withinss betweenss  iter\n##   <dbl>        <dbl>     <dbl> <int>\n## 1  416.         96.4      320.     2\naugment(res_kmeans, data = x_df)## # A tibble: 50 × 3\n##         V1     V2 .cluster\n##      <dbl>  <dbl> <fct>   \n##  1 -0.897  -0.838 2       \n##  2  0.185   2.07  2       \n##  3  1.59   -0.562 2       \n##  4 -1.13    1.28  2       \n##  5 -0.0803 -1.05  2       \n##  6  0.132  -1.97  2       \n##  7  0.708  -0.323 2       \n##  8 -0.240   0.936 2       \n##  9  1.98    1.14  2       \n## 10 -0.139   1.67  2       \n## # … with 40 more rows\naugment(res_kmeans, data = x_df) %>%\n  ggplot(aes(V1, V2, color = .cluster)) +\n  geom_point()\nset.seed(1234)\nmulti_kmeans <- tibble(k = 1:10) %>%\n  mutate(\n    model = purrr::map(k, ~ kmeans(x_df, centers = .x, nstart = 20)),\n    tot.withinss = purrr::map_dbl(model, ~ glance(.x)$tot.withinss)\n  )\n\nmulti_kmeans## # A tibble: 10 × 3\n##        k model    tot.withinss\n##    <int> <list>          <dbl>\n##  1     1 <kmeans>        416. \n##  2     2 <kmeans>        127. \n##  3     3 <kmeans>         96.4\n##  4     4 <kmeans>         73.4\n##  5     5 <kmeans>         57.4\n##  6     6 <kmeans>         42.4\n##  7     7 <kmeans>         32.4\n##  8     8 <kmeans>         27.9\n##  9     9 <kmeans>         23.5\n## 10    10 <kmeans>         20.3\nmulti_kmeans %>%\n  ggplot(aes(k, tot.withinss)) +\n  geom_point() +\n  geom_line()\nfinal_kmeans <- multi_kmeans %>%\n  filter(k == 2) %>%\n  pull(model) %>%\n  pluck(1)\naugment(final_kmeans, data = x_df) %>%\n  ggplot(aes(V1, V2, color = .cluster)) +\n  geom_point()"},{"path":"unsupervised-learning.html","id":"hierarchical-clustering","chapter":"12 Unsupervised Learning","heading":"12.4 Hierarchical Clustering","text":"hclust() function one way perform hierarchical clustering R. needs one input dissimilarity structure produced dist(). Furthermore, can specify couple things, including agglomeration method. Let us cluster data couple different ways see choice agglomeration method changes clustering.factoextra provides functions (fviz_dend()) visualize clustering created using hclust(). use fviz_dend() show dendrogram.don’t know importance different predictors data set beneficial scale data variable influence. can perform scaling using scale() dist().","code":"\nres_hclust_complete <- x_df %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\nres_hclust_average <- x_df %>%\n  dist() %>%\n  hclust(method = \"average\")\n\nres_hclust_single <- x_df %>%\n  dist() %>%\n  hclust(method = \"single\")\nfviz_dend(res_hclust_complete, main = \"complete\", k = 2)## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nfviz_dend(res_hclust_average, main = \"average\", k = 2)## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nfviz_dend(res_hclust_single, main = \"single\", k = 2)## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nx_df %>%\n  scale() %>%\n  dist() %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend(k = 2)## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\n# correlation based distance\nset.seed(2)\nx <- matrix(rnorm(30 * 3), ncol = 3)\n\nx %>%\n  proxy::dist(method = \"correlation\") %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend()## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"unsupervised-learning.html","id":"pca-on-the-nci60-data","chapter":"12 Unsupervised Learning","heading":"12.5 PCA on the NCI60 Data","text":"now explore NCI60 data set. genomic data set, containing cancer cell line microarray data, consists 6830 gene expression measurements 64 cancer cell lines. data comes list containing matrix labels. little work turn data tibble use rest chapter.expect use label variable analysis since emulating unsupervised analysis. Since exploratory task fine using prcomp() since don’t need apply transformations anything else. remove label remember set scale = TRUE perform scaling variables.visualization purposes, now join labels result augment(nci60_pca) can visualize close similar labeled points .14 different labels, make use \"Polychrome 36\" palette help us better differentiate labels.o can plot different PCs . good idea compare first PCs since carry information. just compare pairs 1-2 1-3 can . tends good idea stop interesting things appear plots.see local clustering different cancer types promising, perfect let us see happens compare PC1 PC3 now.Lastly, plot variance explained principal component. can use tidy() matrix = \"eigenvalues\" accomplish easily, start percentage PCwith first PC little 10% fairly fast drop.can get cumulative variance explained just .","code":"\ndata(NCI60, package = \"ISLR\")\nnci60 <- NCI60$data %>%\n  as_tibble() %>%\n  set_colnames(., paste0(\"V_\", 1:ncol(.))) %>%\n  mutate(label = factor(NCI60$labs)) %>%\n  relocate(label)\nnci60_pca <- nci60 %>%\n  select(-label) %>%\n  prcomp(scale = TRUE)\nnci60_pcs <- bind_cols(\n  augment(nci60_pca),\n  nci60 %>% select(label)\n)\ncolors <- unname(palette.colors(n = 14, palette = \"Polychrome 36\"))\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC3, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_point() +\n  geom_line()\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, cumulative)) +\n  geom_point() +\n  geom_line()"},{"path":"unsupervised-learning.html","id":"clustering-on-nci60-dataset","chapter":"12 Unsupervised Learning","heading":"12.6 Clustering on nci60 dataset","text":"Let us now see happens perform clustering nci60 data set. start good create scaled version data set. can use recipes package perform transformations.Now start fitting multiple hierarchical clustering models using different agglomeration methods.visualize see good natural separations.now color according k = 4 get following separations.now take clustering id extracted cutree calculate Label common within cluster.can also see happens try fit K-means clustering. liked 4 clusters earlier let’s stick .can use tidy() extract cluster information, note look cluster, size, withinss thousands variables denoting location cluster.lastly, let us see two different methods used compare . Let us save cluster ids cluster_kmeans cluster_hclust use conf_mat() different way quickly generate heatmap two methods.lot agreement labels makes sense, since labels arbitrarily added. important tend agree quite lot (confusion matrix sparse).One last thing sometimes useful perform dimensionality reduction using clustering method. Let us use recipes package calculate PCA nci60 keep 5 first components. (started nci60 added step_rm() step_normalize()).can now use hclust() reduced data set, sometimes get quite good results since clustering method doesn’t work high dimensions.","code":"\nnci60_scaled <- recipe(~ ., data = nci60) %>%\n  step_rm(label) %>%\n  step_normalize(all_predictors()) %>%\n  prep() %>%\n  bake(new_data = NULL)\nnci60_complete <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"complete\")\n\nnci60_average <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"average\")\n\nnci60_single <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"single\")\nfviz_dend(nci60_complete, main = \"Complete\")## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nfviz_dend(nci60_average, main = \"Average\")## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nfviz_dend(nci60_single, main = \"Single\")## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nnci60_complete %>%\n  fviz_dend(k = 4, main = \"hclust(complete) on nci60\")## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\ntibble(\n  label = nci60$label,\n  cluster_id = cutree(nci60_complete, k = 4)\n) %>%\n  count(label, cluster_id) %>%\n  group_by(cluster_id) %>%\n  mutate(prop = n / sum(n)) %>%\n  slice_max(n = 1, order_by = prop) %>%\n  ungroup()## # A tibble: 6 × 4\n##   label    cluster_id     n  prop\n##   <fct>         <int> <int> <dbl>\n## 1 MELANOMA          1     8 0.2  \n## 2 NSCLC             1     8 0.2  \n## 3 RENAL             1     8 0.2  \n## 4 BREAST            2     3 0.429\n## 5 LEUKEMIA          3     6 0.75 \n## 6 COLON             4     5 0.556\nset.seed(2)\nres_kmeans_scaled <- kmeans(nci60_scaled, centers = 4, nstart = 50)\ntidy(res_kmeans_scaled) %>%\n  select(cluster, size, withinss)## # A tibble: 4 × 3\n##   cluster  size withinss\n##   <fct>   <int>    <dbl>\n## 1 1          20  108801.\n## 2 2          27  154545.\n## 3 3           9   37150.\n## 4 4           8   44071.\ncluster_kmeans <- res_kmeans_scaled$cluster\ncluster_hclust <- cutree(nci60_complete, k = 4)\n\ntibble(\n  kmeans = factor(cluster_kmeans),\n  hclust = factor(cluster_hclust)\n) %>%\n  conf_mat(kmeans, hclust) %>%\n  autoplot(type = \"heatmap\")\nnci60_pca <- recipe(~., nci60_scaled) %>%\n  step_pca(all_predictors(), num_comp = 5) %>%\n  prep() %>%\n  bake(new_data = NULL)\nnci60_pca %>%\n  dist() %>%\n  hclust() %>%\n  fviz_dend(k = 4, main = \"hclust on first five PCs\")## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"multiple-testing.html","id":"multiple-testing","chapter":"13 Multiple Testing","heading":"13 Multiple Testing","text":"chapter WIP. like something specific chapter please open issue.","code":""}]
