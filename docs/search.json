[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"book aims complement 1st version Introduction Statistical Learning book translations labs using tidymodels set packages.labs mirrored quite closely stay true original material.book open view Github.","code":""},{"path":"statististical-learning.html","id":"statististical-learning","chapter":"2 Statististical learning","heading":"2 Statististical learning","text":"","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"","code":""},{"path":"linear-regression.html","id":"libraries","chapter":"3 Linear Regression","heading":"3.1 Libraries","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.2 ──## ✓ broom     0.7.5           ✓ recipes   0.1.15.9000\n## ✓ dials     0.0.9           ✓ rsample   0.0.9      \n## ✓ dplyr     1.0.5           ✓ tibble    3.1.0      \n## ✓ ggplot2   3.3.3           ✓ tidyr     1.1.3      \n## ✓ infer     0.5.4           ✓ tune      0.1.3      \n## ✓ modeldata 0.1.0           ✓ workflows 0.2.2      \n## ✓ parsnip   0.1.5.9002      ✓ yardstick 0.0.8      \n## ✓ purrr     0.3.4## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\nlibrary(ISLR)\nlibrary(MASS) # For Boston data set## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nBoston <- as_tibble(Boston)\nCarseats <- as_tibble(Carseats)"},{"path":"linear-regression.html","id":"simple-linear-regression","chapter":"3 Linear Regression","heading":"3.2 Simple linear regression","text":"","code":"\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat, data = Boston)\nnames(lm_fit)## [1] \"lvl\"     \"spec\"    \"fit\"     \"preproc\" \"elapsed\"\nnames(lm_fit$fit)##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\nsummary(lm_fit$fit)## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\ntidy(lm_fit)## # A tibble: 2 x 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   34.6      0.563       61.4 3.74e-236\n## 2 lstat         -0.950    0.0387     -24.5 5.08e- 88\nglance(lm_fit)## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.544         0.543  6.22      602. 5.08e-88     1 -1641. 3289. 3302.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npredict(lm_fit)## Error in predict_numeric(object = object, new_data = new_data, ...): argument \"new_data\" is missing, with no default\npredict(lm_fit, new_data = Boston)## # A tibble: 506 x 1\n##    .pred\n##    <dbl>\n##  1 29.8 \n##  2 25.9 \n##  3 30.7 \n##  4 31.8 \n##  5 29.5 \n##  6 29.6 \n##  7 22.7 \n##  8 16.4 \n##  9  6.12\n## 10 18.3 \n## # … with 496 more rows\npredict(lm_fit, new_data = Boston, type = \"conf_int\")## # A tibble: 506 x 2\n##    .pred_lower .pred_upper\n##          <dbl>       <dbl>\n##  1       29.0        30.6 \n##  2       25.3        26.5 \n##  3       29.9        31.6 \n##  4       30.8        32.7 \n##  5       28.7        30.3 \n##  6       28.8        30.4 \n##  7       22.2        23.3 \n##  8       15.6        17.1 \n##  9        4.70        7.54\n## 10       17.7        18.9 \n## # … with 496 more rows\naugment(lm_fit, new_data = Boston)## # A tibble: 506 x 16\n##       crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio black\n##      <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl>   <dbl> <dbl>\n##  1 0.00632  18    2.31     0 0.538  6.58  65.2  4.09     1   296    15.3  397.\n##  2 0.0273    0    7.07     0 0.469  6.42  78.9  4.97     2   242    17.8  397.\n##  3 0.0273    0    7.07     0 0.469  7.18  61.1  4.97     2   242    17.8  393.\n##  4 0.0324    0    2.18     0 0.458  7.00  45.8  6.06     3   222    18.7  395.\n##  5 0.0690    0    2.18     0 0.458  7.15  54.2  6.06     3   222    18.7  397.\n##  6 0.0298    0    2.18     0 0.458  6.43  58.7  6.06     3   222    18.7  394.\n##  7 0.0883   12.5  7.87     0 0.524  6.01  66.6  5.56     5   311    15.2  396.\n##  8 0.145    12.5  7.87     0 0.524  6.17  96.1  5.95     5   311    15.2  397.\n##  9 0.211    12.5  7.87     0 0.524  5.63 100    6.08     5   311    15.2  387.\n## 10 0.170    12.5  7.87     0 0.524  6.00  85.9  6.59     5   311    15.2  387.\n## # … with 496 more rows, and 4 more variables: lstat <dbl>, medv <dbl>,\n## #   .pred <dbl>, .resid <dbl>\npar(mfrow = c(2, 2))\nplot(lm_fit$fit)"},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"3 Linear Regression","heading":"3.3 Multiple linear regression","text":"","code":"\nlm_fit <- lm_spec %>% \n  fit(medv ~ lstat + age, data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  1ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat + age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  \n##    33.22276     -1.03207      0.03454"},{"path":"linear-regression.html","id":"interaction-terms","chapter":"3 Linear Regression","heading":"3.4 Interaction terms","text":"","code":"\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat * age, data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  1ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat * age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age    lstat:age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560\nrec_spec <- recipe(medv ~ lstat + age, data = Boston) %>%\n  step_interact(~ lstat:age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## ● step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  lstat_x_age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560"},{"path":"linear-regression.html","id":"non-linear-transformations-of-the-predictors","chapter":"3 Linear Regression","heading":"3.5 Non-linear transformations of the predictors","text":"","code":"\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat + I(lstat ^ 2), data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat + I(lstat^2), data = data)\n## \n## Coefficients:\n## (Intercept)        lstat   I(lstat^2)  \n##    42.86201     -2.33282      0.04355\nrec_spec <- recipe(medv ~ lstat, data = Boston) %>%\n  step_mutate(lstat2 = lstat ^ 2)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_fit2 <- lm_wf %>% fit(Boston)\nrec_spec <- recipe(medv ~ lstat, data = Boston) %>%\n  step_log(lstat)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## ● step_log()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       52.12       -12.48"},{"path":"linear-regression.html","id":"qualitative-predictors","chapter":"3 Linear Regression","heading":"3.6 Qualitative predictors","text":"","code":"\nCarseats## # A tibble: 400 x 11\n##    Sales CompPrice Income Advertising Population Price ShelveLoc   Age Education\n##    <dbl>     <dbl>  <dbl>       <dbl>      <dbl> <dbl> <fct>     <dbl>     <dbl>\n##  1  9.5        138     73          11        276   120 Bad          42        17\n##  2 11.2        111     48          16        260    83 Good         65        10\n##  3 10.1        113     35          10        269    80 Medium       59        12\n##  4  7.4        117    100           4        466    97 Medium       55        14\n##  5  4.15       141     64           3        340   128 Bad          38        13\n##  6 10.8        124    113          13        501    72 Bad          78        16\n##  7  6.63       115    105           0         45   108 Medium       71        15\n##  8 11.8        136     81          15        425   120 Good         67        10\n##  9  6.54       132    110           0        108   124 Medium       76        10\n## 10  4.69       132    113           0        131   124 Medium       76        17\n## # … with 390 more rows, and 2 more variables: Urban <fct>, US <fct>\nlm_spec %>% \n  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = Sales ~ . + Income:Advertising + Price:Age, \n##     data = data)\n## \n## Coefficients:\n##        (Intercept)           CompPrice              Income         Advertising  \n##          6.5755654           0.0929371           0.0108940           0.0702462  \n##         Population               Price       ShelveLocGood     ShelveLocMedium  \n##          0.0001592          -0.1008064           4.8486762           1.9532620  \n##                Age           Education            UrbanYes               USYes  \n##         -0.0579466          -0.0208525           0.1401597          -0.1575571  \n## Income:Advertising           Price:Age  \n##          0.0007510           0.0001068\nrec_spec <- recipe(Sales ~ ., data = Carseats) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~ Income:Advertising + Price:Age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Carseats)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## ● step_dummy()\n## ● step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##          (Intercept)             CompPrice                Income  \n##            6.5755654             0.0929371             0.0108940  \n##          Advertising            Population                 Price  \n##            0.0702462             0.0001592            -0.1008064  \n##                  Age             Education        ShelveLoc_Good  \n##           -0.0579466            -0.0208525             4.8486762  \n##     ShelveLoc_Medium             Urban_Yes                US_Yes  \n##            1.9532620             0.1401597            -0.1575571  \n## Income_x_Advertising           Price_x_Age  \n##            0.0007510             0.0001068"},{"path":"linear-regression.html","id":"writing-functions","chapter":"3 Linear Regression","heading":"3.7 Writing functions","text":"","code":""},{"path":"classification.html","id":"classification","chapter":"4 Classification","heading":"4 Classification","text":"","code":""},{"path":"classification.html","id":"the-stock-market-data","chapter":"4 Classification","heading":"4.1 The Stock Market Data","text":"","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\n\nSmarket <- as_tibble(Smarket)\nCaravan <- as_tibble(Caravan)\nlibrary(corrr)\ncor_Smarket <- correlate(Smarket[-9])## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))## Don't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\nlibrary(paletteer)\ncor_Smarket %>%\n  stretch() %>%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1) +\n  theme_minimal()"},{"path":"classification.html","id":"logistic-regression","chapter":"4 Classification","heading":"4.2 Logistic Regression","text":"","code":"\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket\n    )\n\nlr_fit## parsnip model object\n## \n## Fit time:  6ms \n## \n## Call:  stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Coefficients:\n## (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  \n##   -0.126000    -0.073074    -0.042301     0.011085     0.009359     0.010313  \n##      Volume  \n##    0.135441  \n## \n## Degrees of Freedom: 1249 Total (i.e. Null);  1243 Residual\n## Null Deviance:       1731 \n## Residual Deviance: 1728  AIC: 1742\nsummary(lr_fit$fit)## \n## Call:\n## stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.446  -1.203   1.065   1.145   1.326  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.126000   0.240736  -0.523    0.601\n## Lag1        -0.073074   0.050167  -1.457    0.145\n## Lag2        -0.042301   0.050086  -0.845    0.398\n## Lag3         0.011085   0.049939   0.222    0.824\n## Lag4         0.009359   0.049974   0.187    0.851\n## Lag5         0.010313   0.049511   0.208    0.835\n## Volume       0.135441   0.158360   0.855    0.392\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1731.2  on 1249  degrees of freedom\n## Residual deviance: 1727.6  on 1243  degrees of freedom\n## AIC: 1741.6\n## \n## Number of Fisher Scoring iterations: 3\ntidy(lr_fit)## # A tibble: 7 x 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) -0.126      0.241     -0.523   0.601\n## 2 Lag1        -0.0731     0.0502    -1.46    0.145\n## 3 Lag2        -0.0423     0.0501    -0.845   0.398\n## 4 Lag3         0.0111     0.0499     0.222   0.824\n## 5 Lag4         0.00936    0.0500     0.187   0.851\n## 6 Lag5         0.0103     0.0495     0.208   0.835\n## 7 Volume       0.135      0.158      0.855   0.392\npredict(lr_fit, new_data = Smarket)## # A tibble: 1,250 x 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Down       \n##  3 Down       \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Down       \n##  8 Up         \n##  9 Up         \n## 10 Down       \n## # … with 1,240 more rows\npredict(lr_fit, new_data = Smarket, type = \"prob\")## # A tibble: 1,250 x 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.493    0.507\n##  2      0.519    0.481\n##  3      0.519    0.481\n##  4      0.485    0.515\n##  5      0.489    0.511\n##  6      0.493    0.507\n##  7      0.507    0.493\n##  8      0.491    0.509\n##  9      0.482    0.518\n## 10      0.511    0.489\n## # … with 1,240 more rows\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class)##           Truth\n## Prediction Down  Up\n##       Down  145 141\n##       Up    457 507\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\naugment(lr_fit, new_data = Smarket) %>%\n  accuracy(truth = Direction, estimate = .pred_class)## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.522\nSmarket_train <- Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- Smarket %>%\n  filter(Year == 2005)\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket_train\n    )\naugment(lr_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   77 97\n##       Up     34 44\naugment(lr_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.480\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2,\n    data = Smarket_train\n    )\naugment(lr_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lr_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560\npredict(\n  lr_fit,\n  new_data = tibble(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), \n  type = \"prob\"\n)## # A tibble: 2 x 2\n##   .pred_Down .pred_Up\n##        <dbl>    <dbl>\n## 1      0.521    0.479\n## 2      0.504    0.496"},{"path":"classification.html","id":"linear-discriminant-analysis","chapter":"4 Classification","heading":"4.3 Linear Discriminant Analysis","text":"","code":"\nlibrary(discrim)## \n## Attaching package: 'discrim'## The following object is masked from 'package:dials':\n## \n##     smoothness\nlda_spec <- discrim_linear() %>%\n  set_engine(\"MASS\")\n\nlda_fit <- lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nlda_fit## parsnip model object\n## \n## Fit time:  4ms \n## Call:\n## lda(Direction ~ Lag1 + Lag2, data = data)\n## \n## Prior probabilities of groups:\n##     Down       Up \n## 0.491984 0.508016 \n## \n## Group means:\n##             Lag1        Lag2\n## Down  0.04279022  0.03389409\n## Up   -0.03954635 -0.03132544\n## \n## Coefficients of linear discriminants:\n##             LD1\n## Lag1 -0.6420190\n## Lag2 -0.5135293\npredict(lda_fit, new_data = Smarket_test)## # A tibble: 252 x 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Up         \n##  3 Up         \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Up         \n##  8 Up         \n##  9 Up         \n## 10 Up         \n## # … with 242 more rows\npredict(lda_fit, new_data = Smarket_test, type = \"prob\")## # A tibble: 252 x 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.490    0.510\n##  2      0.479    0.521\n##  3      0.467    0.533\n##  4      0.474    0.526\n##  5      0.493    0.507\n##  6      0.494    0.506\n##  7      0.495    0.505\n##  8      0.487    0.513\n##  9      0.491    0.509\n## 10      0.484    0.516\n## # … with 242 more rows\naugment(lda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560"},{"path":"classification.html","id":"quadratic-discriminant-analysis","chapter":"4 Classification","heading":"4.4 Quadratic Discriminant Analysis","text":"","code":"\nlibrary(discrim)\nqda_spec <- discrim_regularized() %>%\n  set_args(frac_common_cov = 0, frac_identity = 0) %>%\n  set_engine(\"klaR\")\n\nqda_fit <- qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nqda_fit## parsnip model object\n## \n## Fit time:  50ms \n## Call: \n## rda(formula = Direction ~ Lag1 + Lag2, data = data, lambda = ~0, \n##     gamma = ~0)\n## \n## Regularization parameters: \n##  gamma lambda \n##      0      0 \n## \n## Prior probabilities of groups: \n##     Down       Up \n## 0.491984 0.508016 \n## \n## Misclassification rate: \n##        apparent: 48.597 %\naugment(qda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   30  20\n##       Up     81 121\naugment(qda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.599"},{"path":"classification.html","id":"k-nearest-neighbors","chapter":"4 Classification","heading":"4.5 K-Nearest Neighbors","text":"","code":"\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\nknn_fit <- knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nknn_fit## parsnip model object\n## \n## Fit time:  35ms \n## \n## Call:\n## kknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n## \n## Type of response variable: nominal\n## Minimal misclassification: 0.492986\n## Best kernel: optimal\n## Best k: 3\naugment(knn_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   43 58\n##       Up     68 83\naugment(knn_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary           0.5"},{"path":"classification.html","id":"an-application-to-caravan-insurance-data","chapter":"4 Classification","heading":"4.6 An Application to Caravan Insurance Data","text":"","code":"\nCaravan_test <- Caravan[seq_len(1000), ]\nCaravan_train <- Caravan[-seq_len(1000), ]\nrec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%\n  step_normalize(all_numeric_predictors())\nCaravan_wf <- workflow() %>%\n  add_recipe(rec_spec)\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\nknn1_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 1))\nknn1_fit <- fit(knn1_wf, data = Caravan_train)\naugment(knn1_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9\naugment(knn1_fit, new_data = Caravan_test) %>%\n  accuracy(truth = Purchase, estimate = .pred_class)## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.883\nknn3_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 3))\n\nknn3_fit <- fit(knn3_wf, Caravan_train)\naugment(knn3_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  875  50\n##        Yes  66   9\nknn5_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 5))\n\nknn5_fit <- fit(knn5_wf, Caravan_train)\naugment(knn5_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9"},{"path":"resampling-methods.html","id":"resampling-methods","chapter":"5 Resampling Methods","heading":"5 Resampling Methods","text":"","code":"\nlibrary(tidymodels)\nlibrary(ISLR)\n\nAuto <- as_tibble(Auto)\nPortfolio <- as_tibble(Portfolio)"},{"path":"resampling-methods.html","id":"the-validation-set-approach","chapter":"5 Resampling Methods","heading":"5.1 The Validation Set Approach","text":"","code":"\nset.seed(1)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\nlm_fit <- lm_spec %>% \n  fit(mpg ~ horsepower, data = Auto_train)\naugment(lm_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        5.11\npoly_fit <- lm_spec %>% \n  fit(mpg ~ poly(horsepower, 2), data = Auto_train)\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.54\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = 2)\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\npoly_fit <- fit(poly_wf, data = Auto_train)\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.54\nset.seed(2)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\npoly_fit <- fit(poly_wf, data = Auto_train)\n\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.66"},{"path":"resampling-methods.html","id":"leave-one-out-cross-validation","chapter":"5 Resampling Methods","heading":"5.2 Leave-One-Out Cross-Validation","text":"Leave-One-Cross-Validation integrated broader tidymodels framework. information read .","code":""},{"path":"resampling-methods.html","id":"k-fold-cross-validation","chapter":"5 Resampling Methods","heading":"5.3 k-Fold Cross-Validation","text":"can helpful add control = control_grid(verbose = TRUE)","code":"\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = tune())\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\nAuto_folds <- vfold_cv(Auto_train, v = 10)\n\ndegree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)\n\ntune_res <- tune_grid(\n  object = poly_wf, \n  resamples = Auto_folds, \n  grid = degree_grid\n)\nautoplot(tune_res)\ncollect_metrics(tune_res)## # A tibble: 20 x 7\n##    degree .metric .estimator  mean     n std_err .config              \n##     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1      1 rmse    standard   4.84     10  0.194  Preprocessor01_Model1\n##  2      1 rsq     standard   0.635    10  0.0209 Preprocessor01_Model1\n##  3      2 rmse    standard   4.29     10  0.155  Preprocessor02_Model1\n##  4      2 rsq     standard   0.709    10  0.0224 Preprocessor02_Model1\n##  5      3 rmse    standard   4.31     10  0.154  Preprocessor03_Model1\n##  6      3 rsq     standard   0.707    10  0.0219 Preprocessor03_Model1\n##  7      4 rmse    standard   4.33     10  0.152  Preprocessor04_Model1\n##  8      4 rsq     standard   0.706    10  0.0217 Preprocessor04_Model1\n##  9      5 rmse    standard   4.30     10  0.149  Preprocessor05_Model1\n## 10      5 rsq     standard   0.710    10  0.0208 Preprocessor05_Model1\n## 11      6 rmse    standard   4.32     10  0.149  Preprocessor06_Model1\n## 12      6 rsq     standard   0.708    10  0.0203 Preprocessor06_Model1\n## 13      7 rmse    standard   4.34     10  0.152  Preprocessor07_Model1\n## 14      7 rsq     standard   0.705    10  0.0203 Preprocessor07_Model1\n## 15      8 rmse    standard   4.37     10  0.151  Preprocessor08_Model1\n## 16      8 rsq     standard   0.704    10  0.0212 Preprocessor08_Model1\n## 17      9 rmse    standard   4.41     10  0.172  Preprocessor09_Model1\n## 18      9 rsq     standard   0.702    10  0.0226 Preprocessor09_Model1\n## 19     10 rmse    standard   4.56     10  0.158  Preprocessor10_Model1\n## 20     10 rsq     standard   0.689    10  0.0198 Preprocessor10_Model1\nshow_best(tune_res, metric = \"rmse\")## # A tibble: 5 x 7\n##   degree .metric .estimator  mean     n std_err .config              \n##    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n## 1      2 rmse    standard    4.29    10   0.155 Preprocessor02_Model1\n## 2      5 rmse    standard    4.30    10   0.149 Preprocessor05_Model1\n## 3      3 rmse    standard    4.31    10   0.154 Preprocessor03_Model1\n## 4      6 rmse    standard    4.32    10   0.149 Preprocessor06_Model1\n## 5      4 rmse    standard    4.33    10   0.152 Preprocessor04_Model1\nbest_degree <- select_best(tune_res, metric = \"rmse\")\nfinal_wf <- finalize_workflow(poly_wf, best_degree)\n\nfinal_wf## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## ● step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nfinal_fit <- fit(final_wf, Auto_train)\n\nfinal_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## ● step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##       (Intercept)  horsepower_poly_1  horsepower_poly_2  \n##             23.49            -106.32              38.94"},{"path":"resampling-methods.html","id":"the-bootstrap","chapter":"5 Resampling Methods","heading":"5.4 The Bootstrap","text":"","code":"\nPortfolio_boots <- bootstraps(Portfolio, times = 1000)\n\nalpha.fn <- function(split) {\n  data <- analysis(split)\n  X <- data$X\n  Y <- data$Y\n  \n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nalpha_res <- Portfolio_boots %>%\n  mutate(alpha = map_dbl(splits, alpha.fn))\n\nalpha_res## # Bootstrap sampling \n## # A tibble: 1,000 x 3\n##    splits           id            alpha\n##    <list>           <chr>         <dbl>\n##  1 <split [100/37]> Bootstrap0001 0.457\n##  2 <split [100/38]> Bootstrap0002 0.522\n##  3 <split [100/33]> Bootstrap0003 0.482\n##  4 <split [100/35]> Bootstrap0004 0.614\n##  5 <split [100/35]> Bootstrap0005 0.598\n##  6 <split [100/38]> Bootstrap0006 0.624\n##  7 <split [100/34]> Bootstrap0007 0.700\n##  8 <split [100/40]> Bootstrap0008 0.416\n##  9 <split [100/34]> Bootstrap0009 0.540\n## 10 <split [100/36]> Bootstrap0010 0.735\n## # … with 990 more rows\nAuto_boots <- bootstraps(Auto)\n\nboot.fn <- function(split) {\n  lm_fit <- lm_spec %>% fit(mpg ~ horsepower, data = analysis(split))\n  tidy(lm_fit)\n}\n\nboot_res <- Auto_boots %>%\n  mutate(models = map(splits, boot.fn))\n\nboot_res %>%\n  unnest(cols = c(models)) %>%\n  group_by(term) %>%\n  summarise(mean = mean(estimate),\n            sd = sd(estimate))## # A tibble: 2 x 3\n##   term          mean      sd\n##   <chr>        <dbl>   <dbl>\n## 1 (Intercept) 39.8   0.718  \n## 2 horsepower  -0.156 0.00620"},{"path":"linear-model-selection-and-regularization.html","id":"linear-model-selection-and-regularization","chapter":"6 Linear Model Selection and Regularization","heading":"6 Linear Model Selection and Regularization","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"best-subset-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.1 Best Subset Selection","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"forward-and-backward-stepwise-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.2 Forward and Backward Stepwise Selection","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"choosing-amoung-models-using-the-validation-set-approach-and-cross-validation","chapter":"6 Linear Model Selection and Regularization","heading":"6.3 Choosing Amoung Models Using the Validation Set Approach and Cross-Validation","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"ridge-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.4 Ridge Regression","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"the-lasso","chapter":"6 Linear Model Selection and Regularization","heading":"6.5 The Lasso","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"principal-components-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.6 Principal Components Regression","text":"","code":""},{"path":"linear-model-selection-and-regularization.html","id":"partial-least-squares","chapter":"6 Linear Model Selection and Regularization","heading":"6.7 Partial Least Squares","text":"","code":""},{"path":"moving-beyond-linearity.html","id":"moving-beyond-linearity","chapter":"7 Moving Beyond Linearity","heading":"7 Moving Beyond Linearity","text":"","code":""},{"path":"moving-beyond-linearity.html","id":"polynomial-regression-and-step-functions","chapter":"7 Moving Beyond Linearity","heading":"7.1 Polynomial Regression and Step Functions","text":"","code":""},{"path":"moving-beyond-linearity.html","id":"splines","chapter":"7 Moving Beyond Linearity","heading":"7.2 Splines","text":"","code":""},{"path":"moving-beyond-linearity.html","id":"gams","chapter":"7 Moving Beyond Linearity","heading":"7.3 GAMs","text":"","code":""},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"8 Tree-Based Methods","heading":"8 Tree-Based Methods","text":"","code":""},{"path":"tree-based-methods.html","id":"fitting-classification-trees","chapter":"8 Tree-Based Methods","heading":"8.1 Fitting Classification Trees","text":"","code":""},{"path":"tree-based-methods.html","id":"fitting-regression-trees","chapter":"8 Tree-Based Methods","heading":"8.2 Fitting Regression Trees","text":"","code":""},{"path":"tree-based-methods.html","id":"bagging-and-random-forests","chapter":"8 Tree-Based Methods","heading":"8.3 Bagging and Random Forests","text":"","code":""},{"path":"tree-based-methods.html","id":"boosting","chapter":"8 Tree-Based Methods","heading":"8.4 Boosting","text":"","code":""},{"path":"support-vector-machines.html","id":"support-vector-machines","chapter":"9 Support Vector Machines","heading":"9 Support Vector Machines","text":"","code":""},{"path":"support-vector-machines.html","id":"support-vector-classifier","chapter":"9 Support Vector Machines","heading":"9.1 Support Vector Classifier","text":"","code":""},{"path":"support-vector-machines.html","id":"support-vector-machine","chapter":"9 Support Vector Machines","heading":"9.2 Support Vector Machine","text":"","code":""},{"path":"support-vector-machines.html","id":"roc-curves","chapter":"9 Support Vector Machines","heading":"9.3 ROC Curves","text":"","code":""},{"path":"support-vector-machines.html","id":"svm-with-multiple-classes","chapter":"9 Support Vector Machines","heading":"9.4 SVM with Multiple Classes","text":"","code":""},{"path":"support-vector-machines.html","id":"application-to-gene-expression-data","chapter":"9 Support Vector Machines","heading":"9.5 Application to Gene Expression Data","text":"","code":""},{"path":"unsupervised-learning.html","id":"unsupervised-learning","chapter":"10 Unsupervised Learning","heading":"10 Unsupervised Learning","text":"","code":""},{"path":"unsupervised-learning.html","id":"principal-components-analysis","chapter":"10 Unsupervised Learning","heading":"10.1 Principal Components Analysis","text":"","code":""},{"path":"unsupervised-learning.html","id":"k-means-clustering","chapter":"10 Unsupervised Learning","heading":"10.2 K-Means Clustering","text":"","code":""},{"path":"unsupervised-learning.html","id":"hierarchical-clustering","chapter":"10 Unsupervised Learning","heading":"10.3 Hierarchical Clustering","text":"","code":""},{"path":"unsupervised-learning.html","id":"pca-on-the-nci60-data","chapter":"10 Unsupervised Learning","heading":"10.4 PCA on the NCI60 Data","text":"","code":""},{"path":"unsupervised-learning.html","id":"clustering-the-observations-of-the-nci60-data","chapter":"10 Unsupervised Learning","heading":"10.5 Clustering the Observations of the NCI60 Data","text":"","code":""}]
