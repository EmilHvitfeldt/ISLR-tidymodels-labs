[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"book aims complement 1st version Introduction Statistical Learning book translations labs using tidymodels set packages.labs mirrored quite closely stay true original material.","code":""},{"path":"index.html","id":"colophon","chapter":"1 Introduction","heading":"Colophon","text":"book written RStudio using bookdown. website hosted via GitHub Pages, complete source available GitHub.version book built R version 4.0.2 (2020-06-22) following packages:","code":""},{"path":"statististical-learning.html","id":"statististical-learning","chapter":"2 Statististical learning","heading":"2 Statististical learning","text":"original labs introduce basics R. repeat endeavor done well. notice labs book look slightly different since following tidyverse style guide.primary purpose book rewrite labs ISLR using tidymodels packages. great introduction tidymodels can found tidymodels website.Proper introductions various tidymodels packages present labs since aim mirror labs ISLR. getting started page good introductions toparsniprecipes workflowsrsampletuneThe charts whenever possible created using ggplot2, unfamiliar ggplot2 recommend start reading Data Visualisation chapter R Data Science book.tidymodels packages can installed whole using install.packages(\"tidymodels\") ISLR package contains many data set using can installed using install.packages(\"ISLR\").","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"lab go perform linear regression. include simple linear regression multiple linear regression addition can apply transformations predictors. chapter use parsnip model fitting recipes workflows perform transformations.","code":""},{"path":"linear-regression.html","id":"libraries","chapter":"3 Linear Regression","heading":"3.1 Libraries","text":"load tidymodels ISLR MASS data sets.","code":"\nlibrary(MASS) # For Boston data set\nlibrary(tidymodels)\nlibrary(ISLR)"},{"path":"linear-regression.html","id":"simple-linear-regression","chapter":"3 Linear Regression","heading":"3.2 Simple linear regression","text":"Boston data set contain various statistics 506 neighborhoods Boston. build simple linear regression model related median value owner-occupied homes (medv) response variable indicating percentage population belongs lower status (lstat) predictor.\nBoston data set quite outdated contains really unfortunate variables.\nstart creating parsnip specification linear regression model.unnecessary set mode linear regression since can regression, continue labs explicit.specification doesn’t perform calculations . just specification want .specification can fit supplying formula expression data want fit model .\nformula written form y ~ x y name response x name predictors.\nnames used formula match names variables data set passed data.result fit parsnip model object. object contains underlying fit well parsnip-specific information. want look underlying fit object can access lm_fit$fit withThe lm object nice summary() method shows information fit, including parameter estimates lack--fit statistics.can use packages broom package extract key information model objects tidy formats.tidy() function returns parameter estimates lm objectand glance() can used extract model statistics.Suppose like model fit want generate predictions, typically use predict() function like :produces error used parsnip model object. happening need explicitly supply data set predictions performed via new_data argumentNotice predictions returned tibble. always case parsnip models, matter engine used. useful since consistency allows us combine data sets easily.can also return types predicts specifying type argument. Setting type = \"conf_int\" return 95% confidence interval.\nengines can return types predictions.\nwant evaluate performance model, might want compare observed value predicted value data set. YouYou can get results using augment() function little bit typing","code":"\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\nlm_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat, data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       34.55        -0.95\nlm_fit %>% \n  pluck(\"fit\")## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       34.55        -0.95\nlm_fit %>% \n  pluck(\"fit\") %>%\n  summary()## \n## Call:\n## stats::lm(formula = medv ~ lstat, data = data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\ntidy(lm_fit)## # A tibble: 2 x 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   34.6      0.563       61.4 3.74e-236\n## 2 lstat         -0.950    0.0387     -24.5 5.08e- 88\nglance(lm_fit)## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.544         0.543  6.22      602. 5.08e-88     1 -1641. 3289. 3302.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npredict(lm_fit)## Error in predict_numeric(object = object, new_data = new_data, ...): argument \"new_data\" is missing, with no default\npredict(lm_fit, new_data = Boston)## # A tibble: 506 x 1\n##    .pred\n##    <dbl>\n##  1 29.8 \n##  2 25.9 \n##  3 30.7 \n##  4 31.8 \n##  5 29.5 \n##  6 29.6 \n##  7 22.7 \n##  8 16.4 \n##  9  6.12\n## 10 18.3 \n## # … with 496 more rows\npredict(lm_fit, new_data = Boston, type = \"conf_int\")## # A tibble: 506 x 2\n##    .pred_lower .pred_upper\n##          <dbl>       <dbl>\n##  1       29.0        30.6 \n##  2       25.3        26.5 \n##  3       29.9        31.6 \n##  4       30.8        32.7 \n##  5       28.7        30.3 \n##  6       28.8        30.4 \n##  7       22.2        23.3 \n##  8       15.6        17.1 \n##  9        4.70        7.54\n## 10       17.7        18.9 \n## # … with 496 more rows\nbind_cols(\n  predict(lm_fit, new_data = Boston),\n  Boston\n) %>%\n  select(medv, .pred)## # A tibble: 506 x 2\n##     medv .pred\n##    <dbl> <dbl>\n##  1  24   29.8 \n##  2  21.6 25.9 \n##  3  34.7 30.7 \n##  4  33.4 31.8 \n##  5  36.2 29.5 \n##  6  28.7 29.6 \n##  7  22.9 22.7 \n##  8  27.1 16.4 \n##  9  16.5  6.12\n## 10  18.9 18.3 \n## # … with 496 more rows\naugment(lm_fit, new_data = Boston) %>%\n  select(medv, .pred)## # A tibble: 506 x 2\n##     medv .pred\n##    <dbl> <dbl>\n##  1  24   29.8 \n##  2  21.6 25.9 \n##  3  34.7 30.7 \n##  4  33.4 31.8 \n##  5  36.2 29.5 \n##  6  28.7 29.6 \n##  7  22.9 22.7 \n##  8  27.1 16.4 \n##  9  16.5  6.12\n## 10  18.9 18.3 \n## # … with 496 more rows"},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"3 Linear Regression","heading":"3.3 Multiple linear regression","text":"multiple linear regression model can fit much way simple linear regression model. difference specify predictors. using formula expression y ~ x, can specify multiple values separating +s.Everything else works . extracting parameter estimatesto predicting new valuesA shortcut using formulas use form y ~ . means; set y response set remaining variables predictors. useful lot variables don’t want type .formula syntax look ?formula.","code":"\nlm_fit2 <- lm_spec %>% \n  fit(medv ~ lstat + age, data = Boston)\n\nlm_fit2## parsnip model object\n## \n## Fit time:  2ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat + age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  \n##    33.22276     -1.03207      0.03454\ntidy(lm_fit2)## # A tibble: 3 x 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)  33.2       0.731      45.5  2.94e-180\n## 2 lstat        -1.03      0.0482    -21.4  8.42e- 73\n## 3 age           0.0345    0.0122      2.83 4.91e-  3\npredict(lm_fit2, new_data = Boston)## # A tibble: 506 x 1\n##    .pred\n##    <dbl>\n##  1 30.3 \n##  2 26.5 \n##  3 31.2 \n##  4 31.8 \n##  5 29.6 \n##  6 29.9 \n##  7 22.7 \n##  8 16.8 \n##  9  5.79\n## 10 18.5 \n## # … with 496 more rows\nlm_fit3 <- lm_spec %>% \n  fit(medv ~ ., data = Boston)\n\nlm_fit3## parsnip model object\n## \n## Fit time:  3ms \n## \n## Call:\n## stats::lm(formula = medv ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)         crim           zn        indus         chas          nox  \n##   3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  \n##          rm          age          dis          rad          tax      ptratio  \n##   3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  \n##       black        lstat  \n##   9.312e-03   -5.248e-01"},{"path":"linear-regression.html","id":"interaction-terms","chapter":"3 Linear Regression","heading":"3.4 Interaction terms","text":"","code":"\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat * age, data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  2ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat * age, data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age    lstat:age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560\nrec_spec <- recipe(medv ~ lstat + age, data = Boston) %>%\n  step_interact(~ lstat:age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat          age  lstat_x_age  \n##  36.0885359   -1.3921168   -0.0007209    0.0041560"},{"path":"linear-regression.html","id":"non-linear-transformations-of-the-predictors","chapter":"3 Linear Regression","heading":"3.5 Non-linear transformations of the predictors","text":"","code":"\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat + I(lstat ^ 2), data = Boston)\n\nlm_fit## parsnip model object\n## \n## Fit time:  1ms \n## \n## Call:\n## stats::lm(formula = medv ~ lstat + I(lstat^2), data = data)\n## \n## Coefficients:\n## (Intercept)        lstat   I(lstat^2)  \n##    42.86201     -2.33282      0.04355\nrec_spec <- recipe(medv ~ lstat, data = Boston) %>%\n  step_mutate(lstat2 = lstat ^ 2)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_fit2 <- lm_wf %>% fit(Boston)\nrec_spec <- recipe(medv ~ lstat, data = Boston) %>%\n  step_log(lstat)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Boston)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_log()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)        lstat  \n##       52.12       -12.48"},{"path":"linear-regression.html","id":"qualitative-predictors","chapter":"3 Linear Regression","heading":"3.6 Qualitative predictors","text":"","code":"\nCarseats##     Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n## 1    9.50       138     73          11        276   120       Bad  42        17\n## 2   11.22       111     48          16        260    83      Good  65        10\n## 3   10.06       113     35          10        269    80    Medium  59        12\n## 4    7.40       117    100           4        466    97    Medium  55        14\n## 5    4.15       141     64           3        340   128       Bad  38        13\n## 6   10.81       124    113          13        501    72       Bad  78        16\n## 7    6.63       115    105           0         45   108    Medium  71        15\n## 8   11.85       136     81          15        425   120      Good  67        10\n## 9    6.54       132    110           0        108   124    Medium  76        10\n## 10   4.69       132    113           0        131   124    Medium  76        17\n## 11   9.01       121     78           9        150   100       Bad  26        10\n## 12  11.96       117     94           4        503    94      Good  50        13\n## 13   3.98       122     35           2        393   136    Medium  62        18\n## 14  10.96       115     28          11         29    86      Good  53        18\n## 15  11.17       107    117          11        148   118      Good  52        18\n## 16   8.71       149     95           5        400   144    Medium  76        18\n## 17   7.58       118     32           0        284   110      Good  63        13\n## 18  12.29       147     74          13        251   131      Good  52        10\n## 19  13.91       110    110           0        408    68      Good  46        17\n## 20   8.73       129     76          16         58   121    Medium  69        12\n## 21   6.41       125     90           2        367   131    Medium  35        18\n## 22  12.13       134     29          12        239   109      Good  62        18\n## 23   5.08       128     46           6        497   138    Medium  42        13\n## 24   5.87       121     31           0        292   109    Medium  79        10\n## 25  10.14       145    119          16        294   113       Bad  42        12\n## 26  14.90       139     32           0        176    82      Good  54        11\n## 27   8.33       107    115          11        496   131      Good  50        11\n## 28   5.27        98    118           0         19   107    Medium  64        17\n## 29   2.99       103     74           0        359    97       Bad  55        11\n## 30   7.81       104     99          15        226   102       Bad  58        17\n## 31  13.55       125     94           0        447    89      Good  30        12\n## 32   8.25       136     58          16        241   131    Medium  44        18\n## 33   6.20       107     32          12        236   137      Good  64        10\n## 34   8.77       114     38          13        317   128      Good  50        16\n## 35   2.67       115     54           0        406   128    Medium  42        17\n## 36  11.07       131     84          11         29    96    Medium  44        17\n## 37   8.89       122     76           0        270   100      Good  60        18\n## 38   4.95       121     41           5        412   110    Medium  54        10\n## 39   6.59       109     73           0        454   102    Medium  65        15\n## 40   3.24       130     60           0        144   138       Bad  38        10\n## 41   2.07       119     98           0         18   126       Bad  73        17\n## 42   7.96       157     53           0        403   124       Bad  58        16\n## 43  10.43        77     69           0         25    24    Medium  50        18\n## 44   4.12       123     42          11         16   134    Medium  59        13\n## 45   4.16        85     79           6        325    95    Medium  69        13\n## 46   4.56       141     63           0        168   135       Bad  44        12\n## 47  12.44       127     90          14         16    70    Medium  48        15\n## 48   4.38       126     98           0        173   108       Bad  55        16\n## 49   3.91       116     52           0        349    98       Bad  69        18\n## 50  10.61       157     93           0         51   149      Good  32        17\n## 51   1.42        99     32          18        341   108       Bad  80        16\n## 52   4.42       121     90           0        150   108       Bad  75        16\n## 53   7.91       153     40           3        112   129       Bad  39        18\n## 54   6.92       109     64          13         39   119    Medium  61        17\n## 55   4.90       134    103          13         25   144    Medium  76        17\n## 56   6.85       143     81           5         60   154    Medium  61        18\n## 57  11.91       133     82           0         54    84    Medium  50        17\n## 58   0.91        93     91           0         22   117       Bad  75        11\n## 59   5.42       103     93          15        188   103       Bad  74        16\n## 60   5.21       118     71           4        148   114    Medium  80        13\n## 61   8.32       122    102          19        469   123       Bad  29        13\n## 62   7.32       105     32           0        358   107    Medium  26        13\n## 63   1.82       139     45           0        146   133       Bad  77        17\n## 64   8.47       119     88          10        170   101    Medium  61        13\n## 65   7.80       100     67          12        184   104    Medium  32        16\n## 66   4.90       122     26           0        197   128    Medium  55        13\n## 67   8.85       127     92           0        508    91    Medium  56        18\n## 68   9.01       126     61          14        152   115    Medium  47        16\n## 69  13.39       149     69          20        366   134      Good  60        13\n## 70   7.99       127     59           0        339    99    Medium  65        12\n## 71   9.46        89     81          15        237    99      Good  74        12\n## 72   6.50       148     51          16        148   150    Medium  58        17\n## 73   5.52       115     45           0        432   116    Medium  25        15\n## 74  12.61       118     90          10         54   104      Good  31        11\n## 75   6.20       150     68           5        125   136    Medium  64        13\n## 76   8.55        88    111          23        480    92       Bad  36        16\n## 77  10.64       102     87          10        346    70    Medium  64        15\n## 78   7.70       118     71          12         44    89    Medium  67        18\n## 79   4.43       134     48           1        139   145    Medium  65        12\n## 80   9.14       134     67           0        286    90       Bad  41        13\n## 81   8.01       113    100          16        353    79       Bad  68        11\n## 82   7.52       116     72           0        237   128      Good  70        13\n## 83  11.62       151     83           4        325   139      Good  28        17\n## 84   4.42       109     36           7        468    94       Bad  56        11\n## 85   2.23       111     25           0         52   121       Bad  43        18\n## 86   8.47       125    103           0        304   112    Medium  49        13\n## 87   8.70       150     84           9        432   134    Medium  64        15\n## 88  11.70       131     67           7        272   126      Good  54        16\n## 89   6.56       117     42           7        144   111    Medium  62        10\n## 90   7.95       128     66           3        493   119    Medium  45        16\n## 91   5.33       115     22           0        491   103    Medium  64        11\n## 92   4.81        97     46          11        267   107    Medium  80        15\n## 93   4.53       114    113           0         97   125    Medium  29        12\n## 94   8.86       145     30           0         67   104    Medium  55        17\n## 95   8.39       115     97           5        134    84       Bad  55        11\n## 96   5.58       134     25          10        237   148    Medium  59        13\n## 97   9.48       147     42          10        407   132      Good  73        16\n## 98   7.45       161     82           5        287   129       Bad  33        16\n## 99  12.49       122     77          24        382   127      Good  36        16\n## 100  4.88       121     47           3        220   107       Bad  56        16\n## 101  4.11       113     69          11         94   106    Medium  76        12\n## 102  6.20       128     93           0         89   118    Medium  34        18\n## 103  5.30       113     22           0         57    97    Medium  65        16\n## 104  5.07       123     91           0        334    96       Bad  78        17\n## 105  4.62       121     96           0        472   138    Medium  51        12\n## 106  5.55       104    100           8        398    97    Medium  61        11\n## 107  0.16       102     33           0        217   139    Medium  70        18\n## 108  8.55       134    107           0        104   108    Medium  60        12\n## 109  3.47       107     79           2        488   103       Bad  65        16\n## 110  8.98       115     65           0        217    90    Medium  60        17\n## 111  9.00       128     62           7        125   116    Medium  43        14\n## 112  6.62       132    118          12        272   151    Medium  43        14\n## 113  6.67       116     99           5        298   125      Good  62        12\n## 114  6.01       131     29          11        335   127       Bad  33        12\n## 115  9.31       122     87           9         17   106    Medium  65        13\n## 116  8.54       139     35           0         95   129    Medium  42        13\n## 117  5.08       135     75           0        202   128    Medium  80        10\n## 118  8.80       145     53           0        507   119    Medium  41        12\n## 119  7.57       112     88           2        243    99    Medium  62        11\n## 120  7.37       130     94           8        137   128    Medium  64        12\n## 121  6.87       128    105          11        249   131    Medium  63        13\n## 122 11.67       125     89          10        380    87       Bad  28        10\n## 123  6.88       119    100           5         45   108    Medium  75        10\n## 124  8.19       127    103           0        125   155      Good  29        15\n## 125  8.87       131    113           0        181   120      Good  63        14\n## 126  9.34        89     78           0        181    49    Medium  43        15\n## 127 11.27       153     68           2         60   133      Good  59        16\n## 128  6.52       125     48           3        192   116    Medium  51        14\n## 129  4.96       133    100           3        350   126       Bad  55        13\n## 130  4.47       143    120           7        279   147       Bad  40        10\n## 131  8.41        94     84          13        497    77    Medium  51        12\n## 132  6.50       108     69           3        208    94    Medium  77        16\n## 133  9.54       125     87           9        232   136      Good  72        10\n## 134  7.62       132     98           2        265    97       Bad  62        12\n## 135  3.67       132     31           0        327   131    Medium  76        16\n## 136  6.44        96     94          14        384   120    Medium  36        18\n## 137  5.17       131     75           0         10   120       Bad  31        18\n## 138  6.52       128     42           0        436   118    Medium  80        11\n## 139 10.27       125    103          12        371   109    Medium  44        10\n## 140 12.30       146     62          10        310    94    Medium  30        13\n## 141  6.03       133     60          10        277   129    Medium  45        18\n## 142  6.53       140     42           0        331   131       Bad  28        15\n## 143  7.44       124     84           0        300   104    Medium  77        15\n## 144  0.53       122     88           7         36   159       Bad  28        17\n## 145  9.09       132     68           0        264   123      Good  34        11\n## 146  8.77       144     63          11         27   117    Medium  47        17\n## 147  3.90       114     83           0        412   131       Bad  39        14\n## 148 10.51       140     54           9        402   119      Good  41        16\n## 149  7.56       110    119           0        384    97    Medium  72        14\n## 150 11.48       121    120          13        140    87    Medium  56        11\n## 151 10.49       122     84           8        176   114      Good  57        10\n## 152 10.77       111     58          17        407   103      Good  75        17\n## 153  7.64       128     78           0        341   128      Good  45        13\n## 154  5.93       150     36           7        488   150    Medium  25        17\n## 155  6.89       129     69          10        289   110    Medium  50        16\n## 156  7.71        98     72           0         59    69    Medium  65        16\n## 157  7.49       146     34           0        220   157      Good  51        16\n## 158 10.21       121     58           8        249    90    Medium  48        13\n## 159 12.53       142     90           1        189   112      Good  39        10\n## 160  9.32       119     60           0        372    70       Bad  30        18\n## 161  4.67       111     28           0        486   111    Medium  29        12\n## 162  2.93       143     21           5         81   160    Medium  67        12\n## 163  3.63       122     74           0        424   149    Medium  51        13\n## 164  5.68       130     64           0         40   106       Bad  39        17\n## 165  8.22       148     64           0         58   141    Medium  27        13\n## 166  0.37       147     58           7        100   191       Bad  27        15\n## 167  6.71       119     67          17        151   137    Medium  55        11\n## 168  6.71       106     73           0        216    93    Medium  60        13\n## 169  7.30       129     89           0        425   117    Medium  45        10\n## 170 11.48       104     41          15        492    77      Good  73        18\n## 171  8.01       128     39          12        356   118    Medium  71        10\n## 172 12.49        93    106          12        416    55    Medium  75        15\n## 173  9.03       104    102          13        123   110      Good  35        16\n## 174  6.38       135     91           5        207   128    Medium  66        18\n## 175  0.00       139     24           0        358   185    Medium  79        15\n## 176  7.54       115     89           0         38   122    Medium  25        12\n## 177  5.61       138    107           9        480   154    Medium  47        11\n## 178 10.48       138     72           0        148    94    Medium  27        17\n## 179 10.66       104     71          14         89    81    Medium  25        14\n## 180  7.78       144     25           3         70   116    Medium  77        18\n## 181  4.94       137    112          15        434   149       Bad  66        13\n## 182  7.43       121     83           0         79    91    Medium  68        11\n## 183  4.74       137     60           4        230   140       Bad  25        13\n## 184  5.32       118     74           6        426   102    Medium  80        18\n## 185  9.95       132     33           7         35    97    Medium  60        11\n## 186 10.07       130    100          11        449   107    Medium  64        10\n## 187  8.68       120     51           0         93    86    Medium  46        17\n## 188  6.03       117     32           0        142    96       Bad  62        17\n## 189  8.07       116     37           0        426    90    Medium  76        15\n## 190 12.11       118    117          18        509   104    Medium  26        15\n## 191  8.79       130     37          13        297   101    Medium  37        13\n## 192  6.67       156     42          13        170   173      Good  74        14\n## 193  7.56       108     26           0        408    93    Medium  56        14\n## 194 13.28       139     70           7         71    96      Good  61        10\n## 195  7.23       112     98          18        481   128    Medium  45        11\n## 196  4.19       117     93           4        420   112       Bad  66        11\n## 197  4.10       130     28           6        410   133       Bad  72        16\n## 198  2.52       124     61           0        333   138    Medium  76        16\n## 199  3.62       112     80           5        500   128    Medium  69        10\n## 200  6.42       122     88           5        335   126    Medium  64        14\n## 201  5.56       144     92           0        349   146    Medium  62        12\n## 202  5.94       138     83           0        139   134    Medium  54        18\n## 203  4.10       121     78           4        413   130       Bad  46        10\n## 204  2.05       131     82           0        132   157       Bad  25        14\n## 205  8.74       155     80           0        237   124    Medium  37        14\n## 206  5.68       113     22           1        317   132    Medium  28        12\n## 207  4.97       162     67           0         27   160    Medium  77        17\n## 208  8.19       111    105           0        466    97       Bad  61        10\n## 209  7.78        86     54           0        497    64       Bad  33        12\n## 210  3.02        98     21          11        326    90       Bad  76        11\n## 211  4.36       125     41           2        357   123       Bad  47        14\n## 212  9.39       117    118          14        445   120    Medium  32        15\n## 213 12.04       145     69          19        501   105    Medium  45        11\n## 214  8.23       149     84           5        220   139    Medium  33        10\n## 215  4.83       115    115           3         48   107    Medium  73        18\n## 216  2.34       116     83          15        170   144       Bad  71        11\n## 217  5.73       141     33           0        243   144    Medium  34        17\n## 218  4.34       106     44           0        481   111    Medium  70        14\n## 219  9.70       138     61          12        156   120    Medium  25        14\n## 220 10.62       116     79          19        359   116      Good  58        17\n## 221 10.59       131    120          15        262   124    Medium  30        10\n## 222  6.43       124     44           0        125   107    Medium  80        11\n## 223  7.49       136    119           6        178   145    Medium  35        13\n## 224  3.45       110     45           9        276   125    Medium  62        14\n## 225  4.10       134     82           0        464   141    Medium  48        13\n## 226  6.68       107     25           0        412    82       Bad  36        14\n## 227  7.80       119     33           0        245   122      Good  56        14\n## 228  8.69       113     64          10         68   101    Medium  57        16\n## 229  5.40       149     73          13        381   163       Bad  26        11\n## 230 11.19        98    104           0        404    72    Medium  27        18\n## 231  5.16       115     60           0        119   114       Bad  38        14\n## 232  8.09       132     69           0        123   122    Medium  27        11\n## 233 13.14       137     80          10         24   105      Good  61        15\n## 234  8.65       123     76          18        218   120    Medium  29        14\n## 235  9.43       115     62          11        289   129      Good  56        16\n## 236  5.53       126     32           8         95   132    Medium  50        17\n## 237  9.32       141     34          16        361   108    Medium  69        10\n## 238  9.62       151     28           8        499   135    Medium  48        10\n## 239  7.36       121     24           0        200   133      Good  73        13\n## 240  3.89       123    105           0        149   118       Bad  62        16\n## 241 10.31       159     80           0        362   121    Medium  26        18\n## 242 12.01       136     63           0        160    94    Medium  38        12\n## 243  4.68       124     46           0        199   135    Medium  52        14\n## 244  7.82       124     25          13         87   110    Medium  57        10\n## 245  8.78       130     30           0        391   100    Medium  26        18\n## 246 10.00       114     43           0        199    88      Good  57        10\n## 247  6.90       120     56          20        266    90       Bad  78        18\n## 248  5.04       123    114           0        298   151       Bad  34        16\n## 249  5.36       111     52           0         12   101    Medium  61        11\n## 250  5.05       125     67           0         86   117       Bad  65        11\n## 251  9.16       137    105          10        435   156      Good  72        14\n## 252  3.72       139    111           5        310   132       Bad  62        13\n## 253  8.31       133     97           0         70   117    Medium  32        16\n## 254  5.64       124     24           5        288   122    Medium  57        12\n## 255  9.58       108    104          23        353   129      Good  37        17\n## 256  7.71       123     81           8        198    81       Bad  80        15\n## 257  4.20       147     40           0        277   144    Medium  73        10\n## 258  8.67       125     62          14        477   112    Medium  80        13\n## 259  3.47       108     38           0        251    81       Bad  72        14\n## 260  5.12       123     36          10        467   100       Bad  74        11\n## 261  7.67       129    117           8        400   101       Bad  36        10\n## 262  5.71       121     42           4        188   118    Medium  54        15\n## 263  6.37       120     77          15         86   132    Medium  48        18\n## 264  7.77       116     26           6        434   115    Medium  25        17\n## 265  6.95       128     29           5        324   159      Good  31        15\n## 266  5.31       130     35          10        402   129       Bad  39        17\n## 267  9.10       128     93          12        343   112      Good  73        17\n## 268  5.83       134     82           7        473   112       Bad  51        12\n## 269  6.53       123     57           0         66   105    Medium  39        11\n## 270  5.01       159     69           0        438   166    Medium  46        17\n## 271 11.99       119     26           0        284    89      Good  26        10\n## 272  4.55       111     56           0        504   110    Medium  62        16\n## 273 12.98       113     33           0         14    63      Good  38        12\n## 274 10.04       116    106           8        244    86    Medium  58        12\n## 275  7.22       135     93           2         67   119    Medium  34        11\n## 276  6.67       107    119          11        210   132    Medium  53        11\n## 277  6.93       135     69          14        296   130    Medium  73        15\n## 278  7.80       136     48          12        326   125    Medium  36        16\n## 279  7.22       114    113           2        129   151      Good  40        15\n## 280  3.42       141     57          13        376   158    Medium  64        18\n## 281  2.86       121     86          10        496   145       Bad  51        10\n## 282 11.19       122     69           7        303   105      Good  45        16\n## 283  7.74       150     96           0         80   154      Good  61        11\n## 284  5.36       135    110           0        112   117    Medium  80        16\n## 285  6.97       106     46          11        414    96       Bad  79        17\n## 286  7.60       146     26          11        261   131    Medium  39        10\n## 287  7.53       117    118          11        429   113    Medium  67        18\n## 288  6.88        95     44           4        208    72       Bad  44        17\n## 289  6.98       116     40           0         74    97    Medium  76        15\n## 290  8.75       143     77          25        448   156    Medium  43        17\n## 291  9.49       107    111          14        400   103    Medium  41        11\n## 292  6.64       118     70           0        106    89       Bad  39        17\n## 293 11.82       113     66          16        322    74      Good  76        15\n## 294 11.28       123     84           0         74    89      Good  59        10\n## 295 12.66       148     76           3        126    99      Good  60        11\n## 296  4.21       118     35          14        502   137    Medium  79        10\n## 297  8.21       127     44          13        160   123      Good  63        18\n## 298  3.07       118     83          13        276   104       Bad  75        10\n## 299 10.98       148     63           0        312   130      Good  63        15\n## 300  9.40       135     40          17        497    96    Medium  54        17\n## 301  8.57       116     78           1        158    99    Medium  45        11\n## 302  7.41        99     93           0        198    87    Medium  57        16\n## 303  5.28       108     77          13        388   110       Bad  74        14\n## 304 10.01       133     52          16        290    99    Medium  43        11\n## 305 11.93       123     98          12        408   134      Good  29        10\n## 306  8.03       115     29          26        394   132    Medium  33        13\n## 307  4.78       131     32           1         85   133    Medium  48        12\n## 308  5.90       138     92           0         13   120       Bad  61        12\n## 309  9.24       126     80          19        436   126    Medium  52        10\n## 310 11.18       131    111          13         33    80       Bad  68        18\n## 311  9.53       175     65          29        419   166    Medium  53        12\n## 312  6.15       146     68          12        328   132       Bad  51        14\n## 313  6.80       137    117           5        337   135       Bad  38        10\n## 314  9.33       103     81           3        491    54    Medium  66        13\n## 315  7.72       133     33          10        333   129      Good  71        14\n## 316  6.39       131     21           8        220   171      Good  29        14\n## 317 15.63       122     36           5        369    72      Good  35        10\n## 318  6.41       142     30           0        472   136      Good  80        15\n## 319 10.08       116     72          10        456   130      Good  41        14\n## 320  6.97       127     45          19        459   129    Medium  57        11\n## 321  5.86       136     70          12        171   152    Medium  44        18\n## 322  7.52       123     39           5        499    98    Medium  34        15\n## 323  9.16       140     50          10        300   139      Good  60        15\n## 324 10.36       107    105          18        428   103    Medium  34        12\n## 325  2.66       136     65           4        133   150       Bad  53        13\n## 326 11.70       144     69          11        131   104    Medium  47        11\n## 327  4.69       133     30           0        152   122    Medium  53        17\n## 328  6.23       112     38          17        316   104    Medium  80        16\n## 329  3.15       117     66           1         65   111       Bad  55        11\n## 330 11.27       100     54           9        433    89      Good  45        12\n## 331  4.99       122     59           0        501   112       Bad  32        14\n## 332 10.10       135     63          15        213   134    Medium  32        10\n## 333  5.74       106     33          20        354   104    Medium  61        12\n## 334  5.87       136     60           7        303   147    Medium  41        10\n## 335  7.63        93    117           9        489    83       Bad  42        13\n## 336  6.18       120     70          15        464   110    Medium  72        15\n## 337  5.17       138     35           6         60   143       Bad  28        18\n## 338  8.61       130     38           0        283   102    Medium  80        15\n## 339  5.97       112     24           0        164   101    Medium  45        11\n## 340 11.54       134     44           4        219   126      Good  44        15\n## 341  7.50       140     29           0        105    91       Bad  43        16\n## 342  7.38        98    120           0        268    93    Medium  72        10\n## 343  7.81       137    102          13        422   118    Medium  71        10\n## 344  5.99       117     42          10        371   121       Bad  26        14\n## 345  8.43       138     80           0        108   126      Good  70        13\n## 346  4.81       121     68           0        279   149      Good  79        12\n## 347  8.97       132    107           0        144   125    Medium  33        13\n## 348  6.88        96     39           0        161   112      Good  27        14\n## 349 12.57       132    102          20        459   107      Good  49        11\n## 350  9.32       134     27          18        467    96    Medium  49        14\n## 351  8.64       111    101          17        266    91    Medium  63        17\n## 352 10.44       124    115          16        458   105    Medium  62        16\n## 353 13.44       133    103          14        288   122      Good  61        17\n## 354  9.45       107     67          12        430    92    Medium  35        12\n## 355  5.30       133     31           1         80   145    Medium  42        18\n## 356  7.02       130    100           0        306   146      Good  42        11\n## 357  3.58       142    109           0        111   164      Good  72        12\n## 358 13.36       103     73           3        276    72    Medium  34        15\n## 359  4.17       123     96          10         71   118       Bad  69        11\n## 360  3.13       130     62          11        396   130       Bad  66        14\n## 361  8.77       118     86           7        265   114      Good  52        15\n## 362  8.68       131     25          10        183   104    Medium  56        15\n## 363  5.25       131     55           0         26   110       Bad  79        12\n## 364 10.26       111     75           1        377   108      Good  25        12\n## 365 10.50       122     21          16        488   131      Good  30        14\n## 366  6.53       154     30           0        122   162    Medium  57        17\n## 367  5.98       124     56          11        447   134    Medium  53        12\n## 368 14.37        95    106           0        256    53      Good  52        17\n## 369 10.71       109     22          10        348    79      Good  74        14\n## 370 10.26       135    100          22        463   122    Medium  36        14\n## 371  7.68       126     41          22        403   119       Bad  42        12\n## 372  9.08       152     81           0        191   126    Medium  54        16\n## 373  7.80       121     50           0        508    98    Medium  65        11\n## 374  5.58       137     71           0        402   116    Medium  78        17\n## 375  9.44       131     47           7         90   118    Medium  47        12\n## 376  7.90       132     46           4        206   124    Medium  73        11\n## 377 16.27       141     60          19        319    92      Good  44        11\n## 378  6.81       132     61           0        263   125    Medium  41        12\n## 379  6.11       133     88           3        105   119    Medium  79        12\n## 380  5.81       125    111           0        404   107       Bad  54        15\n## 381  9.64       106     64          10         17    89    Medium  68        17\n## 382  3.90       124     65          21        496   151       Bad  77        13\n## 383  4.95       121     28          19        315   121    Medium  66        14\n## 384  9.35        98    117           0         76    68    Medium  63        10\n## 385 12.85       123     37          15        348   112      Good  28        12\n## 386  5.87       131     73          13        455   132    Medium  62        17\n## 387  5.32       152    116           0        170   160    Medium  39        16\n## 388  8.67       142     73          14        238   115    Medium  73        14\n## 389  8.14       135     89          11        245    78       Bad  79        16\n## 390  8.44       128     42           8        328   107    Medium  35        12\n## 391  5.47       108     75           9         61   111    Medium  67        12\n## 392  6.10       153     63           0         49   124       Bad  56        16\n## 393  4.53       129     42          13        315   130       Bad  34        13\n## 394  5.57       109     51          10         26   120    Medium  30        17\n## 395  5.35       130     58          19        366   139       Bad  33        16\n## 396 12.57       138    108          17        203   128      Good  33        14\n## 397  6.14       139     23           3         37   120    Medium  55        11\n## 398  7.41       162     26          12        368   159    Medium  40        18\n## 399  5.94       100     79           7        284    95       Bad  50        12\n## 400  9.71       134     37           0         27   120      Good  49        16\n##     Urban  US\n## 1     Yes Yes\n## 2     Yes Yes\n## 3     Yes Yes\n## 4     Yes Yes\n## 5     Yes  No\n## 6      No Yes\n## 7     Yes  No\n## 8     Yes Yes\n## 9      No  No\n## 10     No Yes\n## 11     No Yes\n## 12    Yes Yes\n## 13    Yes  No\n## 14    Yes Yes\n## 15    Yes Yes\n## 16     No  No\n## 17    Yes  No\n## 18    Yes Yes\n## 19     No Yes\n## 20    Yes Yes\n## 21    Yes Yes\n## 22     No Yes\n## 23    Yes  No\n## 24    Yes  No\n## 25    Yes Yes\n## 26     No  No\n## 27     No Yes\n## 28    Yes  No\n## 29    Yes Yes\n## 30    Yes Yes\n## 31    Yes  No\n## 32    Yes Yes\n## 33     No Yes\n## 34    Yes Yes\n## 35    Yes Yes\n## 36     No Yes\n## 37     No  No\n## 38    Yes Yes\n## 39    Yes  No\n## 40     No  No\n## 41     No  No\n## 42    Yes  No\n## 43    Yes  No\n## 44    Yes Yes\n## 45    Yes Yes\n## 46    Yes Yes\n## 47     No Yes\n## 48    Yes  No\n## 49    Yes  No\n## 50    Yes  No\n## 51    Yes Yes\n## 52    Yes  No\n## 53    Yes Yes\n## 54    Yes Yes\n## 55     No Yes\n## 56    Yes Yes\n## 57    Yes  No\n## 58    Yes  No\n## 59    Yes Yes\n## 60    Yes  No\n## 61    Yes Yes\n## 62     No  No\n## 63    Yes Yes\n## 64    Yes Yes\n## 65     No Yes\n## 66     No  No\n## 67    Yes  No\n## 68    Yes Yes\n## 69    Yes Yes\n## 70    Yes  No\n## 71    Yes Yes\n## 72     No Yes\n## 73    Yes  No\n## 74     No Yes\n## 75     No Yes\n## 76     No Yes\n## 77    Yes Yes\n## 78     No Yes\n## 79    Yes Yes\n## 80    Yes  No\n## 81    Yes Yes\n## 82    Yes  No\n## 83    Yes Yes\n## 84    Yes Yes\n## 85     No  No\n## 86     No  No\n## 87    Yes  No\n## 88     No Yes\n## 89    Yes Yes\n## 90     No  No\n## 91     No  No\n## 92    Yes Yes\n## 93    Yes  No\n## 94    Yes  No\n## 95    Yes Yes\n## 96    Yes Yes\n## 97     No Yes\n## 98    Yes Yes\n## 99     No Yes\n## 100    No Yes\n## 101    No Yes\n## 102   Yes  No\n## 103    No  No\n## 104   Yes Yes\n## 105   Yes  No\n## 106   Yes Yes\n## 107    No  No\n## 108   Yes  No\n## 109   Yes  No\n## 110    No  No\n## 111   Yes Yes\n## 112   Yes Yes\n## 113   Yes Yes\n## 114   Yes Yes\n## 115   Yes Yes\n## 116   Yes  No\n## 117    No  No\n## 118   Yes  No\n## 119   Yes Yes\n## 120   Yes Yes\n## 121   Yes Yes\n## 122   Yes Yes\n## 123   Yes Yes\n## 124    No Yes\n## 125   Yes  No\n## 126    No  No\n## 127   Yes Yes\n## 128   Yes Yes\n## 129   Yes Yes\n## 130    No Yes\n## 131   Yes Yes\n## 132   Yes  No\n## 133   Yes Yes\n## 134   Yes Yes\n## 135   Yes  No\n## 136    No Yes\n## 137    No  No\n## 138   Yes  No\n## 139   Yes Yes\n## 140    No Yes\n## 141   Yes Yes\n## 142   Yes  No\n## 143   Yes  No\n## 144   Yes Yes\n## 145    No  No\n## 146   Yes Yes\n## 147   Yes  No\n## 148    No Yes\n## 149    No Yes\n## 150   Yes Yes\n## 151    No Yes\n## 152    No Yes\n## 153    No  No\n## 154    No Yes\n## 155    No Yes\n## 156   Yes  No\n## 157   Yes  No\n## 158    No Yes\n## 159    No Yes\n## 160    No  No\n## 161    No  No\n## 162    No Yes\n## 163   Yes  No\n## 164    No  No\n## 165    No Yes\n## 166   Yes Yes\n## 167   Yes Yes\n## 168   Yes  No\n## 169   Yes  No\n## 170   Yes Yes\n## 171   Yes Yes\n## 172   Yes Yes\n## 173   Yes Yes\n## 174   Yes Yes\n## 175    No  No\n## 176   Yes  No\n## 177    No Yes\n## 178   Yes Yes\n## 179    No Yes\n## 180   Yes Yes\n## 181   Yes Yes\n## 182   Yes  No\n## 183   Yes  No\n## 184   Yes Yes\n## 185    No Yes\n## 186   Yes Yes\n## 187    No  No\n## 188   Yes  No\n## 189   Yes  No\n## 190    No Yes\n## 191    No Yes\n## 192   Yes Yes\n## 193    No  No\n## 194   Yes Yes\n## 195   Yes Yes\n## 196   Yes Yes\n## 197   Yes Yes\n## 198   Yes  No\n## 199   Yes Yes\n## 200   Yes Yes\n## 201    No  No\n## 202   Yes  No\n## 203    No Yes\n## 204   Yes  No\n## 205   Yes  No\n## 206   Yes  No\n## 207   Yes Yes\n## 208    No  No\n## 209   Yes  No\n## 210    No Yes\n## 211    No Yes\n## 212   Yes Yes\n## 213   Yes Yes\n## 214   Yes Yes\n## 215   Yes Yes\n## 216   Yes Yes\n## 217   Yes  No\n## 218    No  No\n## 219   Yes Yes\n## 220   Yes Yes\n## 221   Yes Yes\n## 222   Yes  No\n## 223   Yes Yes\n## 224   Yes Yes\n## 225    No  No\n## 226   Yes  No\n## 227   Yes  No\n## 228   Yes Yes\n## 229    No Yes\n## 230    No  No\n## 231    No  No\n## 232    No  No\n## 233   Yes Yes\n## 234    No Yes\n## 235    No Yes\n## 236   Yes Yes\n## 237   Yes Yes\n## 238   Yes Yes\n## 239   Yes  No\n## 240   Yes Yes\n## 241   Yes  No\n## 242   Yes  No\n## 243    No  No\n## 244   Yes Yes\n## 245   Yes  No\n## 246    No Yes\n## 247   Yes Yes\n## 248   Yes  No\n## 249   Yes Yes\n## 250   Yes  No\n## 251   Yes Yes\n## 252   Yes Yes\n## 253   Yes  No\n## 254    No Yes\n## 255   Yes Yes\n## 256   Yes Yes\n## 257   Yes  No\n## 258   Yes Yes\n## 259    No  No\n## 260    No Yes\n## 261   Yes Yes\n## 262   Yes Yes\n## 263   Yes Yes\n## 264   Yes Yes\n## 265   Yes Yes\n## 266   Yes Yes\n## 267    No Yes\n## 268    No Yes\n## 269   Yes  No\n## 270   Yes  No\n## 271   Yes  No\n## 272   Yes  No\n## 273   Yes  No\n## 274   Yes Yes\n## 275   Yes Yes\n## 276   Yes Yes\n## 277   Yes Yes\n## 278   Yes Yes\n## 279    No Yes\n## 280   Yes Yes\n## 281   Yes Yes\n## 282    No Yes\n## 283   Yes  No\n## 284    No  No\n## 285    No  No\n## 286   Yes Yes\n## 287    No Yes\n## 288   Yes Yes\n## 289    No  No\n## 290   Yes Yes\n## 291    No Yes\n## 292   Yes  No\n## 293   Yes Yes\n## 294   Yes  No\n## 295   Yes Yes\n## 296    No Yes\n## 297   Yes Yes\n## 298   Yes Yes\n## 299   Yes  No\n## 300    No Yes\n## 301   Yes Yes\n## 302   Yes Yes\n## 303   Yes Yes\n## 304   Yes Yes\n## 305   Yes Yes\n## 306   Yes Yes\n## 307   Yes Yes\n## 308   Yes  No\n## 309   Yes Yes\n## 310   Yes Yes\n## 311   Yes Yes\n## 312   Yes Yes\n## 313   Yes Yes\n## 314   Yes  No\n## 315   Yes Yes\n## 316   Yes Yes\n## 317   Yes Yes\n## 318    No  No\n## 319    No Yes\n## 320    No Yes\n## 321   Yes Yes\n## 322   Yes  No\n## 323   Yes Yes\n## 324   Yes Yes\n## 325   Yes Yes\n## 326   Yes Yes\n## 327   Yes  No\n## 328   Yes Yes\n## 329   Yes Yes\n## 330   Yes Yes\n## 331    No  No\n## 332   Yes Yes\n## 333   Yes Yes\n## 334   Yes Yes\n## 335   Yes Yes\n## 336   Yes Yes\n## 337   Yes  No\n## 338   Yes  No\n## 339   Yes  No\n## 340   Yes Yes\n## 341   Yes  No\n## 342    No  No\n## 343    No Yes\n## 344   Yes Yes\n## 345    No Yes\n## 346   Yes  No\n## 347    No  No\n## 348    No  No\n## 349   Yes Yes\n## 350    No Yes\n## 351    No Yes\n## 352    No Yes\n## 353   Yes Yes\n## 354    No Yes\n## 355   Yes Yes\n## 356   Yes  No\n## 357   Yes  No\n## 358   Yes Yes\n## 359   Yes Yes\n## 360   Yes Yes\n## 361    No Yes\n## 362    No Yes\n## 363   Yes Yes\n## 364   Yes  No\n## 365   Yes Yes\n## 366    No  No\n## 367    No Yes\n## 368   Yes  No\n## 369    No Yes\n## 370   Yes Yes\n## 371   Yes Yes\n## 372   Yes  No\n## 373    No  No\n## 374   Yes  No\n## 375   Yes Yes\n## 376   Yes  No\n## 377   Yes Yes\n## 378    No  No\n## 379   Yes Yes\n## 380   Yes  No\n## 381   Yes Yes\n## 382   Yes Yes\n## 383   Yes Yes\n## 384   Yes  No\n## 385   Yes Yes\n## 386   Yes Yes\n## 387   Yes  No\n## 388    No Yes\n## 389   Yes Yes\n## 390   Yes Yes\n## 391   Yes Yes\n## 392   Yes  No\n## 393   Yes Yes\n## 394    No Yes\n## 395   Yes Yes\n## 396   Yes Yes\n## 397    No Yes\n## 398   Yes Yes\n## 399   Yes Yes\n## 400   Yes Yes\nlm_spec %>% \n  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)## parsnip model object\n## \n## Fit time:  4ms \n## \n## Call:\n## stats::lm(formula = Sales ~ . + Income:Advertising + Price:Age, \n##     data = data)\n## \n## Coefficients:\n##        (Intercept)           CompPrice              Income         Advertising  \n##          6.5755654           0.0929371           0.0108940           0.0702462  \n##         Population               Price       ShelveLocGood     ShelveLocMedium  \n##          0.0001592          -0.1008064           4.8486762           1.9532620  \n##                Age           Education            UrbanYes               USYes  \n##         -0.0579466          -0.0208525           0.1401597          -0.1575571  \n## Income:Advertising           Price:Age  \n##          0.0007510           0.0001068\nrec_spec <- recipe(Sales ~ ., data = Carseats) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~ Income:Advertising + Price:Age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Carseats)## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_dummy()\n## • step_interact()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##          (Intercept)             CompPrice                Income  \n##            6.5755654             0.0929371             0.0108940  \n##          Advertising            Population                 Price  \n##            0.0702462             0.0001592            -0.1008064  \n##                  Age             Education        ShelveLoc_Good  \n##           -0.0579466            -0.0208525             4.8486762  \n##     ShelveLoc_Medium             Urban_Yes                US_Yes  \n##            1.9532620             0.1401597            -0.1575571  \n## Income_x_Advertising           Price_x_Age  \n##            0.0007510             0.0001068"},{"path":"linear-regression.html","id":"writing-functions","chapter":"3 Linear Regression","heading":"3.7 Writing functions","text":"","code":""},{"path":"classification.html","id":"classification","chapter":"4 Classification","heading":"4 Classification","text":"","code":""},{"path":"classification.html","id":"the-stock-market-data","chapter":"4 Classification","heading":"4.1 The Stock Market Data","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)\n\nSmarket <- as_tibble(Smarket)\nCaravan <- as_tibble(Caravan)\nlibrary(corrr)\ncor_Smarket <- correlate(Smarket[-9])## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))## Don't know how to automatically pick scale for object of type noquote. Defaulting to continuous.\nlibrary(paletteer)\ncor_Smarket %>%\n  stretch() %>%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1) +\n  theme_minimal()"},{"path":"classification.html","id":"logistic-regression","chapter":"4 Classification","heading":"4.2 Logistic Regression","text":"","code":"\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket\n    )\n\nlr_fit## parsnip model object\n## \n## Fit time:  9ms \n## \n## Call:  stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Coefficients:\n## (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  \n##   -0.126000    -0.073074    -0.042301     0.011085     0.009359     0.010313  \n##      Volume  \n##    0.135441  \n## \n## Degrees of Freedom: 1249 Total (i.e. Null);  1243 Residual\n## Null Deviance:       1731 \n## Residual Deviance: 1728  AIC: 1742\nsummary(lr_fit$fit)## \n## Call:\n## stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n##     Lag5 + Volume, family = stats::binomial, data = data)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.446  -1.203   1.065   1.145   1.326  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.126000   0.240736  -0.523    0.601\n## Lag1        -0.073074   0.050167  -1.457    0.145\n## Lag2        -0.042301   0.050086  -0.845    0.398\n## Lag3         0.011085   0.049939   0.222    0.824\n## Lag4         0.009359   0.049974   0.187    0.851\n## Lag5         0.010313   0.049511   0.208    0.835\n## Volume       0.135441   0.158360   0.855    0.392\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1731.2  on 1249  degrees of freedom\n## Residual deviance: 1727.6  on 1243  degrees of freedom\n## AIC: 1741.6\n## \n## Number of Fisher Scoring iterations: 3\ntidy(lr_fit)## # A tibble: 7 x 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) -0.126      0.241     -0.523   0.601\n## 2 Lag1        -0.0731     0.0502    -1.46    0.145\n## 3 Lag2        -0.0423     0.0501    -0.845   0.398\n## 4 Lag3         0.0111     0.0499     0.222   0.824\n## 5 Lag4         0.00936    0.0500     0.187   0.851\n## 6 Lag5         0.0103     0.0495     0.208   0.835\n## 7 Volume       0.135      0.158      0.855   0.392\npredict(lr_fit, new_data = Smarket)## # A tibble: 1,250 x 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Down       \n##  3 Down       \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Down       \n##  8 Up         \n##  9 Up         \n## 10 Down       \n## # … with 1,240 more rows\npredict(lr_fit, new_data = Smarket, type = \"prob\")## # A tibble: 1,250 x 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.493    0.507\n##  2      0.519    0.481\n##  3      0.519    0.481\n##  4      0.485    0.515\n##  5      0.489    0.511\n##  6      0.493    0.507\n##  7      0.507    0.493\n##  8      0.491    0.509\n##  9      0.482    0.518\n## 10      0.511    0.489\n## # … with 1,240 more rows\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class)##           Truth\n## Prediction Down  Up\n##       Down  145 141\n##       Up    457 507\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\naugment(lr_fit, new_data = Smarket) %>%\n  accuracy(truth = Direction, estimate = .pred_class)## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.522\nSmarket_train <- Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- Smarket %>%\n  filter(Year == 2005)\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket_train\n    )\naugment(lr_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   77 97\n##       Up     34 44\naugment(lr_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.480\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2,\n    data = Smarket_train\n    )\naugment(lr_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lr_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560\npredict(\n  lr_fit,\n  new_data = tibble(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), \n  type = \"prob\"\n)## # A tibble: 2 x 2\n##   .pred_Down .pred_Up\n##        <dbl>    <dbl>\n## 1      0.521    0.479\n## 2      0.504    0.496"},{"path":"classification.html","id":"linear-discriminant-analysis","chapter":"4 Classification","heading":"4.3 Linear Discriminant Analysis","text":"","code":"\nlibrary(discrim)## \n## Attaching package: 'discrim'## The following object is masked from 'package:dials':\n## \n##     smoothness\nlda_spec <- discrim_linear() %>%\n  set_engine(\"MASS\")\n\nlda_fit <- lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nlda_fit## parsnip model object\n## \n## Fit time:  4ms \n## Call:\n## lda(Direction ~ Lag1 + Lag2, data = data)\n## \n## Prior probabilities of groups:\n##     Down       Up \n## 0.491984 0.508016 \n## \n## Group means:\n##             Lag1        Lag2\n## Down  0.04279022  0.03389409\n## Up   -0.03954635 -0.03132544\n## \n## Coefficients of linear discriminants:\n##             LD1\n## Lag1 -0.6420190\n## Lag2 -0.5135293\npredict(lda_fit, new_data = Smarket_test)## # A tibble: 252 x 1\n##    .pred_class\n##    <fct>      \n##  1 Up         \n##  2 Up         \n##  3 Up         \n##  4 Up         \n##  5 Up         \n##  6 Up         \n##  7 Up         \n##  8 Up         \n##  9 Up         \n## 10 Up         \n## # … with 242 more rows\npredict(lda_fit, new_data = Smarket_test, type = \"prob\")## # A tibble: 252 x 2\n##    .pred_Down .pred_Up\n##         <dbl>    <dbl>\n##  1      0.490    0.510\n##  2      0.479    0.521\n##  3      0.467    0.533\n##  4      0.474    0.526\n##  5      0.493    0.507\n##  6      0.494    0.506\n##  7      0.495    0.505\n##  8      0.487    0.513\n##  9      0.491    0.509\n## 10      0.484    0.516\n## # … with 242 more rows\naugment(lda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   35  35\n##       Up     76 106\naugment(lda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.560"},{"path":"classification.html","id":"quadratic-discriminant-analysis","chapter":"4 Classification","heading":"4.4 Quadratic Discriminant Analysis","text":"","code":"\nlibrary(discrim)\nqda_spec <- discrim_regularized() %>%\n  set_args(frac_common_cov = 0, frac_identity = 0) %>%\n  set_engine(\"klaR\")\n\nqda_fit <- qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nqda_fit## parsnip model object\n## \n## Fit time:  42ms \n## Call: \n## rda(formula = Direction ~ Lag1 + Lag2, data = data, lambda = ~0, \n##     gamma = ~0)\n## \n## Regularization parameters: \n##  gamma lambda \n##      0      0 \n## \n## Prior probabilities of groups: \n##     Down       Up \n## 0.491984 0.508016 \n## \n## Misclassification rate: \n##        apparent: 48.597 %\naugment(qda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down  Up\n##       Down   30  20\n##       Up     81 121\naugment(qda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.599"},{"path":"classification.html","id":"k-nearest-neighbors","chapter":"4 Classification","heading":"4.5 K-Nearest Neighbors","text":"","code":"\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\nknn_fit <- knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nknn_fit## parsnip model object\n## \n## Fit time:  36ms \n## \n## Call:\n## kknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n## \n## Type of response variable: nominal\n## Minimal misclassification: 0.492986\n## Best kernel: optimal\n## Best k: 3\naugment(knn_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) ##           Truth\n## Prediction Down Up\n##       Down   43 58\n##       Up     68 83\naugment(knn_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) ## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary           0.5"},{"path":"classification.html","id":"an-application-to-caravan-insurance-data","chapter":"4 Classification","heading":"4.6 An Application to Caravan Insurance Data","text":"","code":"\nCaravan_test <- Caravan[seq_len(1000), ]\nCaravan_train <- Caravan[-seq_len(1000), ]\nrec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%\n  step_normalize(all_numeric_predictors())\nCaravan_wf <- workflow() %>%\n  add_recipe(rec_spec)\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\nknn1_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 1))\nknn1_fit <- fit(knn1_wf, data = Caravan_train)\naugment(knn1_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9\naugment(knn1_fit, new_data = Caravan_test) %>%\n  accuracy(truth = Purchase, estimate = .pred_class)## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.883\nknn3_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 3))\n\nknn3_fit <- fit(knn3_wf, Caravan_train)\naugment(knn3_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  875  50\n##        Yes  66   9\nknn5_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 5))\n\nknn5_fit <- fit(knn5_wf, Caravan_train)\naugment(knn5_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  874  50\n##        Yes  67   9"},{"path":"resampling-methods.html","id":"resampling-methods","chapter":"5 Resampling Methods","heading":"5 Resampling Methods","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)\n\nAuto <- as_tibble(Auto)\nPortfolio <- as_tibble(Portfolio)"},{"path":"resampling-methods.html","id":"the-validation-set-approach","chapter":"5 Resampling Methods","heading":"5.1 The Validation Set Approach","text":"","code":"\nset.seed(1)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\nlm_fit <- lm_spec %>% \n  fit(mpg ~ horsepower, data = Auto_train)\naugment(lm_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        5.11\npoly_fit <- lm_spec %>% \n  fit(mpg ~ poly(horsepower, 2), data = Auto_train)\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.54\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = 2)\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\npoly_fit <- fit(poly_wf, data = Auto_train)\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.54\nset.seed(2)\nAuto_split <- initial_split(Auto)\n\nAuto_train <- training(Auto_split)\nAuto_test <- testing(Auto_split)\npoly_fit <- fit(poly_wf, data = Auto_train)\n\naugment(poly_fit, new_data = Auto_test) %>%\n  rmse(truth = mpg, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        4.66"},{"path":"resampling-methods.html","id":"leave-one-out-cross-validation","chapter":"5 Resampling Methods","heading":"5.2 Leave-One-Out Cross-Validation","text":"Leave-One-Cross-Validation integrated broader tidymodels framework. information read .","code":""},{"path":"resampling-methods.html","id":"k-fold-cross-validation","chapter":"5 Resampling Methods","heading":"5.3 k-Fold Cross-Validation","text":"can helpful add control = control_grid(verbose = TRUE)","code":"\npoly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%\n  step_poly(horsepower, degree = tune())\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\npoly_wf <- workflow() %>%\n  add_recipe(poly_rec) %>%\n  add_model(lm_spec)\n\nAuto_folds <- vfold_cv(Auto_train, v = 10)\n\ndegree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)\n\ntune_res <- tune_grid(\n  object = poly_wf, \n  resamples = Auto_folds, \n  grid = degree_grid\n)\nautoplot(tune_res)\ncollect_metrics(tune_res)## # A tibble: 20 x 7\n##    degree .metric .estimator  mean     n std_err .config              \n##     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1      1 rmse    standard   4.84     10  0.194  Preprocessor01_Model1\n##  2      1 rsq     standard   0.635    10  0.0209 Preprocessor01_Model1\n##  3      2 rmse    standard   4.29     10  0.155  Preprocessor02_Model1\n##  4      2 rsq     standard   0.709    10  0.0224 Preprocessor02_Model1\n##  5      3 rmse    standard   4.31     10  0.154  Preprocessor03_Model1\n##  6      3 rsq     standard   0.707    10  0.0219 Preprocessor03_Model1\n##  7      4 rmse    standard   4.33     10  0.152  Preprocessor04_Model1\n##  8      4 rsq     standard   0.706    10  0.0217 Preprocessor04_Model1\n##  9      5 rmse    standard   4.30     10  0.149  Preprocessor05_Model1\n## 10      5 rsq     standard   0.710    10  0.0208 Preprocessor05_Model1\n## 11      6 rmse    standard   4.32     10  0.149  Preprocessor06_Model1\n## 12      6 rsq     standard   0.708    10  0.0203 Preprocessor06_Model1\n## 13      7 rmse    standard   4.34     10  0.152  Preprocessor07_Model1\n## 14      7 rsq     standard   0.705    10  0.0203 Preprocessor07_Model1\n## 15      8 rmse    standard   4.37     10  0.151  Preprocessor08_Model1\n## 16      8 rsq     standard   0.704    10  0.0212 Preprocessor08_Model1\n## 17      9 rmse    standard   4.41     10  0.172  Preprocessor09_Model1\n## 18      9 rsq     standard   0.702    10  0.0226 Preprocessor09_Model1\n## 19     10 rmse    standard   4.56     10  0.158  Preprocessor10_Model1\n## 20     10 rsq     standard   0.689    10  0.0198 Preprocessor10_Model1\nshow_best(tune_res, metric = \"rmse\")## # A tibble: 5 x 7\n##   degree .metric .estimator  mean     n std_err .config              \n##    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n## 1      2 rmse    standard    4.29    10   0.155 Preprocessor02_Model1\n## 2      5 rmse    standard    4.30    10   0.149 Preprocessor05_Model1\n## 3      3 rmse    standard    4.31    10   0.154 Preprocessor03_Model1\n## 4      6 rmse    standard    4.32    10   0.149 Preprocessor06_Model1\n## 5      4 rmse    standard    4.33    10   0.152 Preprocessor04_Model1\nbest_degree <- select_best(tune_res, metric = \"rmse\")\nfinal_wf <- finalize_workflow(poly_wf, best_degree)\n\nfinal_wf## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nfinal_fit <- fit(final_wf, Auto_train)\n\nfinal_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_poly()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##       (Intercept)  horsepower_poly_1  horsepower_poly_2  \n##             23.49            -106.32              38.94"},{"path":"resampling-methods.html","id":"the-bootstrap","chapter":"5 Resampling Methods","heading":"5.4 The Bootstrap","text":"","code":"\nPortfolio_boots <- bootstraps(Portfolio, times = 1000)\n\nalpha.fn <- function(split) {\n  data <- analysis(split)\n  X <- data$X\n  Y <- data$Y\n  \n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nalpha_res <- Portfolio_boots %>%\n  mutate(alpha = map_dbl(splits, alpha.fn))\n\nalpha_res## # Bootstrap sampling \n## # A tibble: 1,000 x 3\n##    splits           id            alpha\n##    <list>           <chr>         <dbl>\n##  1 <split [100/37]> Bootstrap0001 0.457\n##  2 <split [100/38]> Bootstrap0002 0.522\n##  3 <split [100/33]> Bootstrap0003 0.482\n##  4 <split [100/35]> Bootstrap0004 0.614\n##  5 <split [100/35]> Bootstrap0005 0.598\n##  6 <split [100/38]> Bootstrap0006 0.624\n##  7 <split [100/34]> Bootstrap0007 0.700\n##  8 <split [100/40]> Bootstrap0008 0.416\n##  9 <split [100/34]> Bootstrap0009 0.540\n## 10 <split [100/36]> Bootstrap0010 0.735\n## # … with 990 more rows\nAuto_boots <- bootstraps(Auto)\n\nboot.fn <- function(split) {\n  lm_fit <- lm_spec %>% fit(mpg ~ horsepower, data = analysis(split))\n  tidy(lm_fit)\n}\n\nboot_res <- Auto_boots %>%\n  mutate(models = map(splits, boot.fn))\n\nboot_res %>%\n  unnest(cols = c(models)) %>%\n  group_by(term) %>%\n  summarise(mean = mean(estimate),\n            sd = sd(estimate))## # A tibble: 2 x 3\n##   term          mean      sd\n##   <chr>        <dbl>   <dbl>\n## 1 (Intercept) 39.8   0.718  \n## 2 horsepower  -0.156 0.00620"},{"path":"linear-model-selection-and-regularization.html","id":"linear-model-selection-and-regularization","chapter":"6 Linear Model Selection and Regularization","heading":"6 Linear Model Selection and Regularization","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)\n\nHitters <- as_tibble(Hitters) %>%\n  filter(!is.na(Salary))"},{"path":"linear-model-selection-and-regularization.html","id":"best-subset-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.1 Best Subset Selection","text":"tidymodels currently support subset selection methods, unlikely include near future.","code":""},{"path":"linear-model-selection-and-regularization.html","id":"forward-and-backward-stepwise-selection","chapter":"6 Linear Model Selection and Regularization","heading":"6.2 Forward and Backward Stepwise Selection","text":"tidymodels currently support forward backward stepwise selection methods, unlikely include near future.","code":""},{"path":"linear-model-selection-and-regularization.html","id":"ridge-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.3 Ridge Regression","text":"","code":"\nridge_spec <- linear_reg(mixture = 0) %>%\n  set_engine(\"glmnet\")\nridge_fit <- fit(ridge_spec, Salary ~ ., data = Hitters)\ntidy(ridge_fit, penalty = 11498)## Loading required package: Matrix## \n## Attaching package: 'Matrix'## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack## Loaded glmnet 4.1-1## # A tibble: 20 x 3\n##    term         estimate penalty\n##    <chr>           <dbl>   <dbl>\n##  1 (Intercept) 407.        11498\n##  2 AtBat         0.0370    11498\n##  3 Hits          0.138     11498\n##  4 HmRun         0.525     11498\n##  5 Runs          0.231     11498\n##  6 RBI           0.240     11498\n##  7 Walks         0.290     11498\n##  8 Years         1.11      11498\n##  9 CAtBat        0.00314   11498\n## 10 CHits         0.0117    11498\n## 11 CHmRun        0.0876    11498\n## 12 CRuns         0.0234    11498\n## 13 CRBI          0.0242    11498\n## 14 CWalks        0.0250    11498\n## 15 LeagueN       0.0866    11498\n## 16 DivisionW    -6.23      11498\n## 17 PutOuts       0.0165    11498\n## 18 Assists       0.00262   11498\n## 19 Errors       -0.0206    11498\n## 20 NewLeagueN    0.303     11498\ntidy(ridge_fit, penalty = 705)## # A tibble: 20 x 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)  54.4        705\n##  2 AtBat         0.112      705\n##  3 Hits          0.656      705\n##  4 HmRun         1.18       705\n##  5 Runs          0.937      705\n##  6 RBI           0.847      705\n##  7 Walks         1.32       705\n##  8 Years         2.58       705\n##  9 CAtBat        0.0108     705\n## 10 CHits         0.0468     705\n## 11 CHmRun        0.338      705\n## 12 CRuns         0.0937     705\n## 13 CRBI          0.0979     705\n## 14 CWalks        0.0718     705\n## 15 LeagueN      13.7        705\n## 16 DivisionW   -54.7        705\n## 17 PutOuts       0.119      705\n## 18 Assists       0.0161     705\n## 19 Errors       -0.704      705\n## 20 NewLeagueN    8.61       705\ntidy(ridge_fit, penalty = 50)## # A tibble: 20 x 3\n##    term          estimate penalty\n##    <chr>            <dbl>   <dbl>\n##  1 (Intercept)   48.2          50\n##  2 AtBat         -0.354        50\n##  3 Hits           1.95         50\n##  4 HmRun         -1.29         50\n##  5 Runs           1.16         50\n##  6 RBI            0.809        50\n##  7 Walks          2.71         50\n##  8 Years         -6.20         50\n##  9 CAtBat         0.00609      50\n## 10 CHits          0.107        50\n## 11 CHmRun         0.629        50\n## 12 CRuns          0.217        50\n## 13 CRBI           0.215        50\n## 14 CWalks        -0.149        50\n## 15 LeagueN       45.9          50\n## 16 DivisionW   -118.           50\n## 17 PutOuts        0.250        50\n## 18 Assists        0.121        50\n## 19 Errors        -3.28         50\n## 20 NewLeagueN    -9.42         50\npredict(ridge_fit, new_data = Hitters, penalty = 50)## # A tibble: 263 x 1\n##     .pred\n##     <dbl>\n##  1  469. \n##  2  663. \n##  3 1023. \n##  4  505. \n##  5  550. \n##  6  200. \n##  7   79.4\n##  8  105. \n##  9  836. \n## 10  865. \n## # … with 253 more rows\nHitters_split <- initial_split(Hitters)\n\nHitters_train <- training(Hitters_split)\nHitters_test <- testing(Hitters_split)\n\nHitters_fold <- vfold_cv(Hitters_train, v = 10)\nridge_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) \n\nridge_spec <- \n  linear_reg(penalty = tune(), mixture = 0) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nridge_workflow <- \n  workflow() %>% \n  add_recipe(ridge_recipe) %>% \n  add_model(ridge_spec) \n\npenalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\ntune_res <- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nautoplot(tune_res)\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nridge_final <- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit <- fit(ridge_final, data = Hitters_train)\n\ntidy(ridge_final_fit)## # A tibble: 20 x 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept) 533.        356.\n##  2 AtBat        25.0       356.\n##  3 Hits         33.4       356.\n##  4 HmRun        11.5       356.\n##  5 Runs         29.2       356.\n##  6 RBI          31.9       356.\n##  7 Walks        32.3       356.\n##  8 Years        12.5       356.\n##  9 CAtBat       28.5       356.\n## 10 CHits        33.1       356.\n## 11 CHmRun       27.5       356.\n## 12 CRuns        34.8       356.\n## 13 CRBI         33.0       356.\n## 14 CWalks       12.6       356.\n## 15 PutOuts      27.2       356.\n## 16 Assists      -0.762     356.\n## 17 Errors       -7.45      356.\n## 18 League_N     10.8       356.\n## 19 Division_W  -19.8       356.\n## 20 NewLeague_N   0.0189    356."},{"path":"linear-model-selection-and-regularization.html","id":"the-lasso","chapter":"6 Linear Model Selection and Regularization","heading":"6.4 The Lasso","text":"","code":"\nlasso_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) \n\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nlasso_workflow <- \n  workflow() %>% \n  add_recipe(lasso_recipe) %>% \n  add_model(lasso_spec) \n\npenalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)\n\ntune_res <- tune_grid(\n  lasso_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nautoplot(tune_res)\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nlasso_final <- finalize_workflow(lasso_workflow, best_penalty)\n\nlasso_final_fit <- fit(lasso_final, data = Hitters_train)\n\ntidy(lasso_final_fit)## # A tibble: 20 x 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)    533.     4.09\n##  2 AtBat            0      4.09\n##  3 Hits            59.5    4.09\n##  4 HmRun            0      4.09\n##  5 Runs             0      4.09\n##  6 RBI             46.5    4.09\n##  7 Walks           80.2    4.09\n##  8 Years            0      4.09\n##  9 CAtBat           0      4.09\n## 10 CHits            0      4.09\n## 11 CHmRun           0      4.09\n## 12 CRuns          200.     4.09\n## 13 CRBI            80.0    4.09\n## 14 CWalks         -97.9    4.09\n## 15 PutOuts         41.0    4.09\n## 16 Assists          0      4.09\n## 17 Errors         -19.0    4.09\n## 18 League_N        24.3    4.09\n## 19 Division_W     -32.7    4.09\n## 20 NewLeague_N    -10.8    4.09"},{"path":"linear-model-selection-and-regularization.html","id":"principal-components-regression","chapter":"6 Linear Model Selection and Regularization","heading":"6.5 Principal Components Regression","text":"","code":"\npca_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>%\n  step_pca(all_predictors(), threshold = tune())\n\nlm_spec <- \n  linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") \n\npca_workflow <- \n  workflow() %>% \n  add_recipe(pca_recipe) %>% \n  add_model(lm_spec) \n\nthreshold_grid <- grid_regular(threshold(), levels = 10)\n\ntune_res <- tune_grid(\n  pca_workflow,\n  resamples = Hitters_fold, \n  grid = threshold_grid\n)\n\nautoplot(tune_res)\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\npca_final <- finalize_workflow(pca_workflow, best_threshold)\n\npca_final_fit <- fit(pca_final, data = Hitters_train)\n\ntidy(pca_final_fit)## # A tibble: 3 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    533.      20.7      25.7  1.32e-64\n## 2 PC1            107.       7.80     13.7  1.68e-30\n## 3 PC2            -28.8     10.1      -2.87 4.63e- 3"},{"path":"linear-model-selection-and-regularization.html","id":"partial-least-squares","chapter":"6 Linear Model Selection and Regularization","heading":"6.6 Partial Least Squares","text":"","code":"\npls_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>%\n  step_pls(all_predictors(), num_comp = tune(), outcome = \"Salary\")\n\nlm_spec <- \n  linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") \n\npls_workflow <- \n  workflow() %>% \n  add_recipe(pls_recipe) %>% \n  add_model(lm_spec) \n\nnum_comp_grid <- grid_regular(num_comp(c(1, 20)), levels = 10)\n\ntune_res <- tune_grid(\n  pls_workflow,\n  resamples = Hitters_fold, \n  grid = num_comp_grid\n)\n\nautoplot(tune_res)\nbest_threshold <- select_best(tune_res, metric = \"rmse\")\npls_final <- finalize_workflow(pls_workflow, best_threshold)\n\npls_final_fit <- fit(pls_final, data = Hitters_train)\n\ntidy(pls_final_fit)## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     533.     20.6       25.9 2.93e-65\n## 2 PLS1            111.      7.80      14.2 5.51e-32"},{"path":"moving-beyond-linearity.html","id":"moving-beyond-linearity","chapter":"7 Moving Beyond Linearity","heading":"7 Moving Beyond Linearity","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)\n\nWage <- as_tibble(Wage)"},{"path":"moving-beyond-linearity.html","id":"polynomial-regression-and-step-functions","chapter":"7 Moving Beyond Linearity","heading":"7.1 Polynomial Regression and Step Functions","text":"","code":"\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nlm_fit <- fit(lm_spec, wage ~ poly(age, 4), data = Wage)\n\ntidy(lm_fit)## # A tibble: 5 x 5\n##   term          estimate std.error statistic  p.value\n##   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      112.      0.729    153.   0       \n## 2 poly(age, 4)1    447.     39.9       11.2  1.48e-28\n## 3 poly(age, 4)2   -478.     39.9      -12.0  2.36e-32\n## 4 poly(age, 4)3    126.     39.9        3.14 1.68e- 3\n## 5 poly(age, 4)4    -77.9    39.9       -1.95 5.10e- 2\nrec_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\npoly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_poly)\n\npoly_fit <- fit(poly_wf, data = Wage)\n\ntidy(poly_fit)## # A tibble: 5 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    112.      0.729    153.   0       \n## 2 age_poly_1     447.     39.9       11.2  1.48e-28\n## 3 age_poly_2    -478.     39.9      -12.0  2.36e-32\n## 4 age_poly_3     126.     39.9        3.14 1.68e- 3\n## 5 age_poly_4     -77.9    39.9       -1.95 5.10e- 2\npoly(1:4, degree = 2, raw = TRUE)##      1  2\n## [1,] 1  1\n## [2,] 2  4\n## [3,] 3  9\n## [4,] 4 16\n## attr(,\"degree\")\n## [1] 1 2\n## attr(,\"class\")\n## [1] \"poly\"   \"matrix\"\npoly(1:4, degree = 2, raw = FALSE)##               1    2\n## [1,] -0.6708204  0.5\n## [2,] -0.2236068 -0.5\n## [3,]  0.2236068 -0.5\n## [4,]  0.6708204  0.5\n## attr(,\"coefs\")\n## attr(,\"coefs\")$alpha\n## [1] 2.5 2.5\n## \n## attr(,\"coefs\")$norm2\n## [1] 1 4 5 4\n## \n## attr(,\"degree\")\n## [1] 1 2\n## attr(,\"class\")\n## [1] \"poly\"   \"matrix\"\nrec_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4, options = list(raw = TRUE))\n\npoly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_poly)\n\nraw_poly_fit <- fit(poly_wf, data = Wage)\n\ntidy(raw_poly_fit)## # A tibble: 5 x 5\n##   term            estimate  std.error statistic  p.value\n##   <chr>              <dbl>      <dbl>     <dbl>    <dbl>\n## 1 (Intercept) -184.        60.0           -3.07 0.00218 \n## 2 age_poly_1    21.2        5.89           3.61 0.000312\n## 3 age_poly_2    -0.564      0.206         -2.74 0.00626 \n## 4 age_poly_3     0.00681    0.00307        2.22 0.0264  \n## 5 age_poly_4    -0.0000320  0.0000164     -1.95 0.0510\nage_range <- tibble(age = seq(min(Wage$age), max(Wage$age)))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = age_range),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"blue\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\nWage <- Wage %>%\n  mutate(high = factor(wage > 250, \n                       levels = c(TRUE, FALSE), \n                       labels = c(\"High\", \"Low\")))\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\nrec_poly <- recipe(high ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\npoly_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_poly)\n\npoly_fit <- fit(poly_wf, data = Wage)\n\npredict(poly_fit, new_data = Wage)## # A tibble: 3,000 x 1\n##    .pred_class\n##    <fct>      \n##  1 Low        \n##  2 Low        \n##  3 Low        \n##  4 Low        \n##  5 Low        \n##  6 Low        \n##  7 Low        \n##  8 Low        \n##  9 Low        \n## 10 Low        \n## # … with 2,990 more rows\npredict(poly_fit, new_data = Wage, type = \"prob\")## # A tibble: 3,000 x 2\n##       .pred_High .pred_Low\n##            <dbl>     <dbl>\n##  1 0.00000000983     1.00 \n##  2 0.000120          1.00 \n##  3 0.0307            0.969\n##  4 0.0320            0.968\n##  5 0.0305            0.970\n##  6 0.0352            0.965\n##  7 0.0313            0.969\n##  8 0.00820           0.992\n##  9 0.0334            0.967\n## 10 0.0323            0.968\n## # … with 2,990 more rows\npredict(poly_fit, new_data = Wage, type = \"conf_int\")## # A tibble: 3,000 x 4\n##    .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n##               <dbl>            <dbl>           <dbl>           <dbl>\n##  1         2.22e-16          0.00166           0.998           1    \n##  2         1.82e- 6          0.00786           0.992           1.00 \n##  3         2.19e- 2          0.0428            0.957           0.978\n##  4         2.31e- 2          0.0442            0.956           0.977\n##  5         2.13e- 2          0.0434            0.957           0.979\n##  6         2.45e- 2          0.0503            0.950           0.975\n##  7         2.25e- 2          0.0434            0.957           0.977\n##  8         3.01e- 3          0.0222            0.978           0.997\n##  9         2.39e- 2          0.0465            0.953           0.976\n## 10         2.26e- 2          0.0458            0.954           0.977\n## # … with 2,990 more rows\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = age_range, type = \"prob\"),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nregression_lines %>%\n  ggplot(aes(age)) +\n  ylim(c(0, 0.2)) +\n  geom_line(aes(y = .pred_High), color = \"blue\") +\n  geom_line(aes(y = .pred_lower_High), color = \"blue\", linetype = \"dashed\") +\n  geom_line(aes(y = .pred_upper_High), color = \"blue\", linetype = \"dashed\") +\n  geom_jitter(aes(y = (high == \"High\") / 5), data = Wage, \n              shape = \"|\", height = 0, width = 0.2)## Warning: Removed 8 row(s) containing missing values (geom_path).\nrec_cut <- recipe(high ~ age, data = Wage) %>%\n  step_discretize(age, num_breaks = 4)\n\ncut_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_cut)\n\ncut_fit <- fit(cut_wf, data = Wage)\n\npredict(cut_fit, new_data = Wage)## # A tibble: 3,000 x 1\n##    .pred_class\n##    <fct>      \n##  1 Low        \n##  2 Low        \n##  3 Low        \n##  4 Low        \n##  5 Low        \n##  6 Low        \n##  7 Low        \n##  8 Low        \n##  9 Low        \n## 10 Low        \n## # … with 2,990 more rows\npredict(cut_fit, new_data = Wage, type = \"prob\")## # A tibble: 3,000 x 2\n##    .pred_High .pred_Low\n##         <dbl>     <dbl>\n##  1    0.00667     0.993\n##  2    0.00667     0.993\n##  3    0.0343      0.966\n##  4    0.0343      0.966\n##  5    0.0343      0.966\n##  6    0.0356      0.964\n##  7    0.0343      0.966\n##  8    0.00667     0.993\n##  9    0.0290      0.971\n## 10    0.0356      0.964\n## # … with 2,990 more rows\npredict(cut_fit, new_data = Wage, type = \"conf_int\")## # A tibble: 3,000 x 4\n##    .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n##               <dbl>            <dbl>           <dbl>           <dbl>\n##  1          0.00278           0.0159           0.984           0.997\n##  2          0.00278           0.0159           0.984           0.997\n##  3          0.0238            0.0492           0.951           0.976\n##  4          0.0238            0.0492           0.951           0.976\n##  5          0.0238            0.0492           0.951           0.976\n##  6          0.0240            0.0526           0.947           0.976\n##  7          0.0238            0.0492           0.951           0.976\n##  8          0.00278           0.0159           0.984           0.997\n##  9          0.0192            0.0436           0.956           0.981\n## 10          0.0240            0.0526           0.947           0.976\n## # … with 2,990 more rows"},{"path":"moving-beyond-linearity.html","id":"splines","chapter":"7 Moving Beyond Linearity","heading":"7.2 Splines","text":"","code":"\nrec_spline <- recipe(wage ~ age, data = Wage) %>%\n  step_bs(age, options = list(knots = 25, 40, 60))\n\nspline_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spline)\n\ncut_fit <- fit(spline_wf, data = Wage)\n\npredict(cut_fit, new_data = Wage)## # A tibble: 3,000 x 1\n##    .pred\n##    <dbl>\n##  1  58.7\n##  2  84.3\n##  3 120. \n##  4 120. \n##  5 120. \n##  6 119. \n##  7 120. \n##  8 102. \n##  9 119. \n## 10 120. \n## # … with 2,990 more rows\npredict(cut_fit, new_data = Wage, type = \"conf_int\")## # A tibble: 3,000 x 2\n##    .pred_lower .pred_upper\n##          <dbl>       <dbl>\n##  1        50.8        66.6\n##  2        80.7        87.8\n##  3       119.        122. \n##  4       118.        122. \n##  5       118.        122. \n##  6       116.        121. \n##  7       118.        122. \n##  8        99.8       104. \n##  9       117.        121. \n## 10       117.        122. \n## # … with 2,990 more rows\nregression_lines <- bind_cols(\n  augment(cut_fit, new_data = age_range),\n  predict(cut_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"blue\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")"},{"path":"moving-beyond-linearity.html","id":"gams","chapter":"7 Moving Beyond Linearity","heading":"7.3 GAMs","text":"","code":""},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"8 Tree-Based Methods","heading":"8 Tree-Based Methods","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)\nlibrary(MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nBoston <- as_tibble(Boston)"},{"path":"tree-based-methods.html","id":"fitting-classification-trees","chapter":"8 Tree-Based Methods","heading":"8.1 Fitting Classification Trees","text":"","code":"\nCarseats <- as_tibble(Carseats) %>%\n  mutate(High = factor(if_else(Sales <= 8, \"No\", \"Yes\")))\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\")\nclass_tree_spec <- tree_spec %>%\n  set_mode(\"classification\")\nclass_tree_fit <- fit(class_tree_spec, High ~ . - Sales, data = Carseats)\nclass_tree_fit$fit## n= 400 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 400 164 No (0.59000000 0.41000000)  \n##     2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  \n##       4) Price>=92.5 269  66 No (0.75464684 0.24535316)  \n##         8) Advertising< 13.5 224  41 No (0.81696429 0.18303571)  \n##          16) CompPrice< 124.5 96   6 No (0.93750000 0.06250000) *\n##          17) CompPrice>=124.5 128  35 No (0.72656250 0.27343750)  \n##            34) Price>=109.5 107  20 No (0.81308411 0.18691589)  \n##              68) Price>=126.5 65   6 No (0.90769231 0.09230769) *\n##              69) Price< 126.5 42  14 No (0.66666667 0.33333333)  \n##               138) Age>=49.5 22   2 No (0.90909091 0.09090909) *\n##               139) Age< 49.5 20   8 Yes (0.40000000 0.60000000) *\n##            35) Price< 109.5 21   6 Yes (0.28571429 0.71428571) *\n##         9) Advertising>=13.5 45  20 Yes (0.44444444 0.55555556)  \n##          18) Age>=54.5 20   5 No (0.75000000 0.25000000) *\n##          19) Age< 54.5 25   5 Yes (0.20000000 0.80000000) *\n##       5) Price< 92.5 46  14 Yes (0.30434783 0.69565217)  \n##        10) Income< 57 10   3 No (0.70000000 0.30000000) *\n##        11) Income>=57 36   7 Yes (0.19444444 0.80555556) *\n##     3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  \n##       6) Price>=142.5 12   3 No (0.75000000 0.25000000) *\n##       7) Price< 142.5 73  10 Yes (0.13698630 0.86301370) *\nlibrary(rpart.plot)## Loading required package: rpart## \n## Attaching package: 'rpart'## The following object is masked from 'package:dials':\n## \n##     prune\nrpart.plot(class_tree_fit$fit)\naugment(class_tree_fit, new_data = Carseats) %>%\n  accuracy(truth = High, estimate = .pred_class)## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.848\nCarseats_split <- initial_split(Carseats)\n\nCarseats_train <- training(Carseats_split)\nCarseats_test <- testing(Carseats_split)\n\n\nclass_tree_fit <- fit(class_tree_spec, High ~ . - Sales, data = Carseats_train)\naugment(class_tree_fit, new_data = Carseats_train) %>%\n  conf_mat(truth = High, estimate = .pred_class)##           Truth\n## Prediction  No Yes\n##        No  166  35\n##        Yes  11  88\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  conf_mat(truth = High, estimate = .pred_class)##           Truth\n## Prediction No Yes\n##        No  53  24\n##        Yes  6  17\nclass_tree_wf <- workflow() %>%\n  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(High ~ . - Sales)\n\nset.seed(1234)\nCarseats_fold <- vfold_cv(Carseats_train)\n\nparam_grid <- grid_regular(cost_complexity(), levels = 10)\n\ntune_res <- tune_grid(\n  class_tree_wf, \n  resamples = Carseats_fold, \n  grid = param_grid, \n  metrics = metric_set(accuracy)\n)\n\nautoplot(tune_res)\nbest_complexity <- select_best(tune_res)\n\nclass_tree_final <- finalize_workflow(class_tree_wf, best_complexity)\n\nclass_tree_final_fit <- fit(class_tree_final, data = Carseats_train)\n\nrpart.plot(class_tree_final_fit$fit$fit$fit)"},{"path":"tree-based-methods.html","id":"fitting-regression-trees","chapter":"8 Tree-Based Methods","heading":"8.2 Fitting Regression Trees","text":"","code":"\nreg_tree_spec <- tree_spec %>%\n  set_mode(\"regression\")\nset.seed(1234)\nBoston_split <- initial_split(Boston)\n\nBoston_train <- training(Boston_split)\nBoston_test <- testing(Boston_split)\n\nreg_tree_fit <- fit(reg_tree_spec, medv ~ ., Boston_train)\n\nrpart.plot(reg_tree_fit$fit)\nreg_tree_wf <- workflow() %>%\n  add_model(reg_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(medv ~ .)\n\nset.seed(1234)\nBoston_fold <- vfold_cv(Boston_train)\n\nparam_grid <- grid_regular(cost_complexity(), levels = 10)\n\ntune_res <- tune_grid(\n  reg_tree_wf, \n  resamples = Boston_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\nbest_complexity <- select_best(tune_res, metric = \"rmse\")\n\nreg_tree_final <- finalize_workflow(reg_tree_wf, best_complexity)\n\nreg_tree_final_fit <- fit(reg_tree_final, data = Boston_train)\n\nrpart.plot(reg_tree_final_fit$fit$fit$fit)"},{"path":"tree-based-methods.html","id":"bagging-and-random-forests","chapter":"8 Tree-Based Methods","heading":"8.3 Bagging and Random Forests","text":"","code":"\nbagging_spec <- rand_forest(mtry = .cols()) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\nbagging_fit <- fit(bagging_spec, medv ~ ., data = Boston_train)\naugment(bagging_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.85\naugment(bagging_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\nlibrary(vip)## \n## Attaching package: 'vip'## The following object is masked from 'package:utils':\n## \n##     vi\nvip(bagging_fit)\nrf_spec <- rand_forest(mtry = 6) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\nrf_fit <- fit(rf_spec, medv ~ ., data = Boston_train)\naugment(rf_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.72\naugment(rf_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\nlibrary(vip)\nvip(rf_fit)"},{"path":"tree-based-methods.html","id":"boosting","chapter":"8 Tree-Based Methods","heading":"8.4 Boosting","text":"","code":"\nboost_spec <- boost_tree(trees = 5000, tree_depth = 4) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\nboost_fit <- fit(boost_spec, medv ~ ., data = Boston_train)\naugment(boost_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.73\naugment(boost_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\nboost_spec <- boost_tree(trees = 5000, tree_depth = 4, learn_rate = 0.2) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\nboost_fit <- fit(boost_spec, medv ~ ., data = Boston_train)\naugment(boost_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        3.57\naugment(boost_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)"},{"path":"support-vector-machines.html","id":"support-vector-machines","chapter":"9 Support Vector Machines","heading":"9 Support Vector Machines","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(ISLR)"},{"path":"support-vector-machines.html","id":"support-vector-classifier","chapter":"9 Support Vector Machines","heading":"9.1 Support Vector Classifier","text":"","code":"\nset.seed(1)\nsim_data <- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1, x1),\n         x2 = ifelse(y == 1, x2 + 1, x2))\n\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit## parsnip model object\n## \n## Fit time:  578ms \n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 10 \n## \n## Polynomial kernel function. \n##  Hyperparameters : degree =  1  scale =  1  offset =  1 \n## \n## Number of Support Vectors : 12 \n## \n## Objective Function Value : -108.1122 \n## Training error : 0.2 \n## Probability model included.\nlibrary(kernlab)## \n## Attaching package: 'kernlab'## The following object is masked from 'package:purrr':\n## \n##     cross## The following object is masked from 'package:ggplot2':\n## \n##     alpha## The following object is masked from 'package:scales':\n## \n##     alpha\nplot(svm_linear_fit$fit)\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit## parsnip model object\n## \n## Fit time:  26ms \n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 0.1 \n## \n## Polynomial kernel function. \n##  Hyperparameters : degree =  1  scale =  1  offset =  1 \n## \n## Number of Support Vectors : 18 \n## \n## Objective Function Value : -1.5443 \n## Training error : 0.2 \n## Probability model included.\nsvm_linear_wf <- workflow() %>%\n  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>%\n  fit(sim_data)\nset.seed(2)\nsim_data_test <- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1, x1),\n         x2 = ifelse(y == 1, x2 + 1, x2))\naugment(svm_linear_fit, new_data = sim_data_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)##           Truth\n## Prediction -1 1\n##         -1  8 2\n##         1   2 8"},{"path":"support-vector-machines.html","id":"support-vector-machine","chapter":"9 Support Vector Machines","heading":"9.2 Support Vector Machine","text":"","code":""},{"path":"support-vector-machines.html","id":"roc-curves","chapter":"9 Support Vector Machines","heading":"9.3 ROC Curves","text":"","code":""},{"path":"support-vector-machines.html","id":"svm-with-multiple-classes","chapter":"9 Support Vector Machines","heading":"9.4 SVM with Multiple Classes","text":"","code":""},{"path":"support-vector-machines.html","id":"application-to-gene-expression-data","chapter":"9 Support Vector Machines","heading":"9.5 Application to Gene Expression Data","text":"","code":""},{"path":"unsupervised-learning.html","id":"unsupervised-learning","chapter":"10 Unsupervised Learning","heading":"10 Unsupervised Learning","text":"","code":"\nlibrary(tidymodels)## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ──## ✓ broom        0.7.6          ✓ recipes      0.1.16    \n## ✓ dials        0.0.9          ✓ rsample      0.0.9     \n## ✓ dplyr        1.0.5          ✓ tibble       3.1.1     \n## ✓ ggplot2      3.3.3          ✓ tidyr        1.1.3     \n## ✓ infer        0.5.4          ✓ tune         0.1.5     \n## ✓ modeldata    0.1.0          ✓ workflows    0.2.2     \n## ✓ parsnip      0.1.5.9002     ✓ workflowsets 0.0.2     \n## ✓ purrr        0.3.4          ✓ yardstick    0.0.8## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x purrr::discard() masks scales::discard()\n## x dplyr::filter()  masks stats::filter()\n## x dplyr::lag()     masks stats::lag()\n## x recipes::step()  masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✓ readr   1.4.0     ✓ forcats 0.5.1\n## ✓ stringr 1.4.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x readr::col_factor() masks scales::col_factor()\n## x purrr::discard()    masks scales::discard()\n## x dplyr::filter()     masks stats::filter()\n## x stringr::fixed()    masks recipes::fixed()\n## x dplyr::lag()        masks stats::lag()\n## x readr::spec()       masks yardstick::spec()\nlibrary(magrittr)## \n## Attaching package: 'magrittr'## The following object is masked from 'package:tidyr':\n## \n##     extract## The following object is masked from 'package:purrr':\n## \n##     set_names\nlibrary(factoextra)## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nlibrary(patchwork)\nlibrary(proxy)## \n## Attaching package: 'proxy'## The following objects are masked from 'package:stats':\n## \n##     as.dist, dist## The following object is masked from 'package:base':\n## \n##     as.matrix\nlibrary(mclust)## Package 'mclust' version 5.4.6\n## Type 'citation(\"mclust\")' for citing this R package in publications.## \n## Attaching package: 'mclust'## The following object is masked from 'package:purrr':\n## \n##     map"},{"path":"unsupervised-learning.html","id":"principal-components-analysis","chapter":"10 Unsupervised Learning","heading":"10.1 Principal Components Analysis","text":"","code":"\nusarrests <- as_tibble(USArrests, rownames = \"state\")\nglimpse(usarrests)## Rows: 50\n## Columns: 5\n## $ state    <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Co…\n## $ Murder   <dbl> 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n## $ Assault  <int> 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n## $ UrbanPop <int> 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n## $ Rape     <dbl> 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n# scale before applyig PCA\nset.seed(1)\npca_recipe <- recipe(~., data = usarrests) %>%\n  step_scale(all_numeric()) %>%\n  step_pca(all_numeric(), id = \"pca\") %>%\n  prep()\n\nusarrests %>%\n  column_to_rownames(\"state\") %>%\n  prcomp(scale = TRUE) %>%\n  fviz_pca_biplot(title = \"Biplot PCA on usarrests\")\n# loadings\ntidy(pca_recipe, type = \"coef\", id = \"pca\")## # A tibble: 16 x 4\n##    terms      value component id   \n##    <chr>      <dbl> <chr>     <chr>\n##  1 Murder   -0.319  PC1       pca  \n##  2 Assault  -0.368  PC1       pca  \n##  3 UrbanPop -0.774  PC1       pca  \n##  4 Rape     -0.405  PC1       pca  \n##  5 Murder   -0.579  PC2       pca  \n##  6 Assault  -0.484  PC2       pca  \n##  7 UrbanPop  0.603  PC2       pca  \n##  8 Rape     -0.258  PC2       pca  \n##  9 Murder    0.418  PC3       pca  \n## 10 Assault   0.197  PC3       pca  \n## 11 UrbanPop  0.188  PC3       pca  \n## 12 Rape     -0.867  PC3       pca  \n## 13 Murder   -0.624  PC4       pca  \n## 14 Assault   0.769  PC4       pca  \n## 15 UrbanPop -0.0381 PC4       pca  \n## 16 Rape     -0.134  PC4       pca\ntidy(pca_recipe, type = \"coef\", id = \"pca\") %>%\n  pivot_wider(\n    id_cols = \"terms\",\n    names_from = \"component\",\n    values_from = \"value\"\n  )## # A tibble: 4 x 5\n##   terms       PC1    PC2    PC3     PC4\n##   <chr>     <dbl>  <dbl>  <dbl>   <dbl>\n## 1 Murder   -0.319 -0.579  0.418 -0.624 \n## 2 Assault  -0.368 -0.484  0.197  0.769 \n## 3 UrbanPop -0.774  0.603  0.188 -0.0381\n## 4 Rape     -0.405 -0.258 -0.867 -0.134\n# variance\ntidy(pca_recipe, type = \"variance\", id = \"pca\")## # A tibble: 16 x 4\n##    terms                         value component id   \n##    <chr>                         <dbl>     <int> <chr>\n##  1 variance                     35.7           1 pca  \n##  2 variance                      1.47          2 pca  \n##  3 variance                      0.394         3 pca  \n##  4 variance                      0.180         4 pca  \n##  5 cumulative variance          35.7           1 pca  \n##  6 cumulative variance          37.1           2 pca  \n##  7 cumulative variance          37.5           3 pca  \n##  8 cumulative variance          37.7           4 pca  \n##  9 percent variance             94.6           1 pca  \n## 10 percent variance              3.90          2 pca  \n## 11 percent variance              1.05          3 pca  \n## 12 percent variance              0.478         4 pca  \n## 13 cumulative percent variance  94.6           1 pca  \n## 14 cumulative percent variance  98.5           2 pca  \n## 15 cumulative percent variance  99.5           3 pca  \n## 16 cumulative percent variance 100             4 pca\ntidy(pca_recipe, type = \"variance\", id = \"pca\") %>%\n  pivot_wider(\n    id_cols = \"terms\",\n    names_from = \"component\",\n    names_prefix = \"PC_\",\n    values_from = \"value\"\n  )## # A tibble: 4 x 5\n##   terms                        PC_1  PC_2   PC_3    PC_4\n##   <chr>                       <dbl> <dbl>  <dbl>   <dbl>\n## 1 variance                     35.7  1.47  0.394   0.180\n## 2 cumulative variance          35.7 37.1  37.5    37.7  \n## 3 percent variance             94.6  3.90  1.05    0.478\n## 4 cumulative percent variance  94.6 98.5  99.5   100\n# cumulative varianvce plot\ntidy(pca_recipe, type = \"variance\", id = \"pca\") %>%\n  filter(terms == \"cumulative variance\") %>%\n  ggplot(aes(component, value)) +\n  geom_point() +\n  geom_line() +\n  ylim(c(0, 100)) +\n  ylab(\"Cumulative variance\")\n# on the direct PCA object\nusarrests_pca <- usarrests %>%\n  select(-state) %>%\n  prcomp(scale = TRUE)\n\ntidy(usarrests_pca)## # A tibble: 200 x 3\n##      row    PC  value\n##    <int> <dbl>  <dbl>\n##  1     1     1 -0.976\n##  2     1     2  1.12 \n##  3     1     3 -0.440\n##  4     1     4  0.155\n##  5     2     1 -1.93 \n##  6     2     2  1.06 \n##  7     2     3  2.02 \n##  8     2     4 -0.434\n##  9     3     1 -1.75 \n## 10     3     2 -0.738\n## # … with 190 more rows\naugment(usarrests_pca)## # A tibble: 50 x 5\n##    .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4\n##    <chr>          <dbl>      <dbl>      <dbl>      <dbl>\n##  1 1            -0.976      1.12      -0.440     0.155  \n##  2 2            -1.93       1.06       2.02     -0.434  \n##  3 3            -1.75      -0.738      0.0542   -0.826  \n##  4 4             0.140      1.11       0.113    -0.181  \n##  5 5            -2.50      -1.53       0.593    -0.339  \n##  6 6            -1.50      -0.978      1.08      0.00145\n##  7 7             1.34      -1.08      -0.637    -0.117  \n##  8 8            -0.0472    -0.322     -0.711    -0.873  \n##  9 9            -2.98       0.0388    -0.571    -0.0953 \n## 10 10           -1.62       1.27      -0.339     1.07   \n## # … with 40 more rows"},{"path":"unsupervised-learning.html","id":"kmeans-clustering","chapter":"10 Unsupervised Learning","heading":"10.2 Kmeans Clustering","text":"","code":"\nset.seed(2)\nx <- matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1] <- x[1:25, 1] + 3\nx[1:25, 2] <- x[1:25, 2] - 4\n\ncolnames(x) <- c(\"V1\", \"V2\")\nx_df <- as_tibble(x)\nx_df## # A tibble: 50 x 2\n##       V1    V2\n##    <dbl> <dbl>\n##  1  2.10 -4.84\n##  2  3.18 -1.93\n##  3  4.59 -4.56\n##  4  1.87 -2.72\n##  5  2.92 -5.05\n##  6  3.13 -5.97\n##  7  3.71 -4.32\n##  8  2.76 -3.06\n##  9  4.98 -2.86\n## 10  2.86 -2.33\n## # … with 40 more rows\nres_kmeans <- kmeans(x_df, centers = 3, nstart = 20)\nglance(res_kmeans)## # A tibble: 1 x 4\n##   totss tot.withinss betweenss  iter\n##   <dbl>        <dbl>     <dbl> <int>\n## 1  474.         98.0      376.     2\ntidy(res_kmeans)## # A tibble: 3 x 5\n##       V1      V2  size withinss cluster\n##    <dbl>   <dbl> <int>    <dbl> <fct>  \n## 1  3.78  -4.56      17     25.7 1      \n## 2 -0.382 -0.0874    23     52.7 2      \n## 3  2.30  -2.70      10     19.6 3\naugment(res_kmeans, data = x_df)## # A tibble: 50 x 3\n##       V1    V2 .cluster\n##    <dbl> <dbl> <fct>   \n##  1  2.10 -4.84 1       \n##  2  3.18 -1.93 3       \n##  3  4.59 -4.56 1       \n##  4  1.87 -2.72 3       \n##  5  2.92 -5.05 1       \n##  6  3.13 -5.97 1       \n##  7  3.71 -4.32 1       \n##  8  2.76 -3.06 3       \n##  9  4.98 -2.86 1       \n## 10  2.86 -2.33 3       \n## # … with 40 more rows"},{"path":"unsupervised-learning.html","id":"hierarchical-clustering","chapter":"10 Unsupervised Learning","heading":"10.3 Hierarchical Clustering","text":"","code":"\nres_hclust_complete <- x_df %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\nres_hclust_average <- x_df %>%\n  dist() %>%\n  hclust(method = \"average\")\n\nres_hclust_single <- x_df %>%\n  dist() %>%\n  hclust(method = \"single\")\n\nwrap_plots(\n  fviz_dend(res_hclust_complete, main = \"complete\", k = 2),\n  fviz_dend(res_hclust_average, main = \"average\", k = 2),\n  fviz_dend(res_hclust_single, main = \"single\", k = 2),\n  ncol = 1\n)\n# clustering with scaled features\nx_df %>%\n  scale() %>%\n  dist() %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend(k = 2)\n# correlation based distance\nset.seed(2)\nx <- matrix(rnorm(30 * 3), ncol = 3)\n\nx %>%\n  proxy::dist(method = \"correlation\") %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend()"},{"path":"unsupervised-learning.html","id":"pca-on-the-nci60-data","chapter":"10 Unsupervised Learning","heading":"10.4 PCA on the NCI60 Data","text":"","code":"\ndata(NCI60, package = \"ISLR\")\nnci60 <- NCI60$data %>%\n  as_tibble() %>%\n  set_colnames(., paste0(\"V_\", 1:ncol(.))) %>%\n  mutate(label = factor(NCI60$labs)) %>%\n  relocate(label)\n\nnci60 %>%\n  count(label, sort = TRUE)## # A tibble: 14 x 2\n##    label           n\n##    <fct>       <int>\n##  1 NSCLC           9\n##  2 RENAL           9\n##  3 MELANOMA        8\n##  4 BREAST          7\n##  5 COLON           7\n##  6 LEUKEMIA        6\n##  7 OVARIAN         6\n##  8 CNS             5\n##  9 PROSTATE        2\n## 10 K562A-repro     1\n## 11 K562B-repro     1\n## 12 MCF7A-repro     1\n## 13 MCF7D-repro     1\n## 14 UNKNOWN         1\nnci60_pca <- prcomp(nci60 %>% select(-label), scale = TRUE)\ntidy(nci60_pca)## # A tibble: 4,096 x 3\n##      row    PC   value\n##    <int> <dbl>   <dbl>\n##  1     1     1 -19.7  \n##  2     1     2   3.53 \n##  3     1     3  -9.74 \n##  4     1     4   0.818\n##  5     1     5 -12.5  \n##  6     1     6   7.41 \n##  7     1     7 -14.1  \n##  8     1     8   3.17 \n##  9     1     9 -21.8  \n## 10     1    10  20.2  \n## # … with 4,086 more rows\naugment(nci60_pca)## # A tibble: 64 x 65\n##    .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4 .fittedPC5 .fittedPC6\n##    <chr>          <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n##  1 1              -19.7       3.53     -9.74       0.818     -12.5       7.41 \n##  2 2              -22.9       6.39    -13.4       -5.59       -7.97      3.69 \n##  3 3              -27.2       2.45     -3.51       1.33      -12.5      17.2  \n##  4 4              -42.5      -9.69     -0.883     -3.42      -41.9      27.0  \n##  5 5              -55.0      -5.16    -20.9      -15.7       -10.4      12.9  \n##  6 6              -27.0       6.73    -21.6      -13.7         7.93      0.707\n##  7 7              -31.2       3.83    -30.1      -41.3        10.3     -16.9  \n##  8 8              -22.2      10.3     -18.6       -6.90       -5.48     11.6  \n##  9 9              -14.2      16.0     -19.6       -6.51       -3.77     -7.96 \n## 10 10             -29.5      23.8      -5.84       9.94        3.42     11.6  \n## # … with 54 more rows, and 58 more variables: .fittedPC7 <dbl>,\n## #   .fittedPC8 <dbl>, .fittedPC9 <dbl>, .fittedPC10 <dbl>, .fittedPC11 <dbl>,\n## #   .fittedPC12 <dbl>, .fittedPC13 <dbl>, .fittedPC14 <dbl>, .fittedPC15 <dbl>,\n## #   .fittedPC16 <dbl>, .fittedPC17 <dbl>, .fittedPC18 <dbl>, .fittedPC19 <dbl>,\n## #   .fittedPC20 <dbl>, .fittedPC21 <dbl>, .fittedPC22 <dbl>, .fittedPC23 <dbl>,\n## #   .fittedPC24 <dbl>, .fittedPC25 <dbl>, .fittedPC26 <dbl>, .fittedPC27 <dbl>,\n## #   .fittedPC28 <dbl>, .fittedPC29 <dbl>, .fittedPC30 <dbl>, .fittedPC31 <dbl>,\n## #   .fittedPC32 <dbl>, .fittedPC33 <dbl>, .fittedPC34 <dbl>, .fittedPC35 <dbl>,\n## #   .fittedPC36 <dbl>, .fittedPC37 <dbl>, .fittedPC38 <dbl>, .fittedPC39 <dbl>,\n## #   .fittedPC40 <dbl>, .fittedPC41 <dbl>, .fittedPC42 <dbl>, .fittedPC43 <dbl>,\n## #   .fittedPC44 <dbl>, .fittedPC45 <dbl>, .fittedPC46 <dbl>, .fittedPC47 <dbl>,\n## #   .fittedPC48 <dbl>, .fittedPC49 <dbl>, .fittedPC50 <dbl>, .fittedPC51 <dbl>,\n## #   .fittedPC52 <dbl>, .fittedPC53 <dbl>, .fittedPC54 <dbl>, .fittedPC55 <dbl>,\n## #   .fittedPC56 <dbl>, .fittedPC57 <dbl>, .fittedPC58 <dbl>, .fittedPC59 <dbl>,\n## #   .fittedPC60 <dbl>, .fittedPC61 <dbl>, .fittedPC62 <dbl>, .fittedPC63 <dbl>,\n## #   .fittedPC64 <dbl>\npc_first_three <- augment(nci60_pca) %>%\n  select(c(.fittedPC1, .fittedPC2, .fittedPC3)) %>%\n  mutate(label = factor(NCI60$labs))\n\nwrap_plots(\n  pc_first_three %>%\n    ggplot(aes(.fittedPC1, .fittedPC2, color = label)) +\n    geom_point(size = 5, alpha = 0.5) +\n    scale_color_discrete(guide = FALSE),\n  pc_first_three %>%\n    ggplot(aes(.fittedPC1, .fittedPC3, color = label)) +\n    geom_point(size = 5, alpha = 0.5)\n)\nsummary(nci60_pca)$importance %>%\n  t() %>%\n  as_tibble(.name_repair = \"universal\") %>%\n  rowid_to_column() %>%\n  select(-Standard.deviation) %>%\n  pivot_longer(cols = -rowid) %>%\n  ggplot(aes(rowid, value)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(name ~ ., scales = \"free\") +\n  xlab(\"Principal Component\")## New names:\n## * `Standard deviation` -> Standard.deviation\n## * `Proportion of Variance` -> Proportion.of.Variance\n## * `Cumulative Proportion` -> Cumulative.Proportion"},{"path":"unsupervised-learning.html","id":"clustering-on-nci60-dataset","chapter":"10 Unsupervised Learning","heading":"10.5 Clustering on nci60 dataset","text":"","code":"\nnci60_scaled_mat <- nci60 %>%\n  select(-label) %>%\n  as.matrix() %>%\n  scale()\nrownames(nci60_scaled_mat) <- as.character(nci60$label)\n\n# hierarchical clustering of NIC60 data\nproduce_dend <- function(method) {\n  nci60_scaled_mat %>%\n    dist() %>%\n    hclust(method = method) %>%\n    fviz_dend()\n}\n\nwrap_plots(\n  produce_dend(\"complete\"),\n  produce_dend(\"average\"),\n  produce_dend(\"single\"),\n  ncol = 1\n)\nres_hclust_complete <- nci60_scaled_mat %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\nres_hclust_complete %>%\n  fviz_dend(k = 4, main = \"hclust(complete) on nci60\")\ntibble(\n  label = nci60$label,\n  cluster_id = cutree(res_hclust_complete, k = 4)\n) %>%\n  count(label, cluster_id) %>%\n  group_by(cluster_id) %>%\n  mutate(prop = n / sum(n)) %>%\n  slice_max(n = 1, order_by = prop) %>%\n  ungroup()## # A tibble: 6 x 4\n##   label    cluster_id     n  prop\n##   <fct>         <int> <int> <dbl>\n## 1 MELANOMA          1     8 0.2  \n## 2 NSCLC             1     8 0.2  \n## 3 RENAL             1     8 0.2  \n## 4 BREAST            2     3 0.429\n## 5 LEUKEMIA          3     6 0.75 \n## 6 COLON             4     5 0.556\nset.seed(2)\nres_kmeans_scaled <- kmeans(nci60_scaled_mat, centers = 4, nstart = 20)\nglance(res_kmeans_scaled)## # A tibble: 1 x 4\n##    totss tot.withinss betweenss  iter\n##    <dbl>        <dbl>     <dbl> <int>\n## 1 430290      344567.    85723.     3\ntidy(res_kmeans_scaled) %>%\n  select(cluster, size, withinss)## # A tibble: 4 x 3\n##   cluster  size withinss\n##   <fct>   <int>    <dbl>\n## 1 1          20  108801.\n## 2 2          27  154545.\n## 3 3           9   37150.\n## 4 4           8   44071.\ncluster_kmeans <- res_kmeans_scaled$cluster\ncluster_hclust <- cutree(res_hclust_complete, k = 4)\n\ntibble(\n  kmeans = factor(cluster_kmeans),\n  hclust = factor(cluster_hclust)\n) %>%\n  conf_mat(kmeans, hclust)##           Truth\n## Prediction  1  2  3  4\n##          1 11 20  9  0\n##          2  0  7  0  0\n##          3  0  0  0  8\n##          4  9  0  0  0\nadjustedRandIndex(cluster_kmeans, cluster_hclust)## [1] 0.2238347\n# pick first five PC and observe the clusters from hclust\nnci60_scaled_mat %>%\n  unname() %>%\n  prcomp() %>%\n  tidy() %>%\n  filter(PC <= 5) %>%\n  pivot_wider(\n    id_cols = row,\n    names_from = PC,\n    values_from = value,\n    names_prefix = \"PC_\"\n  ) %>%\n  select(-row) %>%\n  as.matrix() %>%\n  set_rownames(nci60$label) %>%\n  dist() %>%\n  hclust() %>%\n  fviz_dend(k = 4, main = \"hclust on first five PCs\")"}]
