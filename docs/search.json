[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISLR tidymodels labs",
    "section": "",
    "text": "1 Introduction\nThis book aims to be a complement to the 2nd edition An Introduction to Statistical Learning book with translations of the labs into using the tidymodels set of packages.\nThe labs will be mirrored quite closely to stay true to the original material."
  },
  {
    "objectID": "index.html#edition-differences",
    "href": "index.html#edition-differences",
    "title": "ISLR tidymodels labs",
    "section": "Edition Differences",
    "text": "Edition Differences\nAll listed changes will be relative to the 1st edition.\n\n\nNaive Bayes has been added to chapter 4 on Classification\n\n\nPoisson Regression has been added to chapter 4 on Classification\n\n“Application to Caravan Insurance Data” section is no longer treated as its own section and is now part of the K-Nearest Neighbors section\n\nBayesian Additive Regression Trees has been added to chapter 8 on Tree-Based Methods\n\nchapter on Unsupervised Learning has been renumbered to chapter 12 instead of 10\n\nMatrix Completion has been added to chapter 12 on Unsupervised Learning\n\nchapter on Deep learning has been added as chapter 10\nchapter on Survival Analysis and Censored Data as been added as chapter 11\nchapter on Multiple Testing as been added as chapter 13"
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "ISLR tidymodels labs",
    "section": "Colophon",
    "text": "Colophon\nThis book was written in RStudio using Quarto. The website is hosted via GitHub Pages, and the complete source is available on GitHub.\nThis version of the book was built with R version 4.3.0 (2023-04-21) and the following packages:\n\n\n\npackage\nversion\nsource\n\n\n\nbroom\n1.0.4\nCRAN (R 4.3.0)\n\n\ncorrr\n0.4.4\nCRAN (R 4.3.0)\n\n\ndials\n1.2.0\nCRAN (R 4.3.0)\n\n\ndiscrim\n1.0.1\nCRAN (R 4.3.0)\n\n\ndownlit\n0.4.2\nCRAN (R 4.3.0)\n\n\ndplyr\n1.1.2\nCRAN (R 4.3.0)\n\n\nfactoextra\n1.0.7\nCRAN (R 4.3.0)\n\n\nggplot2\n3.4.2\nCRAN (R 4.3.0)\n\n\nglmnet\n4.1-7\nCRAN (R 4.3.0)\n\n\ninfer\n1.0.4\nCRAN (R 4.3.0)\n\n\nISLR\n1.4\nCRAN (R 4.3.0)\n\n\nISLR2\n1.3-2\nCRAN (R 4.3.0)\n\n\nkernlab\n0.9-32\nCRAN (R 4.3.0)\n\n\nkknn\n1.3.1\nCRAN (R 4.3.0)\n\n\nklaR\n1.7-2\nCRAN (R 4.3.0)\n\n\nMASS\n7.3-58.4\nCRAN (R 4.3.0)\n\n\nmclust\n6.0.0\nCRAN (R 4.3.0)\n\n\nmixOmics\n6.24.0\nBioconduc~\n\n\nmodeldata\n1.1.0\nCRAN (R 4.3.0)\n\n\npaletteer\n1.5.0\nCRAN (R 4.3.0)\n\n\nparsnip\n1.1.0\nCRAN (R 4.3.0)\n\n\npatchwork\n1.1.2\nCRAN (R 4.3.0)\n\n\npoissonreg\n1.0.1\nCRAN (R 4.3.0)\n\n\nproxy\n0.4-27\nCRAN (R 4.3.0)\n\n\npurrr\n1.0.1\nCRAN (R 4.3.0)\n\n\nrandomForest\n4.7-1.1\nCRAN (R 4.3.0)\n\n\nreadr\n2.1.4\nCRAN (R 4.3.0)\n\n\nrecipes\n1.0.6\nCRAN (R 4.3.0)\n\n\nrpart\n4.1.19\nCRAN (R 4.3.0)\n\n\nrpart.plot\n3.1.1\nCRAN (R 4.3.0)\n\n\nrsample\n1.1.1\nCRAN (R 4.3.0)\n\n\nscico\n1.3.1\nCRAN (R 4.3.0)\n\n\ntibble\n3.2.1\nCRAN (R 4.3.0)\n\n\ntidyclust\n0.1.2\nCRAN (R 4.3.0)\n\n\ntidymodels\n1.1.0\nCRAN (R 4.3.0)\n\n\ntidyr\n1.3.0\nCRAN (R 4.3.0)\n\n\ntune\n1.1.1\nCRAN (R 4.3.0)\n\n\nvip\n0.3.2\nCRAN (R 4.3.0)\n\n\nworkflows\n1.1.3\nCRAN (R 4.3.0)\n\n\nworkflowsets\n1.0.1\nCRAN (R 4.3.0)\n\n\nxgboost\n1.7.5.1\nCRAN (R 4.3.0)\n\n\nyardstick\n1.2.0\nCRAN (R 4.3.0)"
  },
  {
    "objectID": "06-regularization.html#best-subset-selection",
    "href": "06-regularization.html#best-subset-selection",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.1 Best Subset Selection",
    "text": "6.1 Best Subset Selection\ntidymodels does not currently support subset selection methods, and it unlikely to include it in the near future."
  },
  {
    "objectID": "06-regularization.html#forward-and-backward-stepwise-selection",
    "href": "06-regularization.html#forward-and-backward-stepwise-selection",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.2 Forward and Backward Stepwise Selection",
    "text": "6.2 Forward and Backward Stepwise Selection\ntidymodels does not currently support forward and backward stepwise selection methods, and it unlikely to include it in the near future."
  },
  {
    "objectID": "06-regularization.html#ridge-regression",
    "href": "06-regularization.html#ridge-regression",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.3 Ridge Regression",
    "text": "6.3 Ridge Regression\nWe will use the glmnet package to perform ridge regression. parsnip does not have a dedicated function to create a ridge regression model specification. You need to use linear_reg() and set mixture = 0 to specify a ridge model. The mixture argument specifies the amount of different types of regularization, mixture = 0 specifies only ridge regularization and mixture = 1 specifies only lasso regularization. Setting mixture to a value between 0 and 1 lets us use both. When using the glmnet engine we also need to set a penalty to be able to fit the model. We will set this value to 0 for now, it is not the best value, but we will look at how to select the best value in a little bit.\n\nridge_spec &lt;- linear_reg(mixture = 0, penalty = 0) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"glmnet\")\n\nOnce the specification is created we can fit it to our data. We will use all the predictors.\n\nridge_fit &lt;- fit(ridge_spec, Salary ~ ., data = Hitters)\n\nThe glmnet package will fit the model for all values of penalty at once, so let us see what the parameter estimate for the model is now that we have penalty = 0.\n\ntidy(ridge_fit)\n\n# A tibble: 20 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   81.1           0\n 2 AtBat         -0.682         0\n 3 Hits           2.77          0\n 4 HmRun         -1.37          0\n 5 Runs           1.01          0\n 6 RBI            0.713         0\n 7 Walks          3.38          0\n 8 Years         -9.07          0\n 9 CAtBat        -0.00120       0\n10 CHits          0.136         0\n11 CHmRun         0.698         0\n12 CRuns          0.296         0\n13 CRBI           0.257         0\n14 CWalks        -0.279         0\n15 LeagueN       53.2           0\n16 DivisionW   -123.            0\n17 PutOuts        0.264         0\n18 Assists        0.170         0\n19 Errors        -3.69          0\n20 NewLeagueN   -18.1           0\n\n\nLet us instead see what the estimates would be if the penalty was 11498.\n\ntidy(ridge_fit, penalty = 11498)\n\n# A tibble: 20 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 407.        11498\n 2 AtBat         0.0370    11498\n 3 Hits          0.138     11498\n 4 HmRun         0.525     11498\n 5 Runs          0.231     11498\n 6 RBI           0.240     11498\n 7 Walks         0.290     11498\n 8 Years         1.11      11498\n 9 CAtBat        0.00314   11498\n10 CHits         0.0117    11498\n11 CHmRun        0.0876    11498\n12 CRuns         0.0234    11498\n13 CRBI          0.0242    11498\n14 CWalks        0.0250    11498\n15 LeagueN       0.0866    11498\n16 DivisionW    -6.23      11498\n17 PutOuts       0.0165    11498\n18 Assists       0.00262   11498\n19 Errors       -0.0206    11498\n20 NewLeagueN    0.303     11498\n\n\nNotice how the estimates are decreasing when the amount of penalty goes up. Look below at the parameter estimates for penalty = 705 and penalty = 50.\n\ntidy(ridge_fit, penalty = 705)\n\n# A tibble: 20 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  54.4        705\n 2 AtBat         0.112      705\n 3 Hits          0.656      705\n 4 HmRun         1.18       705\n 5 Runs          0.937      705\n 6 RBI           0.847      705\n 7 Walks         1.32       705\n 8 Years         2.58       705\n 9 CAtBat        0.0108     705\n10 CHits         0.0468     705\n11 CHmRun        0.338      705\n12 CRuns         0.0937     705\n13 CRBI          0.0979     705\n14 CWalks        0.0718     705\n15 LeagueN      13.7        705\n16 DivisionW   -54.7        705\n17 PutOuts       0.119      705\n18 Assists       0.0161     705\n19 Errors       -0.704      705\n20 NewLeagueN    8.61       705\n\ntidy(ridge_fit, penalty = 50)\n\n# A tibble: 20 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   48.2          50\n 2 AtBat         -0.354        50\n 3 Hits           1.95         50\n 4 HmRun         -1.29         50\n 5 Runs           1.16         50\n 6 RBI            0.809        50\n 7 Walks          2.71         50\n 8 Years         -6.20         50\n 9 CAtBat         0.00609      50\n10 CHits          0.107        50\n11 CHmRun         0.629        50\n12 CRuns          0.217        50\n13 CRBI           0.215        50\n14 CWalks        -0.149        50\n15 LeagueN       45.9          50\n16 DivisionW   -118.           50\n17 PutOuts        0.250        50\n18 Assists        0.121        50\n19 Errors        -3.28         50\n20 NewLeagueN    -9.42         50\n\n\nWe can visualize how the magnitude of the coefficients are being regularized towards zero as the penalty goes up.\n\nridge_fit %&gt;%\n  autoplot()\n\n\n\n\nPrediction is done like normal, if we use predict() by itself, then penalty = 0 as we set in the model specification is used.\n\npredict(ridge_fit, new_data = Hitters)\n\n# A tibble: 263 × 1\n    .pred\n    &lt;dbl&gt;\n 1  442. \n 2  676. \n 3 1059. \n 4  521. \n 5  543. \n 6  218. \n 7   74.7\n 8   96.1\n 9  809. \n10  865. \n# ℹ 253 more rows\n\n\nbut we can also get predictions for other values of penalty by specifying it in predict()\n\npredict(ridge_fit, new_data = Hitters, penalty = 500)\n\n# A tibble: 263 × 1\n   .pred\n   &lt;dbl&gt;\n 1  525.\n 2  620.\n 3  895.\n 4  425.\n 5  589.\n 6  179.\n 7  147.\n 8  187.\n 9  841.\n10  840.\n# ℹ 253 more rows\n\n\nWe saw how we can fit a ridge model and make predictions for different values of penalty. But it would be nice if we could find the “best” value of the penalty. This is something we can use hyperparameter tuning for. Hyperparameter tuning is in its simplest form a way of fitting many models with different sets of hyperparameters trying to find one that performs “best”. The complexity in hyperparameter tuning can come from how you try different models. We will keep it simple for this lab and only look at grid search, only looking at evenly spaced parameter values. This is a fine enough approach if you have one or two tunable parameters but can become computationally infeasible. See the chapter on iterative search from Tidy Modeling with R for more information.\nWe start like normal by setting up a validation split. A K-fold cross-validation data set is created on the training data set with 10 folds.\n\nHitters_split &lt;- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train &lt;- training(Hitters_split)\nHitters_test &lt;- testing(Hitters_split)\n\nHitters_fold &lt;- vfold_cv(Hitters_train, v = 10)\n\nWe can use the tune_grid() function to perform hyperparameter tuning using a grid search. tune_grid() needs 3 different thing;\n\na workflow object containing the model and preprocessor,\na rset object containing the resamples the workflow should be fitted within, and\na tibble containing the parameter values to be evaluated.\n\nOptionally a metric set of performance metrics can be supplied for evaluation. If you don’t set one then a default set of performance metrics is used.\nWe already have a resample object created in Hitters_fold. Now we should create the workflow specification next.\nWe just used the data set as is when we fit the model earlier. But ridge regression is scale sensitive so we need to make sure that the variables are on the same scale. We can use step_normalize(). Secondly let us deal with the factor variables ourself using step_novel() and step_dummy().\n\nridge_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors())\n\nThe model specification will look very similar to what we have seen earlier, but we will set penalty = tune(). This tells tune_grid() that the penalty parameter should be tuned.\n\nridge_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 0) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\")\n\nNow we combine to create a workflow object.\n\nridge_workflow &lt;- workflow() %&gt;% \n  add_recipe(ridge_recipe) %&gt;% \n  add_model(ridge_spec)\n\nThe last thing we need is the values of penalty we are trying. This can be created using grid_regular() which creates a grid of evenly spaces parameter values. We use the penalty() function from the dials package to denote the parameter and set the range of the grid we are searching for. Note that this range is log-scaled.\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)\npenalty_grid\n\n# A tibble: 50 × 1\n     penalty\n       &lt;dbl&gt;\n 1 0.00001  \n 2 0.0000160\n 3 0.0000256\n 4 0.0000409\n 5 0.0000655\n 6 0.000105 \n 7 0.000168 \n 8 0.000268 \n 9 0.000429 \n10 0.000687 \n# ℹ 40 more rows\n\n\nUsing 50 levels for one parameter might seem overkill and in many applications it is. But remember that glmnet fits all the models in one go so adding more levels to penalty doesn’t affect the computational speed much.\nNow we have everything we need and we can fit all the models.\n\ntune_res &lt;- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\ntune_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [176/20]&gt; Fold01 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [176/20]&gt; Fold02 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [176/20]&gt; Fold03 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [176/20]&gt; Fold04 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [176/20]&gt; Fold05 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [176/20]&gt; Fold06 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [177/19]&gt; Fold07 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [177/19]&gt; Fold08 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [177/19]&gt; Fold09 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [177/19]&gt; Fold10 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nThe output of tune_grid() can be hard to read by itself unprocessed. autoplot() creates a great visualization\n\nautoplot(tune_res)\n\n\n\n\nHere we see that the amount of regularization affects the performance metrics differently. Note how there are areas where the amount of regularization doesn’t have any meaningful influence on the coefficient estimates. We can also see the raw metrics that created this chart by calling collect_matrics().\n\ncollect_metrics(tune_res)\n\n# A tibble: 100 × 7\n     penalty .metric .estimator    mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 0.00001   rmse    standard   335.       10 31.2    Preprocessor1_Model01\n 2 0.00001   rsq     standard     0.470    10  0.0615 Preprocessor1_Model01\n 3 0.0000160 rmse    standard   335.       10 31.2    Preprocessor1_Model02\n 4 0.0000160 rsq     standard     0.470    10  0.0615 Preprocessor1_Model02\n 5 0.0000256 rmse    standard   335.       10 31.2    Preprocessor1_Model03\n 6 0.0000256 rsq     standard     0.470    10  0.0615 Preprocessor1_Model03\n 7 0.0000409 rmse    standard   335.       10 31.2    Preprocessor1_Model04\n 8 0.0000409 rsq     standard     0.470    10  0.0615 Preprocessor1_Model04\n 9 0.0000655 rmse    standard   335.       10 31.2    Preprocessor1_Model05\n10 0.0000655 rsq     standard     0.470    10  0.0615 Preprocessor1_Model05\n# ℹ 90 more rows\n\n\nThe “best” values of this can be selected using select_best(), this function requires you to specify a matric that it should select against.\n\nbest_penalty &lt;- select_best(tune_res, metric = \"rsq\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1    33.9 Preprocessor1_Model33\n\n\nThis value of penalty can then be used with finalize_workflow() to update/finalize the recipe by replacing tune() with the value of best_penalty. Now, this model should be fit again, this time using the whole training data set.\n\nridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit &lt;- fit(ridge_final, data = Hitters_train)\n\nThis final model can now be applied on our testing data set to validate the performance\n\naugment(ridge_final_fit, new_data = Hitters_test) %&gt;%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.461\n\n\nAnd it performs fairly well given what we saw earlier."
  },
  {
    "objectID": "06-regularization.html#the-lasso",
    "href": "06-regularization.html#the-lasso",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.4 The Lasso",
    "text": "6.4 The Lasso\nWe will use the glmnet package to perform lasso regression. parsnip does not have a dedicated function to create a ridge regression model specification. You need to use linear_reg() and set mixture = 1 to specify a lasso model. The mixture argument specifies the amount of different types of regularization, mixture = 0 specifies only ridge regularization and mixture = 1 specifies only lasso regularization. Setting mixture to a value between 0 and 1 lets us use both.\nThe following procedure will be very similar to what we saw in the ridge regression section. The preprocessing needed is the same, but let us write it out one more time.\n\nlasso_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors())\n\nNext, we finish the lasso regression workflow.\n\nlasso_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(lasso_recipe) %&gt;% \n  add_model(lasso_spec)\n\nWhile we are doing a different kind of regularization we still use the same penalty argument. I have picked a different range for the values of penalty since I know it will be a good range. You would in practice have to cast a wide net at first and then narrow on the range of interest.\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-2, 2)), levels = 50)\n\nAnd we can use tune_grid() again.\n\ntune_res &lt;- tune_grid(\n  lasso_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nautoplot(tune_res)\n\n\n\n\nWe select the best value of penalty using select_best()\n\nbest_penalty &lt;- select_best(tune_res, metric = \"rsq\")\n\nAnd refit the using the whole training data set.\n\nlasso_final &lt;- finalize_workflow(lasso_workflow, best_penalty)\n\nlasso_final_fit &lt;- fit(lasso_final, data = Hitters_train)\n\nAnd we are done, by calculating the rsq value for the lasso model can we see that for this data ridge regression outperform lasso regression.\n\naugment(lasso_final_fit, new_data = Hitters_test) %&gt;%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.446"
  },
  {
    "objectID": "06-regularization.html#principal-components-regression",
    "href": "06-regularization.html#principal-components-regression",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.5 Principal Components Regression",
    "text": "6.5 Principal Components Regression\nWe will talk more about principal components analysis in chapter 10. This section will show how principal components can be used as a dimensionality reduction preprocessing step.\nI will treat principal component regression as a linear model with PCA transformations in the preprocessing. But using the tidymodels framework then this is still mostly one model.\n\nlm_spec &lt;- \n  linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nThe preprocessing recipe will closely resemble the recipe we saw in the ridge and lasso sections. The main difference is that we end the recipe with step_pca() which will perform principal component analysis on all the predictors, and return the components that explain threshold percent of the variance. We have set threshold = tune() so we can treat the threshold as a hyperparameter to be tuned. By using workflows and tune together can be tune parameters in the preprocessing as well as parameters in the models.\n\npca_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), threshold = tune())\n\npca_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(pca_recipe) %&gt;% \n  add_model(lm_spec)\n\nWe create a smaller grid for threshold and we don’t need to modify the range since [0, 1] is an acceptable range.\n\nthreshold_grid &lt;- grid_regular(threshold(), levels = 10)\nthreshold_grid\n\n# A tibble: 10 × 1\n   threshold\n       &lt;dbl&gt;\n 1     0    \n 2     0.111\n 3     0.222\n 4     0.333\n 5     0.444\n 6     0.556\n 7     0.667\n 8     0.778\n 9     0.889\n10     1    \n\n\nAnd now we fit using tune_grid(). This time we will actually perform 100 fits since we need to fit a model for each value of threshold within each fold.\n\ntune_res &lt;- tune_grid(\n  pca_workflow,\n  resamples = Hitters_fold, \n  grid = threshold_grid\n)\n\nThe results look a little shaky here.\n\nautoplot(tune_res)\n\n\n\n\nBut we can still select the best model.\n\nbest_threshold &lt;- select_best(tune_res, metric = \"rmse\")\n\nAnd fit the model much like have done a couple of times by now. The workflow is finalized using the value we selected with select_best(), and training using the full training data set.\n\npca_final &lt;- finalize_workflow(pca_workflow, best_threshold)\n\npca_final_fit &lt;- fit(pca_final, data = Hitters_train)"
  },
  {
    "objectID": "06-regularization.html#partial-least-squares",
    "href": "06-regularization.html#partial-least-squares",
    "title": "\n6  Linear Model Selection and Regularization\n",
    "section": "\n6.6 Partial Least Squares",
    "text": "6.6 Partial Least Squares\nLastly, we have a partial least squares model. We will treat this much like the PCA section and say that partial least squares calculations will be done as a preprocessing that we tune. The following code is almost identical to previous chapters and will be shown in full without many explanations to avoid repetition. If you skipped to this section, go back and read the previous sections for more commentary.\n\npls_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors()) %&gt;%\n  step_pls(all_predictors(), num_comp = tune(), outcome = \"Salary\")\n\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") \n\npls_workflow &lt;- workflow() %&gt;% \n  add_recipe(pls_recipe) %&gt;% \n  add_model(lm_spec) \n\nnum_comp_grid &lt;- grid_regular(num_comp(c(1, 20)), levels = 10)\n\ntune_res &lt;- tune_grid(\n  pls_workflow,\n  resamples = Hitters_fold, \n  grid = num_comp_grid\n)\n\nbest_threshold &lt;- select_best(tune_res, metric = \"rmse\")\n\npls_final &lt;- finalize_workflow(pls_workflow, best_threshold)\n\npls_final_fit &lt;- fit(pls_final, data = Hitters_train)"
  },
  {
    "objectID": "07-moving-beyond-linearity.html#polynomial-regression-and-step-functions",
    "href": "07-moving-beyond-linearity.html#polynomial-regression-and-step-functions",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.1 Polynomial Regression and Step Functions",
    "text": "7.1 Polynomial Regression and Step Functions\nPolynomial regression can be thought of as doing polynomial expansion on a variable and passing that expansion into a linear regression model. We will be very explicit in this formulation in this chapter. step_poly() allows us to do a polynomial expansion on one or more variables.\nThe following step will take age and replace it with the variables age, age^2, age^3, and age^4 since we set degree = 4.\n\nrec_poly &lt;- recipe(wage ~ age, data = Wage) %&gt;%\n  step_poly(age, degree = 4)\n\nThis recipe is combined with a linear regression specification and combined to create a workflow object.\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\npoly_wf &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_poly)\n\nThis object can now be fit()\n\npoly_fit &lt;- fit(poly_wf, data = Wage)\npoly_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)   age_poly_1   age_poly_2   age_poly_3   age_poly_4  \n     111.70       447.07      -478.32       125.52       -77.91  \n\n\nAnd we cal pull the coefficients using tidy()\n\ntidy(poly_fit)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    112.      0.729    153.   0       \n2 age_poly_1     447.     39.9       11.2  1.48e-28\n3 age_poly_2    -478.     39.9      -12.0  2.36e-32\n4 age_poly_3     126.     39.9        3.14 1.68e- 3\n5 age_poly_4     -77.9    39.9       -1.95 5.10e- 2\n\n\nI was lying when I said that step_poly() returned age, age^2, age^3, and age^4. What is happening is that it returns variables that are a basis of orthogonal polynomials, which means that each of the columns is a linear combination of the variables age, age^2, age^3, and age^4. We can see this by using poly() directly with raw = FALSE since it is the default\n\npoly(1:6, degree = 4, raw = FALSE)\n\n              1          2          3          4\n[1,] -0.5976143  0.5455447 -0.3726780  0.1889822\n[2,] -0.3585686 -0.1091089  0.5217492 -0.5669467\n[3,] -0.1195229 -0.4364358  0.2981424  0.3779645\n[4,]  0.1195229 -0.4364358 -0.2981424  0.3779645\n[5,]  0.3585686 -0.1091089 -0.5217492 -0.5669467\n[6,]  0.5976143  0.5455447  0.3726780  0.1889822\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 3.5 3.5 3.5 3.5\n\nattr(,\"coefs\")$norm2\n[1]  1.00000  6.00000 17.50000 37.33333 64.80000 82.28571\n\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nWe see that these variables don’t directly have a format we would have assumed. But this is still a well-reasoned transformation. We can get the raw polynomial transformation by setting raw = TRUE\n\npoly(1:6, degree = 4, raw = TRUE)\n\n     1  2   3    4\n[1,] 1  1   1    1\n[2,] 2  4   8   16\n[3,] 3  9  27   81\n[4,] 4 16  64  256\n[5,] 5 25 125  625\n[6,] 6 36 216 1296\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nThese transformations align with what we would expect. It is still recommended to stick with the default of raw = FALSE unless you have a reason not to do that. One of the benefits of using raw = FALSE is that the resulting variables are uncorrelated which is a desirable quality when using a linear regression model.\nYou can get the raw polynomials by setting options = list(raw = TRUE) in step_poly()\n\nrec_raw_poly &lt;- recipe(wage ~ age, data = Wage) %&gt;%\n  step_poly(age, degree = 4, options = list(raw = TRUE))\n\nraw_poly_wf &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_raw_poly)\n\nraw_poly_fit &lt;- fit(raw_poly_wf, data = Wage)\n\ntidy(raw_poly_fit)\n\n# A tibble: 5 × 5\n  term            estimate  std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -184.        60.0           -3.07 0.00218 \n2 age_poly_1    21.2        5.89           3.61 0.000312\n3 age_poly_2    -0.564      0.206         -2.74 0.00626 \n4 age_poly_3     0.00681    0.00307        2.22 0.0264  \n5 age_poly_4    -0.0000320  0.0000164     -1.95 0.0510  \n\n\nLet us try something new and visualize the polynomial fit on our data. We can do this easily because we only have 1 predictor and 1 response. Starting with creating a tibble with different ranges of age. Then we take this tibble and predict with it, this will give us the repression curve. We are additionally adding confidence intervals by setting type = \"conf_int\" which we can do since we are using a linear regression model.\n\nage_range &lt;- tibble(age = seq(min(Wage$age), max(Wage$age)))\n\nregression_lines &lt;- bind_cols(\n  augment(poly_fit, new_data = age_range),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\nregression_lines\n\n# A tibble: 63 × 4\n     age .pred .pred_lower .pred_upper\n   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1    18  51.9        41.5        62.3\n 2    19  58.5        49.9        67.1\n 3    20  64.6        57.5        71.6\n 4    21  70.2        64.4        76.0\n 5    22  75.4        70.5        80.2\n 6    23  80.1        76.0        84.2\n 7    24  84.5        80.9        88.1\n 8    25  88.5        85.2        91.7\n 9    26  92.1        89.1        95.2\n10    27  95.4        92.5        98.4\n# ℹ 53 more rows\n\n\nWe will then use ggplot2 to visualize the fitted line and confidence interval. The green line is the regression curve and the dashed blue lines are the confidence interval.\n\nWage %&gt;%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n\n\n\n\nThe regression curve is now a curve instead of a line as we would have gotten with a simple linear regression model. Notice furthermore that the confidence bands are tighter when there is a lot of data and they wider towards the ends of the data.\nLet us take that one step further and see what happens to the regression line once we go past the domain it was trained on. the previous plot showed individuals within the age range 18-80. Let us see what happens once we push this to 18-100. This is not an impossible range but an unrealistic range.\n\nwide_age_range &lt;- tibble(age = seq(18, 100))\n\nregression_lines &lt;- bind_cols(\n  augment(poly_fit, new_data = wide_age_range),\n  predict(poly_fit, new_data = wide_age_range, type = \"conf_int\")\n)\n\nWage %&gt;%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n\n\n\n\nAnd we see that the curve starts diverging once we get to 93 the predicted wage is negative. The confidence bands also get wider and wider as we get farther away from the data.\nWe can also think of this problem as a classification problem, and we will do that just now by setting us the task of predicting whether an individual earns more than $250000 per year. We will add a new factor value denoting this response.\n\nWage &lt;- Wage %&gt;%\n  mutate(high = factor(wage &gt; 250, \n                       levels = c(TRUE, FALSE), \n                       labels = c(\"High\", \"Low\")))\n\nWe cannot use the polynomial expansion recipe rec_poly we created earlier since it had wage as the response and now we want to have high as the response. We also have to create a logistic regression specification that we will use as our classification model.\n\nrec_poly &lt;- recipe(high ~ age, data = Wage) %&gt;%\n  step_poly(age, degree = 4)\n\nlr_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nlr_poly_wf &lt;- workflow() %&gt;%\n  add_model(lr_spec) %&gt;%\n  add_recipe(rec_poly)\n\nThis polynomial logistic regression model workflow can now be fit and predicted with as usual.\n\nlr_poly_fit &lt;- fit(lr_poly_wf, data = Wage)\n\npredict(lr_poly_fit, new_data = Wage)\n\n# A tibble: 3,000 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 Low        \n 2 Low        \n 3 Low        \n 4 Low        \n 5 Low        \n 6 Low        \n 7 Low        \n 8 Low        \n 9 Low        \n10 Low        \n# ℹ 2,990 more rows\n\n\nIf we want we can also get back the underlying probability predictions for the two classes, and their confidence intervals for these probability predictions by setting type = \"prob\" and type = \"conf_int\".\n\npredict(lr_poly_fit, new_data = Wage, type = \"prob\")\n\n# A tibble: 3,000 × 2\n      .pred_High .pred_Low\n           &lt;dbl&gt;     &lt;dbl&gt;\n 1 0.00000000983     1.00 \n 2 0.000120          1.00 \n 3 0.0307            0.969\n 4 0.0320            0.968\n 5 0.0305            0.970\n 6 0.0352            0.965\n 7 0.0313            0.969\n 8 0.00820           0.992\n 9 0.0334            0.967\n10 0.0323            0.968\n# ℹ 2,990 more rows\n\npredict(lr_poly_fit, new_data = Wage, type = \"conf_int\")\n\n# A tibble: 3,000 × 4\n   .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n              &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1         2.22e-16          0.00166           0.998           1    \n 2         1.82e- 6          0.00786           0.992           1.00 \n 3         2.19e- 2          0.0428            0.957           0.978\n 4         2.31e- 2          0.0442            0.956           0.977\n 5         2.13e- 2          0.0434            0.957           0.979\n 6         2.45e- 2          0.0503            0.950           0.975\n 7         2.25e- 2          0.0434            0.957           0.977\n 8         3.01e- 3          0.0222            0.978           0.997\n 9         2.39e- 2          0.0465            0.953           0.976\n10         2.26e- 2          0.0458            0.954           0.977\n# ℹ 2,990 more rows\n\n\nWe can use these to visualize the probability curve for the classification model.\n\nregression_lines &lt;- bind_cols(\n  augment(lr_poly_fit, new_data = age_range, type = \"prob\"),\n  predict(lr_poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nregression_lines %&gt;%\n  ggplot(aes(age)) +\n  ylim(c(0, 0.2)) +\n  geom_line(aes(y = .pred_High), color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower_High), color = \"blue\", linetype = \"dashed\") +\n  geom_line(aes(y = .pred_upper_High), color = \"blue\", linetype = \"dashed\") +\n  geom_jitter(aes(y = (high == \"High\") / 5), data = Wage, \n              shape = \"|\", height = 0, width = 0.2)\n\n\n\n\nNext, let us take a look at the step function and how to fit a model using it as a preprocessor. You can create step functions in a couple of different ways. step_discretize() will convert a numeric variable into a factor variable with n bins, n here is specified with num_breaks. These will have approximately the same number of points in them according to the training data set.\n\nrec_discretize &lt;- recipe(high ~ age, data = Wage) %&gt;%\n  step_discretize(age, num_breaks = 4)\n\ndiscretize_wf &lt;- workflow() %&gt;%\n  add_model(lr_spec) %&gt;%\n  add_recipe(rec_discretize)\n\ndiscretize_fit &lt;- fit(discretize_wf, data = Wage)\ndiscretize_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_discretize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      agebin2      agebin3      agebin4  \n      5.004       -1.492       -1.665       -1.705  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:      730.5 \nResidual Deviance: 710.4    AIC: 718.4\n\n\nIf you already know where you want the step function to break then you can use step_cut() and supply the breaks manually.\n\nrec_cut &lt;- recipe(high ~ age, data = Wage) %&gt;%\n  step_cut(age, breaks = c(30, 50, 70))\n\ncut_wf &lt;- workflow() %&gt;%\n  add_model(lr_spec) %&gt;%\n  add_recipe(rec_cut)\n\ncut_fit &lt;- fit(cut_wf, data = Wage)\ncut_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_cut()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)   age(30,50]   age(50,70]   age(70,80]  \n      6.256       -2.746       -3.038       10.310  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:      730.5 \nResidual Deviance: 704.3    AIC: 712.3"
  },
  {
    "objectID": "07-moving-beyond-linearity.html#splines",
    "href": "07-moving-beyond-linearity.html#splines",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.2 Splines",
    "text": "7.2 Splines\nIn order to fit regression splines, or in other words, use splines as preprocessors when fitting a linear model, we use step_bs() to construct the matrices of basis functions. The bs() function is used and arguments such as knots can be passed to bs() by using passing a named list to options.\n\nrec_spline &lt;- recipe(wage ~ age, data = Wage) %&gt;%\n  step_bs(age, options = list(knots = 25, 40, 60))\n\nWe already have the linear regression specification lm_spec so we can create the workflow, fit the model and predict with it like we have seen how to do in the previous chapters.\n\nspline_wf &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spline)\n\nspline_fit &lt;- fit(spline_wf, data = Wage)\n\npredict(spline_fit, new_data = Wage)\n\n# A tibble: 3,000 × 1\n   .pred\n   &lt;dbl&gt;\n 1  58.7\n 2  84.3\n 3 120. \n 4 120. \n 5 120. \n 6 119. \n 7 120. \n 8 102. \n 9 119. \n10 120. \n# ℹ 2,990 more rows\n\n\nLastly, we can plot the basic spline on top of the data.\n\nregression_lines &lt;- bind_cols(\n  augment(spline_fit, new_data = age_range),\n  predict(spline_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %&gt;%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")"
  },
  {
    "objectID": "07-moving-beyond-linearity.html#gams",
    "href": "07-moving-beyond-linearity.html#gams",
    "title": "\n7  Moving Beyond Linearity\n",
    "section": "\n7.3 GAMs",
    "text": "7.3 GAMs\nGAM section is WIP since they are now supported in parsnip."
  },
  {
    "objectID": "08-tree-based-methods.html#fitting-classification-trees",
    "href": "08-tree-based-methods.html#fitting-classification-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.1 Fitting Classification Trees",
    "text": "8.1 Fitting Classification Trees\nWe will both be fitting a classification and regression tree in this section, so we can save a little bit of typing by creating a general decision tree specification using rpart as the engine.\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\")\n\nThen this decision tree specification can be used to create a classification decision tree engine. This is a good example of how the flexible composition system created by parsnip can be used to create multiple model specifications.\n\nclass_tree_spec &lt;- tree_spec %&gt;%\n  set_mode(\"classification\")\n\nWith both a model specification and our data are we ready to fit the model.\n\nclass_tree_fit &lt;- class_tree_spec %&gt;%\n  fit(High ~ ., data = Carseats)\n\nWhen we look at the model output we see a quite informative summary of the model. It tries to give a written description of the tree that is created.\n\nclass_tree_fit\n\nparsnip model object\n\nn= 400 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 164 No (0.59000000 0.41000000)  \n    2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  \n      4) Price&gt;=92.5 269  66 No (0.75464684 0.24535316)  \n        8) Advertising&lt; 13.5 224  41 No (0.81696429 0.18303571)  \n         16) CompPrice&lt; 124.5 96   6 No (0.93750000 0.06250000) *\n         17) CompPrice&gt;=124.5 128  35 No (0.72656250 0.27343750)  \n           34) Price&gt;=109.5 107  20 No (0.81308411 0.18691589)  \n             68) Price&gt;=126.5 65   6 No (0.90769231 0.09230769) *\n             69) Price&lt; 126.5 42  14 No (0.66666667 0.33333333)  \n              138) Age&gt;=49.5 22   2 No (0.90909091 0.09090909) *\n              139) Age&lt; 49.5 20   8 Yes (0.40000000 0.60000000) *\n           35) Price&lt; 109.5 21   6 Yes (0.28571429 0.71428571) *\n        9) Advertising&gt;=13.5 45  20 Yes (0.44444444 0.55555556)  \n         18) Age&gt;=54.5 20   5 No (0.75000000 0.25000000) *\n         19) Age&lt; 54.5 25   5 Yes (0.20000000 0.80000000) *\n      5) Price&lt; 92.5 46  14 Yes (0.30434783 0.69565217)  \n       10) Income&lt; 57 10   3 No (0.70000000 0.30000000) *\n       11) Income&gt;=57 36   7 Yes (0.19444444 0.80555556) *\n    3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  \n      6) Price&gt;=142.5 12   3 No (0.75000000 0.25000000) *\n      7) Price&lt; 142.5 73  10 Yes (0.13698630 0.86301370) *\n\n\nOnce the tree gets more than a couple of nodes it can become hard to read the printed diagram. The rpart.plot package provides functions to let us easily visualize the decision tree. As the name implies, it only works with rpart trees.\n\nclass_tree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()\n\n\n\n\nWe can see that the most important variable to predict high sales appears to be shelving location as it forms the first node.\nThe training accuracy of this model is 85%\n\naugment(class_tree_fit, new_data = Carseats) %&gt;%\n  accuracy(truth = High, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.848\n\n\nLet us take a look at the confusion matrix to see if the balance is there\n\naugment(class_tree_fit, new_data = Carseats) %&gt;%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  200  25\n       Yes  36 139\n\n\nAnd the model appears to work well overall. But this model was fit on the whole data set so we only get the training accuracy which could be misleading if the model is overfitting. Let us redo the fitting by creating a validation split and fit the model on the training data set.\n\nset.seed(1234)\nCarseats_split &lt;- initial_split(Carseats)\n\nCarseats_train &lt;- training(Carseats_split)\nCarseats_test &lt;- testing(Carseats_split)\n\nNow we can fit the model on the training data set.\n\nclass_tree_fit &lt;- fit(class_tree_spec, High ~ ., data = Carseats_train)\n\nLet us take a look at the confusion matrix for the training data set and testing data set.\n\naugment(class_tree_fit, new_data = Carseats_train) %&gt;%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction  No Yes\n       No  159  21\n       Yes  21  99\n\n\nThe training data set performs well as we would expect\n\naugment(class_tree_fit, new_data = Carseats_test) %&gt;%\n  conf_mat(truth = High, estimate = .pred_class)\n\n          Truth\nPrediction No Yes\n       No  41   8\n       Yes 15  36\n\n\nbut the testing data set doesn’t perform just as well and get a smaller accuracy of 77%\n\naugment(class_tree_fit, new_data = Carseats_test) %&gt;%\n  accuracy(truth = High, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.77\n\n\nLet us try to tune the cost_complexity of the decision tree to find a more optimal complexity. We use the class_tree_spec object and use the set_args() function to specify that we want to tune cost_complexity. This is then passed directly into the workflow object to avoid creating an intermediate object.\n\nclass_tree_wf &lt;- workflow() %&gt;%\n  add_model(class_tree_spec %&gt;% set_args(cost_complexity = tune())) %&gt;%\n  add_formula(High ~ .)\n\nTo be able to tune the variable we need 2 more objects. S resamples object, we will use a k-fold cross-validation data set, and a grid of values to try. Since we are only tuning 1 hyperparameter it is fine to stay with a regular grid.\n\nset.seed(1234)\nCarseats_fold &lt;- vfold_cv(Carseats_train)\n\nparam_grid &lt;- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)\n\ntune_res &lt;- tune_grid(\n  class_tree_wf, \n  resamples = Carseats_fold, \n  grid = param_grid, \n  metrics = metric_set(accuracy)\n)\n\nusing autoplot() shows which values of cost_complexity appear to produce the highest accuracy\n\nautoplot(tune_res)\n\n\n\n\nWe can now select the best performing value with select_best(), finalize the workflow by updating the value of cost_complexity and fit the model on the full training data set.\n\nbest_complexity &lt;- select_best(tune_res)\n\nclass_tree_final &lt;- finalize_workflow(class_tree_wf, best_complexity)\n\nclass_tree_final_fit &lt;- fit(class_tree_final, data = Carseats_train)\nclass_tree_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nHigh ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 300 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 300 120 No (0.6000000 0.4000000)  \n  2) ShelveLoc=Bad,Medium 242  73 No (0.6983471 0.3016529)  \n    4) Price&gt;=92.5 213  51 No (0.7605634 0.2394366) *\n    5) Price&lt; 92.5 29   7 Yes (0.2413793 0.7586207) *\n  3) ShelveLoc=Good 58  11 Yes (0.1896552 0.8103448) *\n\n\nAt last, we can visualize the model, and we see that the better-performing model is less complex than the original model we fit.\n\nclass_tree_final_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()"
  },
  {
    "objectID": "08-tree-based-methods.html#fitting-regression-trees",
    "href": "08-tree-based-methods.html#fitting-regression-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.2 Fitting Regression Trees",
    "text": "8.2 Fitting Regression Trees\nWe will now show how we fit a regression tree. This is very similar to what we saw in the last section. The main difference here is that the response we are looking at will be continuous instead of categorical. We can reuse tree_spec as a base for the regression decision tree specification.\n\nreg_tree_spec &lt;- tree_spec %&gt;%\n  set_mode(\"regression\")\n\nWe are using the Boston data set here so we will do a validation split here.\n\nset.seed(1234)\nBoston_split &lt;- initial_split(Boston)\n\nBoston_train &lt;- training(Boston_split)\nBoston_test &lt;- testing(Boston_split)\n\nfitting the model to the training data set\n\nreg_tree_fit &lt;- fit(reg_tree_spec, medv ~ ., Boston_train)\nreg_tree_fit\n\nparsnip model object\n\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.9500 22.54802  \n   2) rm&lt; 6.941 320 13602.3100 19.86281  \n     4) lstat&gt;=14.395 129  2582.1090 14.51550  \n       8) nox&gt;=0.607 80   984.7339 12.35875  \n        16) lstat&gt;=19.34 47   388.6332 10.35957 *\n        17) lstat&lt; 19.34 33   140.7188 15.20606 *\n       9) nox&lt; 0.607 49   617.6939 18.03673 *\n     5) lstat&lt; 14.395 191  4840.3640 23.47435  \n      10) rm&lt; 6.543 151  2861.3990 22.21192  \n        20) dis&gt;=1.68515 144  1179.5970 21.82083 *\n        21) dis&lt; 1.68515 7  1206.6970 30.25714 *\n      11) rm&gt;=6.543 40   829.8560 28.24000 *\n   3) rm&gt;=6.941 59  4199.1020 37.11186  \n     6) rm&lt; 7.437 35  1012.4100 32.08286 *\n     7) rm&gt;=7.437 24  1010.6200 44.44583  \n      14) ptratio&gt;=15.4 12   585.0767 40.71667 *\n      15) ptratio&lt; 15.4 12    91.7825 48.17500 *\n\n\n\naugment(reg_tree_fit, new_data = Boston_test) %&gt;%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.78\n\n\nand the rpart.plot() function works for the regression decision tree as well\n\nreg_tree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()\n\n\n\n\nNotice how the result is a numeric variable instead of a class.\nNow let us again try to tune the cost_complexity to find the best performing model.\n\nreg_tree_wf &lt;- workflow() %&gt;%\n  add_model(reg_tree_spec %&gt;% set_args(cost_complexity = tune())) %&gt;%\n  add_formula(medv ~ .)\n\nset.seed(1234)\nBoston_fold &lt;- vfold_cv(Boston_train)\n\nparam_grid &lt;- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)\n\ntune_res &lt;- tune_grid(\n  reg_tree_wf, \n  resamples = Boston_fold, \n  grid = param_grid\n)\n\nAnd it appears that higher complexity works are to be preferred according to our cross-validation\n\nautoplot(tune_res)\n\n\n\n\nWe select the best-performing model according to \"rmse\" and fit the final model on the whole training data set.\n\nbest_complexity &lt;- select_best(tune_res, metric = \"rmse\")\n\nreg_tree_final &lt;- finalize_workflow(reg_tree_wf, best_complexity)\n\nreg_tree_final_fit &lt;- fit(reg_tree_final, data = Boston_train)\nreg_tree_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmedv ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.95000 22.54802  \n   2) rm&lt; 6.941 320 13602.31000 19.86281  \n     4) lstat&gt;=14.395 129  2582.10900 14.51550  \n       8) nox&gt;=0.607 80   984.73390 12.35875  \n        16) lstat&gt;=19.34 47   388.63320 10.35957  \n          32) tax&gt;=551.5 40   243.94980  9.67750 *\n          33) tax&lt; 551.5 7    19.73714 14.25714 *\n        17) lstat&lt; 19.34 33   140.71880 15.20606 *\n       9) nox&lt; 0.607 49   617.69390 18.03673  \n        18) crim&gt;=0.381565 25   313.20000 16.20000 *\n        19) crim&lt; 0.381565 24   132.30000 19.95000 *\n     5) lstat&lt; 14.395 191  4840.36400 23.47435  \n      10) rm&lt; 6.543 151  2861.39900 22.21192  \n        20) dis&gt;=1.68515 144  1179.59700 21.82083  \n          40) rm&lt; 6.062 56   306.22860 20.28571 *\n          41) rm&gt;=6.062 88   657.41950 22.79773  \n            82) lstat&gt;=9.98 35    98.32686 21.02571 *\n            83) lstat&lt; 9.98 53   376.61550 23.96792 *\n        21) dis&lt; 1.68515 7  1206.69700 30.25714 *\n      11) rm&gt;=6.543 40   829.85600 28.24000  \n        22) lstat&gt;=4.44 33   274.06180 27.15455 *\n        23) lstat&lt; 4.44 7   333.61710 33.35714 *\n   3) rm&gt;=6.941 59  4199.10200 37.11186  \n     6) rm&lt; 7.437 35  1012.41000 32.08286  \n      12) nox&gt;=0.4885 14   673.46930 28.89286 *\n      13) nox&lt; 0.4885 21   101.49810 34.20952 *\n     7) rm&gt;=7.437 24  1010.62000 44.44583  \n      14) ptratio&gt;=15.4 12   585.07670 40.71667 *\n      15) ptratio&lt; 15.4 12    91.78250 48.17500 *\n\n\nVisualizing the model reveals a much more complex tree than what we saw in the last section.\n\nreg_tree_final_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()"
  },
  {
    "objectID": "08-tree-based-methods.html#bagging-and-random-forests",
    "href": "08-tree-based-methods.html#bagging-and-random-forests",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.3 Bagging and Random Forests",
    "text": "8.3 Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data set. We will be using the randomForest package as the engine. A bagging model is the same as a random forest where mtry is equal to the number of predictors. We can specify the mtry to be .cols() which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and useable to many different data sets. .cols() is one of many descriptors in the parsnip package. We also set importance = TRUE in set_engine() to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the vip package later.\n\nbagging_spec &lt;- rand_forest(mtry = .cols()) %&gt;%\n  set_engine(\"randomForest\", importance = TRUE) %&gt;%\n  set_mode(\"regression\")\n\nWe fit the model like normal\n\nbagging_fit &lt;- fit(bagging_spec, medv ~ ., data = Boston_train)\n\nand we take a look at the testing performance. Which we see is an improvement over the decision tree.\n\naugment(bagging_fit, new_data = Boston_test) %&gt;%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        3.43\n\n\nWe can also create a quick scatterplot between the true and predicted value to see if we can make any diagnostics.\n\naugment(bagging_fit, new_data = Boston_test) %&gt;%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nThere isn’t anything weird going on here so we are happy. Next, let us take a look at the variable importance\n\nvip(bagging_fit)\n\n\n\n\nNext, let us take a look at a random forest. By default, randomForest() p / 3 variables when building a random forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry = 6.\n\nrf_spec &lt;- rand_forest(mtry = 6) %&gt;%\n  set_engine(\"randomForest\", importance = TRUE) %&gt;%\n  set_mode(\"regression\")\n\nand fitting the model like normal\n\nrf_fit &lt;- fit(rf_spec, medv ~ ., data = Boston_train)\n\nthis model has a slightly better performance than the bagging model\n\naugment(rf_fit, new_data = Boston_test) %&gt;%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        3.26\n\n\nWe can likewise plot the true value against the predicted value\n\naugment(rf_fit, new_data = Boston_test) %&gt;%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nit looks fine. No discernible difference between this chart and the one we created for the bagging model.\nThe variable importance plot is also quite similar to what we saw for the bagging model which isn’t surprising.\n\nvip(rf_fit)\n\n\n\n\nyou would normally want to perform hyperparameter tuning for the random forest model to get the best out of your forest. This exercise is left for the reader."
  },
  {
    "objectID": "08-tree-based-methods.html#boosting",
    "href": "08-tree-based-methods.html#boosting",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.4 Boosting",
    "text": "8.4 Boosting\nWe will now fit a boosted tree model. The xgboost packages give a good implementation of boosted trees. It has many parameters to tune and we know that setting trees too high can lead to overfitting. Nevertheless, let us try fitting a boosted tree. We set tree = 5000 to grow 5000 trees with a maximal depth of 4 by setting tree_depth = 4.\n\nboost_spec &lt;- boost_tree(trees = 5000, tree_depth = 4) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nfitting the model like normal\n\nboost_fit &lt;- fit(boost_spec, medv ~ ., data = Boston_train)\n\nand the rmse is a little high in this case which is properly because we didn’t tune any of the parameters.\n\naugment(boost_fit, new_data = Boston_test) %&gt;%\n  rmse(truth = medv, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        3.34\n\n\nWe can look at the scatterplot and we don’t see anything weird going on.\n\naugment(boost_fit, new_data = Boston_test) %&gt;%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n\n\n\n\nYou would normally want to perform hyperparameter tuning for the boosted tree model to get the best out of your model. This exercise is left for the reader. Look at the Iterative search chapter of Tidy Modeling with R for inspiration."
  },
  {
    "objectID": "08-tree-based-methods.html#bayesian-additive-regression-trees",
    "href": "08-tree-based-methods.html#bayesian-additive-regression-trees",
    "title": "\n8  Tree-Based Methods\n",
    "section": "\n8.5 Bayesian Additive Regression Trees",
    "text": "8.5 Bayesian Additive Regression Trees\nThis section is WIP."
  },
  {
    "objectID": "09-support-vector-machines.html#support-vector-classifier",
    "href": "09-support-vector-machines.html#support-vector-classifier",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.1 Support Vector Classifier",
    "text": "9.1 Support Vector Classifier\nLet us start by creating a synthetic data set. We will use some normally distributed data with an added offset to create 2 separate classes.\n\nset.seed(1)\nsim_data &lt;- tibble(\n  x1 = rnorm(40),\n  x2 = rnorm(40),\n  y  = factor(rep(c(-1, 1), 20))\n) %&gt;%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n\nPlotting it shows that we are having two slightly overlapping classes\n\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\n\n\n\n\nWe can then create a linear SVM specification by setting degree = 1 in a polynomial SVM model. We furthermore set scaled = FALSE in set_engine() to have the engine scale the data for us. Once we get to it later we can be performing this scaling in a recipe instead.\n\n\n\n\n\n\n\n\n\n\nset_engine() can be used to pass in additional arguments directly to the underlying engine. In this case, I’m passing in scaled = FALSE to kernlab::ksvm() which is the engine function.\n\n\n\nsvm_linear_spec &lt;- svm_poly(degree = 1) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kernlab\", scaled = FALSE)\n\nTaking the specification, we can add a specific cost of 10 before fitting the model to the data. Using set_args() allows us to set the cost argument without modifying the model specification.\n\nsvm_linear_fit &lt;- svm_linear_spec %&gt;% \n  set_args(cost = 10) %&gt;%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 10 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 17 \n\nObjective Function Value : -152.0188 \nTraining error : 0.125 \nProbability model included. \n\n\nThe kernlab models can be visualized using the plot() function if you load the kernlab package.\n\nlibrary(kernlab)\nsvm_linear_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  plot()\n\n\n\n\nwhat if we instead used a smaller value of the cost parameter?\n\nsvm_linear_fit &lt;- svm_linear_spec %&gt;% \n  set_args(cost = 0.1) %&gt;%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 0.1 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 25 \n\nObjective Function Value : -2.0376 \nTraining error : 0.15 \nProbability model included. \n\n\nNow that a smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider.\nLet us set up a tune_grid() section to find the value of cost that leads to the highest accuracy for the SVM model.\n\nsvm_linear_wf &lt;- workflow() %&gt;%\n  add_model(svm_linear_spec %&gt;% set_args(cost = tune())) %&gt;%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold &lt;- vfold_cv(sim_data, strata = y)\n\nparam_grid &lt;- grid_regular(cost(), levels = 10)\n\ntune_res &lt;- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\n\n\n\n\nusing the tune_res object and select_best() function allows us to find the value of cost that gives the best cross-validated accuracy. Finalize the workflow with finalize_workflow() and fit the new workflow on the data set.\n\nbest_cost &lt;- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final &lt;- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit &lt;- svm_linear_final %&gt;% fit(sim_data)\n\nWe can now generate a different data set to act as the test data set. We will make sure that it is generated using the same model but with a different seed.\n\nset.seed(2)\nsim_data_test &lt;- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %&gt;%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n\nand asseessing the model on this testing data set shows us that the model still performs very well.\n\naugment(svm_linear_fit, new_data = sim_data_test) %&gt;%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction -1 1\n        -1  8 3\n        1   2 7"
  },
  {
    "objectID": "09-support-vector-machines.html#support-vector-machine",
    "href": "09-support-vector-machines.html#support-vector-machine",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.2 Support Vector Machine",
    "text": "9.2 Support Vector Machine\nWe will now see how we can fit an SVM using a non-linear kernel. Let us start by generating some data, but this time generate with a non-linear class boundary.\n\nset.seed(1)\nsim_data2 &lt;- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nsim_data2 %&gt;%\n  ggplot(aes(x1, x2, color = y)) +\n  geom_point()\n\n\n\n\nWe will try an SVM with a radial basis function. Such a kernel would allow us to capture the non-linearity in our data.\n\nsvm_rbf_spec &lt;- svm_rbf() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kernlab\")\n\nfitting the model\n\nsvm_rbf_fit &lt;- svm_rbf_spec %&gt;%\n  fit(y ~ ., data = sim_data2)\n\nand plotting reveals that the model was able to separate the two classes, even though they were non-linearly separated.\n\nsvm_rbf_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  plot()\n\n\n\n\nBut let us see how well this model generalizes to new data from the same generating process.\n\nset.seed(2)\nsim_data2_test &lt;- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nAnd it works well!\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %&gt;%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction   1   2\n         1 137   7\n         2  13  43"
  },
  {
    "objectID": "09-support-vector-machines.html#roc-curves",
    "href": "09-support-vector-machines.html#roc-curves",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.3 ROC Curves",
    "text": "9.3 ROC Curves\nROC curves can easily be created using the roc_curve() function from the yardstick package. We use this function much the same way as we have done using the accuracy() function, but the main difference is that we pass the predicted class probability instead of passing the predicted class.\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %&gt;%\n  roc_curve(truth = y, .pred_1)\n\n# A tibble: 202 × 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1   -Inf          0            1    \n 2      0.104      0            1    \n 3      0.113      0.0200       1    \n 4      0.114      0.0400       1    \n 5      0.115      0.0600       1    \n 6      0.117      0.0800       1    \n 7      0.118      0.1          1    \n 8      0.119      0.12         1    \n 9      0.124      0.14         1    \n10      0.124      0.14         0.993\n# ℹ 192 more rows\n\n\nThis produces the different values of specificity and sensitivity for each threshold. We can get a quick visualization by passing the results of roc_curve() into autoplot()\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %&gt;%\n  roc_curve(truth = y, .pred_1) %&gt;%\n  autoplot()\n\n\n\n\nA common metric is t calculate the area under this curve. This can be done using the roc_auc() function (_auc stands for area under curve).\n\naugment(svm_rbf_fit, new_data = sim_data2_test) %&gt;%\n  roc_auc(truth = y, .pred_1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.925"
  },
  {
    "objectID": "09-support-vector-machines.html#application-to-gene-expression-data",
    "href": "09-support-vector-machines.html#application-to-gene-expression-data",
    "title": "\n9  Support Vector Machines\n",
    "section": "\n9.4 Application to Gene Expression Data",
    "text": "9.4 Application to Gene Expression Data\nWe now examine the Khan data set, which consists of several tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set comes in the Khan list which we will wrangle a little bit to create two tibbles, 1 for the training data and 1 for the testing data.\n\nKhan_train &lt;- bind_cols(\n  y = factor(Khan$ytrain),\n  as_tibble(Khan$xtrain)\n)\n\nKhan_test &lt;- bind_cols(\n  y = factor(Khan$ytest),\n  as_tibble(Khan$xtest)\n)\n\nlooking at the dimensions of the training data reveals that we have 63 observations with 20308 gene expression measurements.\n\ndim(Khan_train)\n\n[1]   63 2309\n\n\nThere is a very large number of predictors compared to the number of rows. This indicates that a linear kernel will be preferable, as the added flexibility we would get from a polynomial or radial kernel is unnecessary.\n\nkhan_fit &lt;- svm_linear_spec %&gt;%\n  set_args(cost = 10) %&gt;%\n  fit(y ~ ., data = Khan_train)\n\nLet us take a look at the training confusion matrix. And look, we get a perfect confusion matrix. We are getting this because the hyperplane was able to fully separate the classes.\n\naugment(khan_fit, new_data = Khan_train) %&gt;%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction  1  2  3  4\n         1  8  0  0  0\n         2  0 23  0  0\n         3  0  0 12  0\n         4  0  0  0 20\n\n\nBut remember we don’t measure the performance by how well it performs on the training data set. We measure the performance of a model on how well it performs on the testing data set, so let us look at the testing confusion matrix\n\naugment(khan_fit, new_data = Khan_test) %&gt;%\n  conf_mat(truth = y, estimate = .pred_class)\n\n          Truth\nPrediction 1 2 3 4\n         1 3 0 0 0\n         2 0 6 2 0\n         3 0 0 4 0\n         4 0 0 0 5\n\n\nAnd it performs fairly well. A couple of misclassifications but nothing too bad."
  },
  {
    "objectID": "10-deep-learning.html",
    "href": "10-deep-learning.html",
    "title": "10  Deep learning",
    "section": "",
    "text": "There are no current plans to recreate this chapter using tidymodels as there isn’t any replacement for keras in tidymodels. If you would like something specific in this chapter please open an issue."
  },
  {
    "objectID": "11-survival-analysis.html",
    "href": "11-survival-analysis.html",
    "title": "\n11  Survival Analysis and Censored Data\n",
    "section": "",
    "text": "Survival analysis is being worked on in tidymodels, please check out censored as it is getting quite good, but still lacking a few key interactions with tidymodels to be included in this book."
  },
  {
    "objectID": "12-unsupervised-learning.html#principal-components-analysis",
    "href": "12-unsupervised-learning.html#principal-components-analysis",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.1 Principal Components Analysis",
    "text": "12.1 Principal Components Analysis\nThis section will be used to explore the USArrests data set using PCA. Before we move on, let is turn USArrests into a tibble and move the rownames into a column.\n\nUSArrests &lt;- as_tibble(USArrests, rownames = \"state\")\nUSArrests\n\n# A tibble: 50 × 5\n   state       Murder Assault UrbanPop  Rape\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;\n 1 Alabama       13.2     236       58  21.2\n 2 Alaska        10       263       48  44.5\n 3 Arizona        8.1     294       80  31  \n 4 Arkansas       8.8     190       50  19.5\n 5 California     9       276       91  40.6\n 6 Colorado       7.9     204       78  38.7\n 7 Connecticut    3.3     110       77  11.1\n 8 Delaware       5.9     238       72  15.8\n 9 Florida       15.4     335       80  31.9\n10 Georgia       17.4     211       60  25.8\n# ℹ 40 more rows\n\n\nNotice how the mean of each of the variables is quite different. if we were to apply PCA directly to the data set then Murder would have a very small influence.\n\nUSArrests %&gt;%\n  select(-state) %&gt;%\n  map_dfr(mean)\n\n# A tibble: 1 × 4\n  Murder Assault UrbanPop  Rape\n   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1   7.79    171.     65.5  21.2\n\n\nWe will show how to perform PCA in two different ways in this section. Firstly, by using prcomp() directly, using broom::tidy() to extract the information we need, and secondly by using recipes. prcomp() takes 1 required argument x which much be a fully numeric data.frame or matrix. Then we pass that to prcomp(). We also set scale = TRUE in prcomp() which will perform the scaling we need.\n\nUSArrests_pca &lt;- USArrests %&gt;%\n  select(-state) %&gt;%\n  prcomp(scale = TRUE)\n\nUSArrests_pca\n\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\nNow we can use our favorite broom function to extract information from this prcomp object. We start with tidy(). tidy() can be used to extract a couple of different things, see ?broom:::tidy.prcomp() for more information. tidy() will by default extract the scores of a PCA object in long tidy format. The score is the location of the observation in PCA space. So we can\n\ntidy(USArrests_pca)\n\n# A tibble: 200 × 3\n     row    PC  value\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1 -0.976\n 2     1     2 -1.12 \n 3     1     3  0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2 -1.06 \n 7     2     3 -2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2  0.738\n# ℹ 190 more rows\n\n\nWe can also explicitly say we want the scores by setting matrix = \"scores\".\n\ntidy(USArrests_pca, matrix = \"scores\")\n\n# A tibble: 200 × 3\n     row    PC  value\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1 -0.976\n 2     1     2 -1.12 \n 3     1     3  0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2 -1.06 \n 7     2     3 -2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2  0.738\n# ℹ 190 more rows\n\n\nNext, we can get the loadings of the PCA.\n\ntidy(USArrests_pca, matrix = \"loadings\")\n\n# A tibble: 16 × 3\n   column      PC   value\n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 Murder       1 -0.536 \n 2 Murder       2 -0.418 \n 3 Murder       3  0.341 \n 4 Murder       4  0.649 \n 5 Assault      1 -0.583 \n 6 Assault      2 -0.188 \n 7 Assault      3  0.268 \n 8 Assault      4 -0.743 \n 9 UrbanPop     1 -0.278 \n10 UrbanPop     2  0.873 \n11 UrbanPop     3  0.378 \n12 UrbanPop     4  0.134 \n13 Rape         1 -0.543 \n14 Rape         2  0.167 \n15 Rape         3 -0.818 \n16 Rape         4  0.0890\n\n\nThis information tells us how each variable contributes to each principal component. If you don’t have too many principal components you can visualize the contribution without filtering\n\ntidy(USArrests_pca, matrix = \"loadings\") %&gt;%\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col() +\n  scale_x_continuous(labels = scales::percent)\n\n\n\n\nLastly, we can set matrix = \"eigenvalues\" and get back the explained standard deviation for each PC including as a percent and cumulative which is quite handy for plotting.\n\ntidy(USArrests_pca, matrix = \"eigenvalues\")\n\n# A tibble: 4 × 4\n     PC std.dev percent cumulative\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1   1.57   0.620       0.620\n2     2   0.995  0.247       0.868\n3     3   0.597  0.0891      0.957\n4     4   0.416  0.0434      1    \n\n\nIf we want to see how the percent standard deviation explained drops off for each PC we can easily get that by using tidy() with matrix = \"eigenvalues\".\n\ntidy(USArrests_pca, matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, percent)) +\n  geom_col()\n\n\n\n\nLastly, we have the augment() function which will give you back the fitted PC transformation if you apply it to the prcomp() object directly\n\naugment(USArrests_pca)\n\n# A tibble: 50 × 5\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 1            -0.976     -1.12       0.440     0.155  \n 2 2            -1.93      -1.06      -2.02     -0.434  \n 3 3            -1.75       0.738     -0.0542   -0.826  \n 4 4             0.140     -1.11      -0.113    -0.181  \n 5 5            -2.50       1.53      -0.593    -0.339  \n 6 6            -1.50       0.978     -1.08      0.00145\n 7 7             1.34       1.08       0.637    -0.117  \n 8 8            -0.0472     0.322      0.711    -0.873  \n 9 9            -2.98      -0.0388     0.571    -0.0953 \n10 10           -1.62      -1.27       0.339     1.07   \n# ℹ 40 more rows\n\n\nand will apply this transformation to new data by passing the new data to newdata\n\naugment(USArrests_pca, newdata = USArrests[1:5, ])\n\n# A tibble: 5 × 10\n  .rownames state Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 1         Alab…   13.2     236       58  21.2     -0.976     -1.12      0.440 \n2 2         Alas…   10       263       48  44.5     -1.93      -1.06     -2.02  \n3 3         Ariz…    8.1     294       80  31       -1.75       0.738    -0.0542\n4 4         Arka…    8.8     190       50  19.5      0.140     -1.11     -0.113 \n5 5         Cali…    9       276       91  40.6     -2.50       1.53     -0.593 \n# ℹ 1 more variable: .fittedPC4 &lt;dbl&gt;\n\n\nIf you are using PCA as a preprocessing method I recommend you use recipes to apply the PCA transformation. This is a good way of doing it since recipe will correctly apply the same transformation to new data that the recipe is used on.\nWe step_normalize() to make sure all the variables are on the same scale. By using all_numeric() we are able to apply PCA on the variables we want without having to remove state. We are also setting an id for step_pca() to make it easier to tidy() later.\n\npca_rec &lt;- recipe(~., data = USArrests) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_pca(all_numeric(), id = \"pca\") %&gt;%\n  prep()\n\nBy calling bake(new_data = NULL) we can get the fitted PC transformation of our numerical variables\n\npca_rec %&gt;%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 5\n   state           PC1     PC2     PC3      PC4\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 Alabama     -0.976  -1.12    0.440   0.155  \n 2 Alaska      -1.93   -1.06   -2.02   -0.434  \n 3 Arizona     -1.75    0.738  -0.0542 -0.826  \n 4 Arkansas     0.140  -1.11   -0.113  -0.181  \n 5 California  -2.50    1.53   -0.593  -0.339  \n 6 Colorado    -1.50    0.978  -1.08    0.00145\n 7 Connecticut  1.34    1.08    0.637  -0.117  \n 8 Delaware    -0.0472  0.322   0.711  -0.873  \n 9 Florida     -2.98   -0.0388  0.571  -0.0953 \n10 Georgia     -1.62   -1.27    0.339   1.07   \n# ℹ 40 more rows\n\n\nbut we can also supply our own data to new_data.\n\npca_rec %&gt;%\n  bake(new_data = USArrests[40:45, ])\n\n# A tibble: 6 × 5\n  state             PC1    PC2    PC3     PC4\n  &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 South Carolina -1.31  -1.91   0.298 -0.130 \n2 South Dakota    1.97  -0.815 -0.385 -0.108 \n3 Tennessee      -0.990 -0.852 -0.186  0.646 \n4 Texas          -1.34   0.408  0.487  0.637 \n5 Utah            0.545  1.46  -0.291 -0.0815\n6 Vermont         2.77  -1.39  -0.833 -0.143 \n\n\nWe can get back the same information as we could for prcomp() but we have to specify the slightly different inside tidy(). Here id = \"pca\" refers to the second step of pca_rec. We get the scores with type = \"coef\".\n\ntidy(pca_rec, id = \"pca\", type = \"coef\")\n\n# A tibble: 16 × 4\n   terms      value component id   \n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n 1 Murder   -0.536  PC1       pca  \n 2 Assault  -0.583  PC1       pca  \n 3 UrbanPop -0.278  PC1       pca  \n 4 Rape     -0.543  PC1       pca  \n 5 Murder   -0.418  PC2       pca  \n 6 Assault  -0.188  PC2       pca  \n 7 UrbanPop  0.873  PC2       pca  \n 8 Rape      0.167  PC2       pca  \n 9 Murder    0.341  PC3       pca  \n10 Assault   0.268  PC3       pca  \n11 UrbanPop  0.378  PC3       pca  \n12 Rape     -0.818  PC3       pca  \n13 Murder    0.649  PC4       pca  \n14 Assault  -0.743  PC4       pca  \n15 UrbanPop  0.134  PC4       pca  \n16 Rape      0.0890 PC4       pca  \n\n\nAnd the eigenvalues with type = \"variance\".\n\ntidy(pca_rec, id = \"pca\", type = \"variance\")\n\n# A tibble: 16 × 4\n   terms                         value component id   \n   &lt;chr&gt;                         &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;\n 1 variance                      2.48          1 pca  \n 2 variance                      0.990         2 pca  \n 3 variance                      0.357         3 pca  \n 4 variance                      0.173         4 pca  \n 5 cumulative variance           2.48          1 pca  \n 6 cumulative variance           3.47          2 pca  \n 7 cumulative variance           3.83          3 pca  \n 8 cumulative variance           4             4 pca  \n 9 percent variance             62.0           1 pca  \n10 percent variance             24.7           2 pca  \n11 percent variance              8.91          3 pca  \n12 percent variance              4.34          4 pca  \n13 cumulative percent variance  62.0           1 pca  \n14 cumulative percent variance  86.8           2 pca  \n15 cumulative percent variance  95.7           3 pca  \n16 cumulative percent variance 100             4 pca  \n\n\nSometimes you don’t want to get back all the principal components of the data. We can either specify how many components we want with num_comp (or rank. in prcomp())\n\nrecipe(~., data = USArrests) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_pca(all_numeric(), num_comp = 3) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 4\n   state           PC1     PC2     PC3\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama     -0.976  -1.12    0.440 \n 2 Alaska      -1.93   -1.06   -2.02  \n 3 Arizona     -1.75    0.738  -0.0542\n 4 Arkansas     0.140  -1.11   -0.113 \n 5 California  -2.50    1.53   -0.593 \n 6 Colorado    -1.50    0.978  -1.08  \n 7 Connecticut  1.34    1.08    0.637 \n 8 Delaware    -0.0472  0.322   0.711 \n 9 Florida     -2.98   -0.0388  0.571 \n10 Georgia     -1.62   -1.27    0.339 \n# ℹ 40 more rows\n\n\nor using a threshold to specify how many components to keep by the variance explained. So by setting threshold = 0.7, step_pca() will generate enough principal components to explain 70% of the variance.\n\nrecipe(~., data = USArrests) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_pca(all_numeric(), threshold = 0.7) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\n# A tibble: 50 × 3\n   state           PC1     PC2\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama     -0.976  -1.12  \n 2 Alaska      -1.93   -1.06  \n 3 Arizona     -1.75    0.738 \n 4 Arkansas     0.140  -1.11  \n 5 California  -2.50    1.53  \n 6 Colorado    -1.50    0.978 \n 7 Connecticut  1.34    1.08  \n 8 Delaware    -0.0472  0.322 \n 9 Florida     -2.98   -0.0388\n10 Georgia     -1.62   -1.27  \n# ℹ 40 more rows"
  },
  {
    "objectID": "12-unsupervised-learning.html#matrix-completion",
    "href": "12-unsupervised-learning.html#matrix-completion",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.2 Matrix Completion",
    "text": "12.2 Matrix Completion\nThis section is WIP."
  },
  {
    "objectID": "12-unsupervised-learning.html#kmeans-clustering",
    "href": "12-unsupervised-learning.html#kmeans-clustering",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.3 Kmeans Clustering",
    "text": "12.3 Kmeans Clustering\nWe will be using the tidyclust package to perform these clustering tasks. It was a similar interface to parsnip, and it interfaces well with the rest of tidymodels.\nBefore we get going let us create a synthetic data set that we know has groups.\n\nset.seed(2)\n\nx_df &lt;- tibble(\n  V1 = rnorm(n = 50, mean = rep(c(0, 3), each = 25)),\n  V2 = rnorm(n = 50, mean = rep(c(0, -4), each = 25))\n)\n\nAnd we can plot it with ggplot2 to see that the groups are really there. Note that we didn’t include this grouping information in x_df as we are trying to emulate a situation where we don’t know of the possible underlying clusters.\n\nx_df %&gt;%\n  ggplot(aes(V1, V2, color = rep(c(\"A\", \"B\"), each = 25))) +\n  geom_point() +\n  labs(color = \"groups\")\n\n\n\n\nNow that we have the data, it is time to create a cluster specification. Since we want to perform K-means clustering, we will use the k_means() function from tidyclust. We use the num_clusters argument to specify how many centroids the K-means algorithm need to use. We also set a mode and engine, which this time are set to the same as the defaults. We also set nstart = 20, this allows the algorithm to have multiple initial starting positions, which we use in the hope of finding global maxima instead of local maxima.\n\nkmeans_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_mode(\"partition\") %&gt;%\n  set_engine(\"stats\") %&gt;%\n  set_args(nstart = 20)\n\nkmeans_spec\n\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = 3\n\nEngine-Specific Arguments:\n  nstart = 20\n\nComputational engine: stats \n\n\nOnce we have this specification we can fit it to our data. We remember to set a seed because the K-means algorithm starts with random initialization\n\nset.seed(1234)\nkmeans_fit &lt;- kmeans_spec %&gt;%\n  fit(~., data = x_df)\n\nThis fitted model has a lot of different kinds of information.\n\nkmeans_fit\n\ntidyclust cluster object\n\nK-means clustering with 3 clusters of sizes 11, 23, 16\n\nCluster means:\n         V1          V2\n1 2.5355362 -2.48605364\n2 0.2339095  0.04414551\n3 2.8241300 -5.01221675\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  3 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n 1  1  1  3  1  3  3  3  3  1  3  3  3  1  1  1  3  3  3  3  1  3  3  3 \n\nWithin cluster sum of squares by cluster:\n[1] 14.56698 54.84869 26.98215\n (between_SS / total_SS =  76.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nAn otherall function to inspect your fitted tidyclust models is extract_fit_summary() which returns all different kind of information\n\nextract_fit_summary(kmeans_fit)\n\n$cluster_names\n[1] Cluster_1 Cluster_2 Cluster_3\nLevels: Cluster_1 Cluster_2 Cluster_3\n\n$centroids\n# A tibble: 3 × 2\n     V1      V2\n  &lt;dbl&gt;   &lt;dbl&gt;\n1 0.234  0.0441\n2 2.54  -2.49  \n3 2.82  -5.01  \n\n$n_members\n[1] 23 11 16\n\n$sse_within_total_total\n[1] 54.84869 14.56698 26.98215\n\n$sse_total\n[1] 415.9045\n\n$orig_labels\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 3 1 1 1 3 1 3 3 3 3 1 3 3\n[39] 3 1 1 1 3 3 3 3 1 3 3 3\n\n$cluster_assignments\n [1] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [8] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[15] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_2\n[22] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_3 Cluster_2 Cluster_2\n[29] Cluster_2 Cluster_3 Cluster_2 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[36] Cluster_2 Cluster_3 Cluster_3 Cluster_3 Cluster_2 Cluster_2 Cluster_2\n[43] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_2 Cluster_3 Cluster_3\n[50] Cluster_3\nLevels: Cluster_1 Cluster_2 Cluster_3\n\n\nWe can also extract some of these quantities directly using extract_centroids()\n\nextract_centroids(kmeans_fit)\n\n# A tibble: 3 × 3\n  .cluster     V1      V2\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Cluster_1 0.234  0.0441\n2 Cluster_2 2.54  -2.49  \n3 Cluster_3 2.82  -5.01  \n\n\nand extract_cluster_assignment()\n\nextract_cluster_assignment(kmeans_fit)\n\n# A tibble: 50 × 1\n   .cluster \n   &lt;fct&gt;    \n 1 Cluster_1\n 2 Cluster_1\n 3 Cluster_1\n 4 Cluster_1\n 5 Cluster_1\n 6 Cluster_1\n 7 Cluster_1\n 8 Cluster_1\n 9 Cluster_1\n10 Cluster_1\n# ℹ 40 more rows\n\n\nprediction in a clustering model isn’t well defined. But we can think of it as “what cluster would these observations be in if they were part of the data set”. For the k-means case, it looks at which centroid these observations are closest to.\n\npredict(kmeans_fit, new_data = x_df)\n\n# A tibble: 50 × 1\n   .pred_cluster\n   &lt;fct&gt;        \n 1 Cluster_1    \n 2 Cluster_1    \n 3 Cluster_1    \n 4 Cluster_1    \n 5 Cluster_1    \n 6 Cluster_1    \n 7 Cluster_1    \n 8 Cluster_1    \n 9 Cluster_1    \n10 Cluster_1    \n# ℹ 40 more rows\n\n\nLastly, we can see what cluster each observation belongs to by using augment(), which does the same thing as predict() but add it to the orginial data set. This makes it handy for EDA and plotting the results.\n\naugment(kmeans_fit, new_data = x_df)\n\n# A tibble: 50 × 3\n        V1     V2 .pred_cluster\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;        \n 1 -0.897  -0.838 Cluster_1    \n 2  0.185   2.07  Cluster_1    \n 3  1.59   -0.562 Cluster_1    \n 4 -1.13    1.28  Cluster_1    \n 5 -0.0803 -1.05  Cluster_1    \n 6  0.132  -1.97  Cluster_1    \n 7  0.708  -0.323 Cluster_1    \n 8 -0.240   0.936 Cluster_1    \n 9  1.98    1.14  Cluster_1    \n10 -0.139   1.67  Cluster_1    \n# ℹ 40 more rows\n\n\nWe can visualize the result of augment() to see how well the clustering performed.\n\naugment(kmeans_fit, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()\n\n\n\n\nThis is all well and good, but it would be nice if we could try out a number of different clusters and then find the best one. For this we will use tune_cluster(). tune_cluster() works pretty much like tune_grid() expect that it works with cluster models.\n\nkmeans_spec_tuned &lt;- kmeans_spec %&gt;% \n  set_args(num_clusters = tune())\n\nkmeans_wf &lt;- workflow() %&gt;%\n  add_model(kmeans_spec_tuned) %&gt;%\n  add_formula(~.)\n\nnow we can use this workflow with tune_cluster() to fit it many times for different values of num_clusters.\n\nset.seed(1234)\nx_boots &lt;- bootstraps(x_df, times = 10)\n\nnum_clusters_grid &lt;- tibble(num_clusters = seq(1, 10))\n\ntune_res &lt;- tune_cluster(\n  object = kmeans_wf,\n  resamples = x_boots,\n  grid = num_clusters_grid\n)\n\nAnd we can use collect_metrics() as before\n\ntune_res %&gt;%\n  collect_metrics()\n\n# A tibble: 20 × 7\n   num_clusters .metric          .estimator  mean     n std_err .config         \n          &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1            1 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 2            1 sse_within_total standard   387.     10   8.86  Preprocessor1_M…\n 3            2 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 4            2 sse_within_total standard   121.     10   4.00  Preprocessor1_M…\n 5            3 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 6            3 sse_within_total standard    82.6    10   2.29  Preprocessor1_M…\n 7            4 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 8            4 sse_within_total standard    59.0    10   2.07  Preprocessor1_M…\n 9            5 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n10            5 sse_within_total standard    43.8    10   1.99  Preprocessor1_M…\n11            6 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n12            6 sse_within_total standard    33.3    10   1.48  Preprocessor1_M…\n13            7 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n14            7 sse_within_total standard    25.0    10   1.25  Preprocessor1_M…\n15            8 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n16            8 sse_within_total standard    20.8    10   1.10  Preprocessor1_M…\n17            9 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n18            9 sse_within_total standard    17.0    10   1.00  Preprocessor1_M…\n19           10 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n20           10 sse_within_total standard    13.9    10   0.695 Preprocessor1_M…\n\n\nNow that we have the total within-cluster sum-of-squares we can plot them against k so we can use the elbow method to find the optimal number of clusters. This actually pops right out if we use autoplot() on the results.\n\ntune_res %&gt;%\n  autoplot()\n\n\n\n\nWe see an elbow when the number of clusters is equal to 2 which makes us happy since the data set is specifically created to have 2 clusters. We can now construct the final kmeans model\n\nfinal_kmeans &lt;- kmeans_wf %&gt;%\n  update_model(kmeans_spec %&gt;% set_args(num_clusters = 2)) %&gt;%\n  fit(x_df)\n\nAnd we can finish by visualizing the clusters it found.\n\naugment(final_kmeans, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()"
  },
  {
    "objectID": "12-unsupervised-learning.html#hierarchical-clustering",
    "href": "12-unsupervised-learning.html#hierarchical-clustering",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.4 Hierarchical Clustering",
    "text": "12.4 Hierarchical Clustering\nThe hclust() function is one way to perform hierarchical clustering in R. It only needs one input and that is a dissimilarity structure as produced by dist(). Furthermore, we can specify a couple of things,\nWe will use the hier_clust() function from tidyclust to perform hierarchical clustering. We will keep all the defaults except for the agglomeration method. Let us cluster this data in a couple of different ways to see how the choice of agglomeration method changes the clustering.\n\nres_hclust_complete &lt;- hier_clust(linkage_method = \"complete\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_average &lt;- hier_clust(linkage_method = \"average\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_single &lt;- hier_clust(linkage_method = \"single\") %&gt;%\n  fit(~., data = x_df)\n\nThe factoextra package provides functions (fviz_dend()) to visualize the clustering created using hclust(). We use fviz_dend() to show the dendrogram. We need to use the extract_fit_engine() to extract the underlying model object that fviz_dend() expects.\n\nres_hclust_complete %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"complete\", k = 2)\n\n\n\n\n\nres_hclust_average %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"average\", k = 2)\n\n\n\n\n\nres_hclust_single %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"single\", k = 2)\n\n\n\n\nIf we don’t know the importance of the different predictors in data set it could be beneficial to scale the data such that each variable has the same influence. We will use a recipe and workflow to do this.\n\nhier_rec &lt;- recipe(~., data = x_df) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nhier_wf &lt;- workflow() %&gt;%\n  add_recipe(hier_rec) %&gt;%\n  add_model(hier_clust(linkage_method = \"complete\"))\n\nhier_fit &lt;- hier_wf %&gt;%\n  fit(data = x_df) \n\nhier_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(k = 2)"
  },
  {
    "objectID": "12-unsupervised-learning.html#pca-on-the-nci60-data",
    "href": "12-unsupervised-learning.html#pca-on-the-nci60-data",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.5 PCA on the NCI60 Data",
    "text": "12.5 PCA on the NCI60 Data\nWe will now explore the NCI60 data set. It is genomic data set, containing cancer cell line microarray data, which consists of 6830 gene expression measurements on 64 cancer cell lines. The data comes as a list containing a matrix and its labels. We do a little work to turn the data into a tibble we will use for the rest of the chapter.\n\ndata(NCI60, package = \"ISLR\")\nnci60 &lt;- NCI60$data %&gt;%\n  as_tibble(.name_repair = ~ paste0(\"V_\", .x)) %&gt;%\n  mutate(label = factor(NCI60$labs)) %&gt;%\n  relocate(label)\n\nWe do not expect to use the label variable doing the analysis since we are emulating an unsupervised analysis. Since we are an exploratory task we will be fine with using prcomp() since we don’t need to apply these transformations to anything else. We remove label and remember to set scale = TRUE to perform scaling of all the variables.\n\nnci60_pca &lt;- nci60 %&gt;%\n  select(-label) %&gt;%\n  prcomp(scale = TRUE)\n\nFor visualization purposes, we will now join up the labels into the result of augment(nci60_pca) so we can visualize how close similar labeled points are to each other.\n\nnci60_pcs &lt;- bind_cols(\n  augment(nci60_pca),\n  nci60 %&gt;% select(label)\n)\n\nWe have 14 different labels, so we will make use of the \"Polychrome 36\" palette to help us better differentiate between the labels.\n\ncolors &lt;- unname(palette.colors(n = 14, palette = \"Polychrome 36\"))\n\nOr we can plot the different PCs against each other. It is a good idea to compare the first PCs against each other since they carry the most information. We will just compare the pairs 1-2 and 1-3 but you can do more yourself. It tends to be a good idea to stop once interesting things appear in the plots.\n\nnci60_pcs %&gt;%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n\n\n\n\nWe see there is some local clustering of the different cancer types which is promising, it is not perfect but let us see what happens when we compare PC1 against PC3 now.\n\nnci60_pcs %&gt;%\n  ggplot(aes(.fittedPC1, .fittedPC3, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n\n\n\n\nLastly, we will plot the variance explained of each principal component. We can use tidy() with matrix = \"eigenvalues\" to accomplish this easily, so we start with the percentage of each PC\n\ntidy(nci60_pca, matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, percent)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 60, by = 5)) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\nwith the first PC having a little more than 10% and a fairly fast drop.\nAnd we can get the cumulative variance explained just the same.\n\ntidy(nci60_pca, matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, cumulative)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "12-unsupervised-learning.html#clustering-on-nci60-dataset",
    "href": "12-unsupervised-learning.html#clustering-on-nci60-dataset",
    "title": "\n12  Unsupervised Learning\n",
    "section": "\n12.6 Clustering on nci60 dataset",
    "text": "12.6 Clustering on nci60 dataset\nLet us now see what happens if we perform clustering on the nci60 data set. Before we start it would be good if we create a scaled version of this data set. We can use the recipes package to perform those transformations. And a workflow to be able to combine it with the cluster model later\n\nnci60_rec &lt;- recipe(~ ., data = nci60) %&gt;%\n  step_rm(label) %&gt;%\n  step_normalize(all_predictors())\n\nnci60_wf &lt;- workflow() %&gt;%\n  add_recipe(nci60_rec)\n\nNow we start by fitting multiple hierarchical clustering models using different agglomeration methods.\n\nnci60_complete &lt;- nci60_wf %&gt;%\n  add_model(hier_clust(linkage_method = \"complete\")) %&gt;%\n  fit(data = nci60)\n\nnci60_average &lt;- nci60_wf %&gt;%\n  add_model(hier_clust(linkage_method = \"average\")) %&gt;%\n  fit(data = nci60)\n\nnci60_single &lt;- nci60_wf %&gt;%\n  add_model(hier_clust(linkage_method = \"single\")) %&gt;%\n  fit(data = nci60)\n\nWe then visualize them to see if any of them have some good natural separations.\n\nnci60_complete %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"Complete\")\n\n\n\n\n\nnci60_average %&gt;% \n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"Average\")\n\n\n\n\n\nnci60_single %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"Single\")\n\n\n\n\nWe now color according to k = 4 and we get the following separations.\n\nnci60_complete %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(k = 4, main = \"hclust(complete) on nci60\")\n\n\n\n\nWe now take find the clustering and calculate which label is the most common one within each cluster.\n\npredict(nci60_complete, new_data = nci60, num_clusters = 4) %&gt;%\n  mutate(label = nci60$label) %&gt;%\n  count(label, .pred_cluster) %&gt;%\n  group_by(.pred_cluster) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  slice_max(n = 1, order_by = prop) %&gt;%\n  ungroup()\n\n# A tibble: 4 × 4\n  label    .pred_cluster     n  prop\n  &lt;fct&gt;    &lt;fct&gt;         &lt;int&gt; &lt;dbl&gt;\n1 MELANOMA Cluster_1         5 0.357\n2 RENAL    Cluster_2         7 0.333\n3 LEUKEMIA Cluster_3         5 0.714\n4 COLON    Cluster_4         7 0.318\n\n\nWe can also see what happens if we try to fit a K-means clustering. We liked 4 clusters from earlier so let’s stick with that.\n\nset.seed(2)\nnci60_kmeans &lt;- nci60_wf %&gt;%\n  add_model(k_means(num_clusters = 4)) %&gt;%\n  fit(data = nci60)\n\nand we can now extract the centroids\n\nnci60_kmeans %&gt;%\n  extract_centroids()\n\n# A tibble: 4 × 6,831\n  .cluster      V_1     V_2     V_3     V_4     V_5     V_6    V_7     V_8\n  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Cluster_1 -0.281  -0.675   0.120  -0.0190  0.0763 -0.260  -0.193  0.0185\n2 Cluster_2 -0.371  -0.0689 -0.0633  0.0788 -0.359  -0.0672 -0.209 -0.124 \n3 Cluster_3  0.325   0.266  -0.0439  0.0210  0.126  -0.0187  0.326  0.218 \n4 Cluster_4  0.0205 -0.0821  0.164  -0.215   0.298   0.431  -0.401 -0.432 \n# ℹ 6,822 more variables: V_9 &lt;dbl&gt;, V_10 &lt;dbl&gt;, V_11 &lt;dbl&gt;, V_12 &lt;dbl&gt;,\n#   V_13 &lt;dbl&gt;, V_14 &lt;dbl&gt;, V_15 &lt;dbl&gt;, V_16 &lt;dbl&gt;, V_17 &lt;dbl&gt;, V_18 &lt;dbl&gt;,\n#   V_19 &lt;dbl&gt;, V_20 &lt;dbl&gt;, V_21 &lt;dbl&gt;, V_22 &lt;dbl&gt;, V_23 &lt;dbl&gt;, V_24 &lt;dbl&gt;,\n#   V_25 &lt;dbl&gt;, V_26 &lt;dbl&gt;, V_27 &lt;dbl&gt;, V_28 &lt;dbl&gt;, V_29 &lt;dbl&gt;, V_30 &lt;dbl&gt;,\n#   V_31 &lt;dbl&gt;, V_32 &lt;dbl&gt;, V_33 &lt;dbl&gt;, V_34 &lt;dbl&gt;, V_35 &lt;dbl&gt;, V_36 &lt;dbl&gt;,\n#   V_37 &lt;dbl&gt;, V_38 &lt;dbl&gt;, V_39 &lt;dbl&gt;, V_40 &lt;dbl&gt;, V_41 &lt;dbl&gt;, V_42 &lt;dbl&gt;,\n#   V_43 &lt;dbl&gt;, V_44 &lt;dbl&gt;, V_45 &lt;dbl&gt;, V_46 &lt;dbl&gt;, V_47 &lt;dbl&gt;, V_48 &lt;dbl&gt;, …\n\n\nand the cluster assignments\n\nnci60_kmeans %&gt;%\n  extract_cluster_assignment(nci60_kmeans)\n\n# A tibble: 64 × 1\n   .cluster \n   &lt;fct&gt;    \n 1 Cluster_1\n 2 Cluster_1\n 3 Cluster_1\n 4 Cluster_2\n 5 Cluster_2\n 6 Cluster_2\n 7 Cluster_2\n 8 Cluster_2\n 9 Cluster_1\n10 Cluster_1\n# ℹ 54 more rows\n\n\nLastly, let us see how the two different methods we used compare against each other. Let us save the cluster ids in cluster_kmeans and cluster_hclust and then use conf_mat() in a different way to quickly generate a heatmap between the two methods.\n\ncluster_kmeans &lt;- predict(nci60_kmeans, nci60)\ncluster_hclust &lt;- predict(nci60_complete, nci60, num_clusters = 4)\n\ntibble(\n  kmeans = cluster_kmeans$.pred_cluster,\n  hclust = cluster_hclust$.pred_cluster\n) %&gt;%\n  conf_mat(kmeans, hclust) %&gt;%\n  autoplot(type = \"heatmap\")\n\n\n\n\nThere is not a lot of agreement between labels which makes sense, since the labels themselves are arbitrarily added. What is important is that they tend to agree quite a lot (the confusion matrix is sparse).\nOne last thing is that it is sometimes useful to perform dimensionality reduction before using the clustering method. Let us use the recipes package to calculate the PCA of nci60 and keep the 5 first components\n\nnci60_pca_rec &lt;- recipe(~ ., data = nci60) %&gt;%\n  step_rm(label) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), num_comp = 5)\n\nnci60_pca_wf &lt;- workflow() %&gt;%\n  add_recipe(nci60_pca_rec)\n\nand now fit this new workflow\n\nnci60_pca &lt;- nci60_pca_wf %&gt;%\n  add_model(hier_clust(linkage_method = \"complete\")) %&gt;%\n  fit(data = nci60)\n\nwe can now visualize on this reduced data set, and sometimes we get quite good results since the clustering method doesn’t have to work in high dimensions.\n\nnci60_pca %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(k = 4, main = \"hclust on first five PCs\")"
  },
  {
    "objectID": "13-multiple-testing.html",
    "href": "13-multiple-testing.html",
    "title": "\n13  Multiple Testing\n",
    "section": "",
    "text": "This chapter is WIP. If you would like something specific in this chapter please open an issue.\nThis section is being worked on to include code using the infer package."
  }
]