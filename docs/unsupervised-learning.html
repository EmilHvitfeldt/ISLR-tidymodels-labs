<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Unsupervised Learning | ISLR tidymodels Labs</title>
<meta name="author" content="Emil Hvitfeldt">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.8/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">ISLR tidymodels Labs</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="statistical-learning.html"><span class="header-section-number">2</span> Statistical learning</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">3</span> Linear Regression</a></li>
<li><a class="" href="classification.html"><span class="header-section-number">4</span> Classification</a></li>
<li><a class="" href="resampling-methods.html"><span class="header-section-number">5</span> Resampling Methods</a></li>
<li><a class="" href="linear-model-selection-and-regularization.html"><span class="header-section-number">6</span> Linear Model Selection and Regularization</a></li>
<li><a class="" href="moving-beyond-linearity.html"><span class="header-section-number">7</span> Moving Beyond Linearity</a></li>
<li><a class="" href="tree-based-methods.html"><span class="header-section-number">8</span> Tree-Based Methods</a></li>
<li><a class="" href="support-vector-machines.html"><span class="header-section-number">9</span> Support Vector Machines</a></li>
<li><a class="active" href="unsupervised-learning.html"><span class="header-section-number">10</span> Unsupervised Learning</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="unsupervised-learning" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Unsupervised Learning<a class="anchor" aria-label="anchor" href="#unsupervised-learning"><i class="fas fa-link"></i></a>
</h1>
<p>This final chapter talks about unsupervised learning. This is broken into two parts. Dimensionality reduction and clustering. One downside at this moment is that clustering is not well integrated into tidymodels at this time. But we are still able to use some of the features in tidymodels.</p>
<div class="sourceCode" id="cb327"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org">magrittr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">proxy</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.StatLearning.com">ISLR</a></span><span class="op">)</span></code></pre></div>
<div id="principal-components-analysis" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Principal Components Analysis<a class="anchor" aria-label="anchor" href="#principal-components-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>This section will be used to explore the <code>USArrests</code> data set using PCA. Before we move on, let is turn <code>USArrests</code> into a tibble and move the rownames into a column.</p>
<div class="sourceCode" id="cb328"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">USArrests</span> <span class="op">&lt;-</span> <span class="fu">as_tibble</span><span class="op">(</span><span class="va">USArrests</span>, rownames <span class="op">=</span> <span class="st">"state"</span><span class="op">)</span>
<span class="va">USArrests</span></code></pre></div>
<pre><code>## # A tibble: 50 x 5
##    state       Murder Assault UrbanPop  Rape
##    &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;
##  1 Alabama       13.2     236       58  21.2
##  2 Alaska        10       263       48  44.5
##  3 Arizona        8.1     294       80  31  
##  4 Arkansas       8.8     190       50  19.5
##  5 California     9       276       91  40.6
##  6 Colorado       7.9     204       78  38.7
##  7 Connecticut    3.3     110       77  11.1
##  8 Delaware       5.9     238       72  15.8
##  9 Florida       15.4     335       80  31.9
## 10 Georgia       17.4     211       60  25.8
## # … with 40 more rows</code></pre>
<p>Notice how the mean of each of the variables is quite different. if we were to apply PCA directly to the data set then <code>Murder</code> would have a very small influence.</p>
<div class="sourceCode" id="cb330"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">USArrests</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">state</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">map_dfr</span><span class="op">(</span><span class="va">mean</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   Murder Assault UrbanPop  Rape
##    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1   7.79    171.     65.5  21.2</code></pre>
<p>We will show how to perform PCA in two different ways in this section. First, by using <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> directly, using <code>broom</code> to extract the information we need, and secondly by using recipes.
<code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> Takes 1 required argument <code>x</code> which much be a fully numeric data.frame or matrix. Then we pass that to <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code>. We also set <code>scale = TRUE</code> in <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> which will perform the scaling we need.</p>
<div class="sourceCode" id="cb332"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">USArrests_pca</span> <span class="op">&lt;-</span> <span class="va">USArrests</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">state</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">USArrests_pca</span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 1.5748783 0.9948694 0.5971291 0.4164494
## 
## Rotation (n x k) = (4 x 4):
##                 PC1        PC2        PC3         PC4
## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<p>now we can use our favorite broom function to extract information from this <code>prcomp</code> object.
We start with <code>tidy()</code>. <code>tidy()</code> can be used to extract a couple of different things, see <code>?broom:::tidy.prcomp()</code> for more information. <code>tidy()</code> will by default extract the scores of a PCA object in long tidy format. The score of is the location of the observation in PCA space. So we can</p>
<div class="sourceCode" id="cb334"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 200 x 3
##      row    PC  value
##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1     1     1 -0.976
##  2     1     2  1.12 
##  3     1     3 -0.440
##  4     1     4  0.155
##  5     2     1 -1.93 
##  6     2     2  1.06 
##  7     2     3  2.02 
##  8     2     4 -0.434
##  9     3     1 -1.75 
## 10     3     2 -0.738
## # … with 190 more rows</code></pre>
<p>We can also explicitly say we want the scores by setting <code>matrix = "scores"</code>.</p>
<div class="sourceCode" id="cb336"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span>, matrix <span class="op">=</span> <span class="st">"scores"</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 200 x 3
##      row    PC  value
##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1     1     1 -0.976
##  2     1     2  1.12 
##  3     1     3 -0.440
##  4     1     4  0.155
##  5     2     1 -1.93 
##  6     2     2  1.06 
##  7     2     3  2.02 
##  8     2     4 -0.434
##  9     3     1 -1.75 
## 10     3     2 -0.738
## # … with 190 more rows</code></pre>
<p>Next, we can get the loadings of the PCA.</p>
<div class="sourceCode" id="cb338"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span>, matrix <span class="op">=</span> <span class="st">"loadings"</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 16 x 3
##    column      PC   value
##    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;
##  1 Murder       1 -0.536 
##  2 Murder       2  0.418 
##  3 Murder       3 -0.341 
##  4 Murder       4  0.649 
##  5 Assault      1 -0.583 
##  6 Assault      2  0.188 
##  7 Assault      3 -0.268 
##  8 Assault      4 -0.743 
##  9 UrbanPop     1 -0.278 
## 10 UrbanPop     2 -0.873 
## 11 UrbanPop     3 -0.378 
## 12 UrbanPop     4  0.134 
## 13 Rape         1 -0.543 
## 14 Rape         2 -0.167 
## 15 Rape         3  0.818 
## 16 Rape         4  0.0890</code></pre>
<p>This information tells us how each variable contributes to each principal component. If you don’t have too many principal components you can visualize the contribution without filtering</p>
<div class="sourceCode" id="cb340"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span>, matrix <span class="op">=</span> <span class="st">"loadings"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">value</span>, <span class="va">column</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span> <span class="va">PC</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-8-1.png" width="672"></div>
<p>Lastly, we can set <code>matrix = "eigenvalues"</code> and get back the explained standard deviation for each PC including as a percent and cumulative which is quite handy for plotting.</p>
<div class="sourceCode" id="cb341"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span>, matrix <span class="op">=</span> <span class="st">"eigenvalues"</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 4
##      PC std.dev percent cumulative
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1     1   1.57   0.620       0.620
## 2     2   0.995  0.247       0.868
## 3     3   0.597  0.0891      0.957
## 4     4   0.416  0.0434      1</code></pre>
<p>If we want to see how the percent standard deviation explained drops off for each PC we can easily get that by using <code>tidy()</code> with <code>matrix = "eigenvalues"</code>.</p>
<div class="sourceCode" id="cb343"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">USArrests_pca</span>, matrix <span class="op">=</span> <span class="st">"eigenvalues"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">PC</span>, <span class="va">percent</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-10-1.png" width="672"></div>
<p>Lastly, we have the <code>augment()</code> function which will give you back the fitted PC transformation if you apply it to the <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> object directly</p>
<div class="sourceCode" id="cb344"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">USArrests_pca</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 x 5
##    .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4
##    &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1 1            -0.976      1.12      -0.440     0.155  
##  2 2            -1.93       1.06       2.02     -0.434  
##  3 3            -1.75      -0.738      0.0542   -0.826  
##  4 4             0.140      1.11       0.113    -0.181  
##  5 5            -2.50      -1.53       0.593    -0.339  
##  6 6            -1.50      -0.978      1.08      0.00145
##  7 7             1.34      -1.08      -0.637    -0.117  
##  8 8            -0.0472    -0.322     -0.711    -0.873  
##  9 9            -2.98       0.0388    -0.571    -0.0953 
## 10 10           -1.62       1.27      -0.339     1.07   
## # … with 40 more rows</code></pre>
<p>and will apply this transformation to new data by passing the new data to <code>newdata</code></p>
<div class="sourceCode" id="cb346"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">USArrests_pca</span>, newdata <span class="op">=</span> <span class="va">USArrests</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 10
##   .rownames state Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3
##   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 1         Alab…   13.2     236       58  21.2     -0.976      1.12     -0.440 
## 2 2         Alas…   10       263       48  44.5     -1.93       1.06      2.02  
## 3 3         Ariz…    8.1     294       80  31       -1.75      -0.738     0.0542
## 4 4         Arka…    8.8     190       50  19.5      0.140      1.11      0.113 
## 5 5         Cali…    9       276       91  40.6     -2.50      -1.53      0.593 
## # … with 1 more variable: .fittedPC4 &lt;dbl&gt;</code></pre>
<p>If you are using PCA as a preprocessing method I recommend you use recipes to apply the PCA transformation. This is a good way of doing it since recipe will correctly apply the same transformation to new data that the recipe is used on.</p>
<p>We <code>step_normalize()</code> to make sure all the variables are on the same scale. By using <code>all_numeric()</code> we are able to apply PCA on the variables we want without having to remove <code>state</code>. We are also setting an <code>id</code> for <code>step_pca()</code> to make it easier to <code>tidy()</code> later.</p>
<div class="sourceCode" id="cb348"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pca_rec</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">USArrests</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_pca</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span>, id <span class="op">=</span> <span class="st">"pca"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>By calling <code>bake(new_data = NULL)</code> we can get the fitted PC transformation of our numerical variables</p>
<div class="sourceCode" id="cb349"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pca_rec</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 x 5
##    state           PC1     PC2     PC3      PC4
##    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1 Alabama     -0.976   1.12   -0.440   0.155  
##  2 Alaska      -1.93    1.06    2.02   -0.434  
##  3 Arizona     -1.75   -0.738   0.0542 -0.826  
##  4 Arkansas     0.140   1.11    0.113  -0.181  
##  5 California  -2.50   -1.53    0.593  -0.339  
##  6 Colorado    -1.50   -0.978   1.08    0.00145
##  7 Connecticut  1.34   -1.08   -0.637  -0.117  
##  8 Delaware    -0.0472 -0.322  -0.711  -0.873  
##  9 Florida     -2.98    0.0388 -0.571  -0.0953 
## 10 Georgia     -1.62    1.27   -0.339   1.07   
## # … with 40 more rows</code></pre>
<p>but we can also supply our own data to <code>new_data</code>.</p>
<div class="sourceCode" id="cb351"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pca_rec</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="va">USArrests</span><span class="op">[</span><span class="fl">40</span><span class="op">:</span><span class="fl">45</span>, <span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##   state             PC1    PC2    PC3     PC4
##   &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1 South Carolina -1.31   1.91  -0.298 -0.130 
## 2 South Dakota    1.97   0.815  0.385 -0.108 
## 3 Tennessee      -0.990  0.852  0.186  0.646 
## 4 Texas          -1.34  -0.408 -0.487  0.637 
## 5 Utah            0.545 -1.46   0.291 -0.0815
## 6 Vermont         2.77   1.39   0.833 -0.143</code></pre>
<p>We can get back the same information as we could for <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> but we have to specify the slightly different inside <code>tidy()</code>. Here <code>id = "pca"</code> refers to the second step of <code>pca_rec</code>. We get the <code>scores</code> with <code>type = "coef"</code></p>
<div class="sourceCode" id="cb353"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">pca_rec</span>, id <span class="op">=</span> <span class="st">"pca"</span>, type <span class="op">=</span> <span class="st">"coef"</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 16 x 4
##    terms      value component id   
##    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;
##  1 Murder   -0.536  PC1       pca  
##  2 Assault  -0.583  PC1       pca  
##  3 UrbanPop -0.278  PC1       pca  
##  4 Rape     -0.543  PC1       pca  
##  5 Murder    0.418  PC2       pca  
##  6 Assault   0.188  PC2       pca  
##  7 UrbanPop -0.873  PC2       pca  
##  8 Rape     -0.167  PC2       pca  
##  9 Murder   -0.341  PC3       pca  
## 10 Assault  -0.268  PC3       pca  
## 11 UrbanPop -0.378  PC3       pca  
## 12 Rape      0.818  PC3       pca  
## 13 Murder    0.649  PC4       pca  
## 14 Assault  -0.743  PC4       pca  
## 15 UrbanPop  0.134  PC4       pca  
## 16 Rape      0.0890 PC4       pca</code></pre>
<p>And the eigenvalues with <code>type = "variance"</code>.</p>
<div class="sourceCode" id="cb355"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">pca_rec</span>, id <span class="op">=</span> <span class="st">"pca"</span>, type <span class="op">=</span> <span class="st">"variance"</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 16 x 4
##    terms                         value component id   
##    &lt;chr&gt;                         &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;
##  1 variance                      2.48          1 pca  
##  2 variance                      0.990         2 pca  
##  3 variance                      0.357         3 pca  
##  4 variance                      0.173         4 pca  
##  5 cumulative variance           2.48          1 pca  
##  6 cumulative variance           3.47          2 pca  
##  7 cumulative variance           3.83          3 pca  
##  8 cumulative variance           4             4 pca  
##  9 percent variance             62.0           1 pca  
## 10 percent variance             24.7           2 pca  
## 11 percent variance              8.91          3 pca  
## 12 percent variance              4.34          4 pca  
## 13 cumulative percent variance  62.0           1 pca  
## 14 cumulative percent variance  86.8           2 pca  
## 15 cumulative percent variance  95.7           3 pca  
## 16 cumulative percent variance 100             4 pca</code></pre>
<p>Sometimes you don’t want to get back all the principal components of the data. We can either specify how many components we want with <code>num_comp</code> (or <code>rank.</code> in <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code>)</p>
<div class="sourceCode" id="cb357"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">recipe</span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">USArrests</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_pca</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span>, num_comp <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 x 4
##    state           PC1     PC2     PC3
##    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 Alabama     -0.976   1.12   -0.440 
##  2 Alaska      -1.93    1.06    2.02  
##  3 Arizona     -1.75   -0.738   0.0542
##  4 Arkansas     0.140   1.11    0.113 
##  5 California  -2.50   -1.53    0.593 
##  6 Colorado    -1.50   -0.978   1.08  
##  7 Connecticut  1.34   -1.08   -0.637 
##  8 Delaware    -0.0472 -0.322  -0.711 
##  9 Florida     -2.98    0.0388 -0.571 
## 10 Georgia     -1.62    1.27   -0.339 
## # … with 40 more rows</code></pre>
<p>or using a <code>threshold</code> to specify how many components to keep by the variance explained. So by setting <code>threshold = 0.7</code> <code>step_pca()</code> will generate enough principal components to explain 70% of the variance.</p>
<div class="sourceCode" id="cb359"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">recipe</span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">USArrests</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_pca</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span>, threshold <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 x 3
##    state           PC1     PC2
##    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;
##  1 Alabama     -0.976   1.12  
##  2 Alaska      -1.93    1.06  
##  3 Arizona     -1.75   -0.738 
##  4 Arkansas     0.140   1.11  
##  5 California  -2.50   -1.53  
##  6 Colorado    -1.50   -0.978 
##  7 Connecticut  1.34   -1.08  
##  8 Delaware    -0.0472 -0.322 
##  9 Florida     -2.98    0.0388
## 10 Georgia     -1.62    1.27  
## # … with 40 more rows</code></pre>
</div>
<div id="kmeans-clustering" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Kmeans Clustering<a class="anchor" aria-label="anchor" href="#kmeans-clustering"><i class="fas fa-link"></i></a>
</h2>
<p>The <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function can be used to perform K-means clustering in R. But before we get to that let us create a synthetic data set that we know has groups.</p>
<div class="sourceCode" id="cb361"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>

<span class="va">x_df</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>
  V1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">50</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, each <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span>,
  V2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">50</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="op">-</span><span class="fl">4</span><span class="op">)</span>, each <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>And we can plot it with ggplot2 to see that the groups are really there. Note that we didn’t include this grouping information in <code>x_df</code> as we are trying to emulate a situation where we don’t know of the possible underlying clusters.</p>
<div class="sourceCode" id="cb362"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x_df</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">V1</span>, <span class="va">V2</span>, color <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"A"</span>, <span class="st">"B"</span><span class="op">)</span>, each <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-21-1.png" width="672"></div>
<p>the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> functions takes a matrix or data.frame and <code>centers</code> which is the number of clusters we want <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> to find. We also set <code>nstart = 20</code>, this allows the algorithm to have multiple initial starting positions, which we use in the hope of finding global maxima instead of local maxima.</p>
<div class="sourceCode" id="cb363"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">res_kmeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span><span class="va">x_df</span>, centers <span class="op">=</span> <span class="fl">3</span>, nstart <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></code></pre></div>
<p>This fitted model has a lot of different kinds of information.</p>
<div class="sourceCode" id="cb364"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res_kmeans</span></code></pre></div>
<pre><code>## K-means clustering with 3 clusters of sizes 11, 23, 16
## 
## Cluster means:
##          V1          V2
## 1 2.5355362 -2.48605364
## 2 0.2339095  0.04414551
## 3 2.8241300 -5.01221675
## 
## Clustering vector:
##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 3 1 1 1 3 1 3 3 3 3 1 3 3
## [39] 3 1 1 1 3 3 3 3 1 3 3 3
## 
## Within cluster sum of squares by cluster:
## [1] 14.56698 54.84869 26.98215
##  (between_SS / total_SS =  76.8 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"</code></pre>
<p>And we can use <a href="https://broom.tidymodels.org/">broom</a> functions to extract information in tidy formats. The <code>tidy()</code> function returns information for each cluster, including their position, size and within-cluster sum-of-squares.</p>
<div class="sourceCode" id="cb366"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">res_kmeans</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 5
##      V1      V2  size withinss cluster
##   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;  
## 1 2.54  -2.49      11     14.6 1      
## 2 0.234  0.0441    23     54.8 2      
## 3 2.82  -5.01      16     27.0 3</code></pre>
<p>The <code>glance()</code> function returns model wise metrics. One of these is <code>tot.withinss</code> which is the total within-cluster sum-of-squares that we seek to minimize when we perform K-means clustering.</p>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">glance</span><span class="op">(</span><span class="va">res_kmeans</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   totss tot.withinss betweenss  iter
##   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1  416.         96.4      320.     2</code></pre>
<p>Lastly, we can see what cluster each observation belongs to by using <code>augment()</code> which “predict” which cluster a given observation belongs to.</p>
<div class="sourceCode" id="cb370"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">res_kmeans</span>, data <span class="op">=</span> <span class="va">x_df</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 x 3
##         V1     V2 .cluster
##      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   
##  1 -0.897  -0.838 2       
##  2  0.185   2.07  2       
##  3  1.59   -0.562 2       
##  4 -1.13    1.28  2       
##  5 -0.0803 -1.05  2       
##  6  0.132  -1.97  2       
##  7  0.708  -0.323 2       
##  8 -0.240   0.936 2       
##  9  1.98    1.14  2       
## 10 -0.139   1.67  2       
## # … with 40 more rows</code></pre>
<p>we can visualize the result of <code>augment()</code> to see how well the clustering performed.</p>
<div class="sourceCode" id="cb372"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">res_kmeans</span>, data <span class="op">=</span> <span class="va">x_df</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">V1</span>, <span class="va">V2</span>, color <span class="op">=</span> <span class="va">.cluster</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-27-1.png" width="672"></div>
<p>This is all well and good, but it would be nice if we could try out a number of different clusters and then find the best one. We will use the <code>mutate()</code> and <code>map()</code> combo to fit multiple models and extract information from them. We remember to set a seed to ensure reproducibility.</p>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">multi_kmeans</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>k <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>
    model <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="va">k</span>, <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span><span class="va">x_df</span>, centers <span class="op">=</span> <span class="va">.x</span>, nstart <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span>,
    tot.withinss <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">model</span>, <span class="op">~</span> <span class="fu">glance</span><span class="op">(</span><span class="va">.x</span><span class="op">)</span><span class="op">$</span><span class="va">tot.withinss</span><span class="op">)</span>
  <span class="op">)</span>

<span class="va">multi_kmeans</span></code></pre></div>
<pre><code>## # A tibble: 10 x 3
##        k model    tot.withinss
##    &lt;int&gt; &lt;list&gt;          &lt;dbl&gt;
##  1     1 &lt;kmeans&gt;        416. 
##  2     2 &lt;kmeans&gt;        127. 
##  3     3 &lt;kmeans&gt;         96.4
##  4     4 &lt;kmeans&gt;         73.4
##  5     5 &lt;kmeans&gt;         57.4
##  6     6 &lt;kmeans&gt;         42.4
##  7     7 &lt;kmeans&gt;         32.4
##  8     8 &lt;kmeans&gt;         27.9
##  9     9 &lt;kmeans&gt;         23.5
## 10    10 &lt;kmeans&gt;         20.3</code></pre>
<p>Now that we have the total within-cluster sum-of-squares we can plot them against <code>k</code> so we can use the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">elbow method</a> to find the optimal number of clusters.</p>
<div class="sourceCode" id="cb375"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">multi_kmeans</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">k</span>, <span class="va">tot.withinss</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-29-1.png" width="672"></div>
<p>We see an elbow at <code>k = 2</code> which makes us happy since the data set is specifically created to have 2 clusters. We can now extract the model where <code>k = 2</code> from <code>multi_kmeans</code>.</p>
<div class="sourceCode" id="cb376"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">final_kmeans</span> <span class="op">&lt;-</span> <span class="va">multi_kmeans</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">k</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">pull</span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">pluck</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></code></pre></div>
<p>And we can finish by visualizing the clusters it found.</p>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">final_kmeans</span>, data <span class="op">=</span> <span class="va">x_df</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">V1</span>, <span class="va">V2</span>, color <span class="op">=</span> <span class="va">.cluster</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-31-1.png" width="672"></div>
</div>
<div id="hierarchical-clustering" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Hierarchical Clustering<a class="anchor" aria-label="anchor" href="#hierarchical-clustering"><i class="fas fa-link"></i></a>
</h2>
<p>The <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code> function is one way to perform hierarchical clustering in R. It only needs one input and that is a dissimilarity structure as produced by <code><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist()</a></code>. Furthermore, we can specify a couple of things, including the agglomeration method. Let us cluster this data in a couple of different ways to see how the choice of agglomeration method changes the clustering.</p>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res_hclust_complete</span> <span class="op">&lt;-</span> <span class="va">x_df</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span>

<span class="va">res_hclust_average</span> <span class="op">&lt;-</span> <span class="va">x_df</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"average"</span><span class="op">)</span>

<span class="va">res_hclust_single</span> <span class="op">&lt;-</span> <span class="va">x_df</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"single"</span><span class="op">)</span></code></pre></div>
<p>the <a href="https://rpkgs.datanovia.com/factoextra/index.html">factoextra</a> provides functions (<code><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend()</a></code>) to visualize the clustering created using <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code>. We use <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend()</a></code> to show the dendrogram.</p>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">res_hclust_complete</span>, main <span class="op">=</span> <span class="st">"complete"</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-33-1.png" width="672"></div>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">res_hclust_average</span>, main <span class="op">=</span> <span class="st">"average"</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-33-2.png" width="672"></div>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">res_hclust_single</span>, main <span class="op">=</span> <span class="st">"single"</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-33-3.png" width="672"></div>
<p>If we don’t know the importance of the different predictors in data set it could be beneficial to scale the data such that each variable has the same influence. We can perform scaling by using <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> before <code><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist()</a></code>.</p>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x_df</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="inline-figure">
<img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-34-1.png" width="672">
Another way of calculating distances is based on correlation. This only makes sense if has 3 or more variables.</div>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># correlation based distance</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">30</span> <span class="op">*</span> <span class="fl">3</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>

<span class="va">x</span> <span class="op">%&gt;%</span>
  <span class="fu">proxy</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/proxy/man/dist.html">dist</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"correlation"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-35-1.png" width="672"></div>
</div>
<div id="pca-on-the-nci60-data" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> PCA on the NCI60 Data<a class="anchor" aria-label="anchor" href="#pca-on-the-nci60-data"><i class="fas fa-link"></i></a>
</h2>
<p>We will now explore the <code>NCI60</code> data set. It is genomic data set, containing cancer cell line microarray data, which consists of 6830 gene expression measurements on 64 cancer cell lines. The data comes as a list containing a matrix and its labels. We do a little work to turn the data into a tibble we will use for the rest of the chapter.</p>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">NCI60</span>, package <span class="op">=</span> <span class="st">"ISLR"</span><span class="op">)</span>
<span class="va">nci60</span> <span class="op">&lt;-</span> <span class="va">NCI60</span><span class="op">$</span><span class="va">data</span> <span class="op">%&gt;%</span>
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">set_colnames</a></span><span class="op">(</span><span class="va">.</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"V_"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">NCI60</span><span class="op">$</span><span class="va">labs</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">relocate</span><span class="op">(</span><span class="va">label</span><span class="op">)</span></code></pre></div>
<p>We do not expect to use the <code>label</code> variable doing the analysis since we are emulating an unsupervised analysis. Since we are an exploratory task we will be fine with using <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> since we don’t need to apply these transformations to anything else. We remove <code>label</code> and remember to set <code>scale = TRUE</code> to perform scaling of all the variables.</p>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pca</span> <span class="op">&lt;-</span> <span class="va">nci60</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">label</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p>For visualization purposes, we will now join up the labels into the result of <code>augment(nci60_pca)</code> so we can visualize how close similar labeled points are to each other.</p>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pcs</span> <span class="op">&lt;-</span> <span class="fu">bind_cols</span><span class="op">(</span>
  <span class="fu">augment</span><span class="op">(</span><span class="va">nci60_pca</span><span class="op">)</span>,
  <span class="va">nci60</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="va">label</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We have 14 different labels, so we will make use of the <code>"Polychrome 36"</code> palette to help us better differentiate between the labels.</p>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/palette.html">palette.colors</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">14</span>, palette <span class="op">=</span> <span class="st">"Polychrome 36"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>o we can plot the different PCs against each other. It is a good idea to compare the first PCs against each other since they carry the most information. We will just compare the pairs 1-2 and 1-3 but you can do more yourself. It tends to be a good idea to stop once interesting things appear in the plots.</p>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pcs</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">.fittedPC1</span>, <span class="va">.fittedPC2</span>, color <span class="op">=</span> <span class="va">label</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="va">colors</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-40-1.png" width="672"></div>
<p>We see there is some local clustering of the different cancer types which is promising, it is not perfect but let us see what happens when we compare PC1 against PC3 now.</p>
<div class="sourceCode" id="cb389"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pcs</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">.fittedPC1</span>, <span class="va">.fittedPC3</span>, color <span class="op">=</span> <span class="va">label</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="va">colors</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-41-1.png" width="672"></div>
<p>Lastly, we will plot the variance explained of each principal component. We can use <code>tidy()</code> with <code>matrix = "eigenvalues"</code> to accomplish this easily, so we start with the percentage of each PC</p>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">nci60_pca</span>, matrix <span class="op">=</span> <span class="st">"eigenvalues"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">PC</span>, <span class="va">percent</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-42-1.png" width="672"></div>
<p>with the first PC having a little more than 10% and a fairly fast drop.</p>
<p>And we can get the cumulative variance explained just the same.</p>
<div class="sourceCode" id="cb391"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">nci60_pca</span>, matrix <span class="op">=</span> <span class="st">"eigenvalues"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">PC</span>, <span class="va">cumulative</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-43-1.png" width="672"></div>
</div>
<div id="clustering-on-nci60-dataset" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Clustering on nci60 dataset<a class="anchor" aria-label="anchor" href="#clustering-on-nci60-dataset"><i class="fas fa-link"></i></a>
</h2>
<p>Let us now see what happens if we perform clustering on the <code>nci60</code> data set. Before we start it would be good if we create a scaled version of this data set. We can use the recipes package to perform those transformations.</p>
<div class="sourceCode" id="cb392"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_scaled</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">nci60</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_rm</span><span class="op">(</span><span class="va">label</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<p>Now we start by fitting multiple hierarchical clustering models using different agglomeration methods.</p>
<div class="sourceCode" id="cb393"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_complete</span> <span class="op">&lt;-</span> <span class="va">nci60_scaled</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span>

<span class="va">nci60_average</span> <span class="op">&lt;-</span> <span class="va">nci60_scaled</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"average"</span><span class="op">)</span>

<span class="va">nci60_single</span> <span class="op">&lt;-</span> <span class="va">nci60_scaled</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"single"</span><span class="op">)</span></code></pre></div>
<p>We then visualize them to see if any of them have some good natural separations.</p>
<div class="sourceCode" id="cb394"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">nci60_complete</span>, main <span class="op">=</span> <span class="st">"Complete"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-46-1.png" width="672"></div>
<div class="sourceCode" id="cb395"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">nci60_average</span>, main <span class="op">=</span> <span class="st">"Average"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-46-2.png" width="672"></div>
<div class="sourceCode" id="cb396"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">nci60_single</span>, main <span class="op">=</span> <span class="st">"Single"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-46-3.png" width="672"></div>
<p>We now color according to <code>k = 4</code> and we get the following separations.</p>
<div class="sourceCode" id="cb397"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_complete</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fl">4</span>, main <span class="op">=</span> <span class="st">"hclust(complete) on nci60"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-47-1.png" width="672"></div>
<p>We now take the clustering id extracted with <code>cutree</code> and calculate which Label is the most common within each cluster.</p>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tibble</span><span class="op">(</span>
  label <span class="op">=</span> <span class="va">nci60</span><span class="op">$</span><span class="va">label</span>,
  cluster_id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span><span class="op">(</span><span class="va">nci60_complete</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>
<span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">count</span><span class="op">(</span><span class="va">label</span>, <span class="va">cluster_id</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">group_by</span><span class="op">(</span><span class="va">cluster_id</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>prop <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">slice_max</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1</span>, order_by <span class="op">=</span> <span class="va">prop</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   label    cluster_id     n  prop
##   &lt;fct&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1 MELANOMA          1     8 0.2  
## 2 NSCLC             1     8 0.2  
## 3 RENAL             1     8 0.2  
## 4 BREAST            2     3 0.429
## 5 LEUKEMIA          3     6 0.75 
## 6 COLON             4     5 0.556</code></pre>
<p>We can also see what happens if we try to fit a K-means clustering. We liked 4 clusters from earlier so let’s stick with that.</p>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>
<span class="va">res_kmeans_scaled</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span><span class="va">nci60_scaled</span>, centers <span class="op">=</span> <span class="fl">4</span>, nstart <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></code></pre></div>
<p>We can again use <code>tidy()</code> to extract cluster information, note that we only look at <code>cluster</code>, <code>size</code>, and <code>withinss</code> as there are thousands of other variables denoting the location of the cluster.</p>
<div class="sourceCode" id="cb401"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">tidy</span><span class="op">(</span><span class="va">res_kmeans_scaled</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="va">cluster</span>, <span class="va">size</span>, <span class="va">withinss</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   cluster  size withinss
##   &lt;fct&gt;   &lt;int&gt;    &lt;dbl&gt;
## 1 1          20  108801.
## 2 2          27  154545.
## 3 3           9   37150.
## 4 4           8   44071.</code></pre>
<p>lastly, let us see how the two different methods we used compare against each other. Let us save the cluster ids in <code>cluster_kmeans</code> and <code>cluster_hclust</code> and then use <code>conf_mat()</code> in a different way to quickly generate a heatmap between the two methods.</p>
<div class="sourceCode" id="cb403"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">cluster_kmeans</span> <span class="op">&lt;-</span> <span class="va">res_kmeans_scaled</span><span class="op">$</span><span class="va">cluster</span>
<span class="va">cluster_hclust</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span><span class="op">(</span><span class="va">nci60_complete</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>

<span class="fu">tibble</span><span class="op">(</span>
  kmeans <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">cluster_kmeans</span><span class="op">)</span>,
  hclust <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">cluster_hclust</span><span class="op">)</span>
<span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">conf_mat</span><span class="op">(</span><span class="va">kmeans</span>, <span class="va">hclust</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">autoplot</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"heatmap"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-51-1.png" width="672"></div>
<p>There is not a lot of agreement between labels which makes sense, since the labels themselves are arbitrarily added. What is important is that they tend to agree quite a lot (the confusion matrix is sparse).</p>
<p>One last thing that is sometimes useful is to perform dimensionality reduction before using the clustering method. Let us use the recipes package to calculate the PCA of <code>nci60</code> and keep the 5 first components. (we could have started with <code>nci60</code> too if we added <code>step_rm()</code> and <code>step_normalize()</code>).</p>
<div class="sourceCode" id="cb404"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pca</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, <span class="va">nci60_scaled</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">step_pca</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span>, num_comp <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">bake</span><span class="op">(</span>new_data <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<p>We can now use <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code> on this reduced data set, and sometimes we get quite good results since the clustering method doesn’t have to work in high dimensions.</p>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nci60_pca</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/dist.html">dist</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fl">4</span>, main <span class="op">=</span> <span class="st">"hclust on first five PCs"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-unsupervised-learning_files/figure-html/unnamed-chunk-53-1.png" width="672"></div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="support-vector-machines.html"><span class="header-section-number">9</span> Support Vector Machines</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#unsupervised-learning"><span class="header-section-number">10</span> Unsupervised Learning</a></li>
<li><a class="nav-link" href="#principal-components-analysis"><span class="header-section-number">10.1</span> Principal Components Analysis</a></li>
<li><a class="nav-link" href="#kmeans-clustering"><span class="header-section-number">10.2</span> Kmeans Clustering</a></li>
<li><a class="nav-link" href="#hierarchical-clustering"><span class="header-section-number">10.3</span> Hierarchical Clustering</a></li>
<li><a class="nav-link" href="#pca-on-the-nci60-data"><span class="header-section-number">10.4</span> PCA on the NCI60 Data</a></li>
<li><a class="nav-link" href="#clustering-on-nci60-dataset"><span class="header-section-number">10.5</span> Clustering on nci60 dataset</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>ISLR tidymodels Labs</strong>" was written by Emil Hvitfeldt. It was last built on 2021-06-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>
</body>
</html>
