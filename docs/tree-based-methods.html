<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Tree-Based Methods | ISLR tidymodels Labs</title>
<meta name="author" content="Emil Hvitfeldt">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.8.6/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4.9003/tabs.js"></script><script src="libs/bs3compat-0.2.4.9003/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">ISLR tidymodels Labs</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="statistical-learning.html"><span class="header-section-number">2</span> Statistical learning</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">3</span> Linear Regression</a></li>
<li><a class="" href="classification.html"><span class="header-section-number">4</span> Classification</a></li>
<li><a class="" href="resampling-methods.html"><span class="header-section-number">5</span> Resampling Methods</a></li>
<li><a class="" href="linear-model-selection-and-regularization.html"><span class="header-section-number">6</span> Linear Model Selection and Regularization</a></li>
<li><a class="" href="moving-beyond-linearity.html"><span class="header-section-number">7</span> Moving Beyond Linearity</a></li>
<li><a class="active" href="tree-based-methods.html"><span class="header-section-number">8</span> Tree-Based Methods</a></li>
<li><a class="" href="support-vector-machines.html"><span class="header-section-number">9</span> Support Vector Machines</a></li>
<li><a class="" href="unsupervised-learning.html"><span class="header-section-number">10</span> Unsupervised Learning</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="tree-based-methods" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Tree-Based Methods<a class="anchor" aria-label="anchor" href="#tree-based-methods"><i class="fas fa-link"></i></a>
</h1>
<p>This lab will take a look at different tree-based models, in doing so we will explore how changing the hyperparameters can help improve performance.
This chapter will use <a href="https://www.tidymodels.org/start/models/">parsnip</a> for model fitting and <a href="https://www.tidymodels.org/start/recipes/">recipes and workflows</a> to perform the transformations, and <a href="https://www.tidymodels.org/start/tuning/">tune and dials</a> to tune the hyperparameters of the model. <code>rpart.plot</code> is used to visualize the decision trees created using the <code>rpart</code> package as engine, and <code>vip</code> is used to visualize variable importance for later models.</p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.StatLearning.com">ISLR</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/koalaverse/vip/">vip</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Boston"</span>, package <span class="op">=</span> <span class="st">"MASS"</span><span class="op">)</span>

<span class="va">Boston</span> <span class="op">&lt;-</span> <span class="fu">as_tibble</span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></code></pre></div>
<p>The <code>Boston</code> data set contain various statistics for 506 neighborhoods in Boston. We will build a regression model that related the median value of owner-occupied homes (<code>medv</code>) as the response with the remaining variables as predictors.</p>
<div class="infobox">
<p>
The <code>Boston</code> data set is quite outdated and contains some really unfortunate variables.
</p>
</div>
<p>We will also use the <code>Carseats</code> data set from the <code>ISLR</code> package to demonstrate a classification model. We create a new variable <code>High</code> to denote if <code>Sales &lt;= 8</code>, then the <code>Sales</code> predictor is removed as it is a perfect predictor of <code>High</code>.</p>
<div class="sourceCode" id="cb262"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Carseats</span> <span class="op">&lt;-</span> <span class="fu">as_tibble</span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>High <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu">if_else</span><span class="op">(</span><span class="va">Sales</span> <span class="op">&lt;=</span> <span class="fl">8</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">Sales</span><span class="op">)</span></code></pre></div>
<div id="fitting-classification-trees" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Fitting Classification Trees<a class="anchor" aria-label="anchor" href="#fitting-classification-trees"><i class="fas fa-link"></i></a>
</h2>
<p>We will both be fitting a classification and regression tree in this section, so we can save a little bit of typing by creating a general decision tree specification using <code>rpart</code> as the engine.</p>
<div class="sourceCode" id="cb263"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tree_spec</span> <span class="op">&lt;-</span> <span class="fu">decision_tree</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"rpart"</span><span class="op">)</span></code></pre></div>
<p>Then this decision tree specification can be used to create a classification decision tree engine. This is a good example of how the flexible composition system created by parsnip can be used to create multiple model specifications.</p>
<div class="sourceCode" id="cb264"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">class_tree_spec</span> <span class="op">&lt;-</span> <span class="va">tree_spec</span> <span class="op">%&gt;%</span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"classification"</span><span class="op">)</span></code></pre></div>
<p>With both a model specification and our data are we ready to fit the model.</p>
<div class="sourceCode" id="cb265"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">class_tree_fit</span> <span class="op">&lt;-</span> <span class="va">class_tree_spec</span> <span class="op">%&gt;%</span>
  <span class="fu">fit</span><span class="op">(</span><span class="va">High</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span></code></pre></div>
<p>When we look at the model output we see a quite informative summary of the model. It tries to give a written description of the tree that is created.</p>
<div class="sourceCode" id="cb266"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">class_tree_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  20ms 
## n= 400 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 400 164 No (0.59000000 0.41000000)  
##     2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  
##       4) Price&gt;=92.5 269  66 No (0.75464684 0.24535316)  
##         8) Advertising&lt; 13.5 224  41 No (0.81696429 0.18303571)  
##          16) CompPrice&lt; 124.5 96   6 No (0.93750000 0.06250000) *
##          17) CompPrice&gt;=124.5 128  35 No (0.72656250 0.27343750)  
##            34) Price&gt;=109.5 107  20 No (0.81308411 0.18691589)  
##              68) Price&gt;=126.5 65   6 No (0.90769231 0.09230769) *
##              69) Price&lt; 126.5 42  14 No (0.66666667 0.33333333)  
##               138) Age&gt;=49.5 22   2 No (0.90909091 0.09090909) *
##               139) Age&lt; 49.5 20   8 Yes (0.40000000 0.60000000) *
##            35) Price&lt; 109.5 21   6 Yes (0.28571429 0.71428571) *
##         9) Advertising&gt;=13.5 45  20 Yes (0.44444444 0.55555556)  
##          18) Age&gt;=54.5 20   5 No (0.75000000 0.25000000) *
##          19) Age&lt; 54.5 25   5 Yes (0.20000000 0.80000000) *
##       5) Price&lt; 92.5 46  14 Yes (0.30434783 0.69565217)  
##        10) Income&lt; 57 10   3 No (0.70000000 0.30000000) *
##        11) Income&gt;=57 36   7 Yes (0.19444444 0.80555556) *
##     3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  
##       6) Price&gt;=142.5 12   3 No (0.75000000 0.25000000) *
##       7) Price&lt; 142.5 73  10 Yes (0.13698630 0.86301370) *</code></pre>
<p>The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> method provides even more information that can be useful.</p>
<div class="sourceCode" id="cb268"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">class_tree_fit</span><span class="op">$</span><span class="va">fit</span><span class="op">)</span></code></pre></div>
<pre><code>## Call:
## rpart::rpart(formula = High ~ ., data = data)
##   n= 400 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.28658537      0 1.0000000 1.0000000 0.05997967
## 2 0.10975610      1 0.7134146 0.7134146 0.05547692
## 3 0.04573171      2 0.6036585 0.6158537 0.05298128
## 4 0.03658537      4 0.5121951 0.6280488 0.05332403
## 5 0.02743902      5 0.4756098 0.5975610 0.05244966
## 6 0.02439024      7 0.4207317 0.6097561 0.05280643
## 7 0.01219512      8 0.3963415 0.5609756 0.05132104
## 8 0.01000000     10 0.3719512 0.5792683 0.05189648
## 
## Variable importance
##       Price   ShelveLoc         Age Advertising   CompPrice      Income 
##          34          25          11          11           9           5 
##  Population   Education 
##           3           1 
## 
## Node number 1: 400 observations,    complexity param=0.2865854
##   predicted class=No   expected loss=0.41  P(node) =1
##     class counts:   236   164
##    probabilities: 0.590 0.410 
##   left son=2 (315 obs) right son=3 (85 obs)
##   Primary splits:
##       ShelveLoc   splits as  LRL,       improve=28.991900, (0 missing)
##       Price       &lt; 92.5  to the right, improve=19.463880, (0 missing)
##       Advertising &lt; 6.5   to the left,  improve=17.277980, (0 missing)
##       Age         &lt; 61.5  to the right, improve= 9.264442, (0 missing)
##       Income      &lt; 60.5  to the left,  improve= 7.249032, (0 missing)
## 
## Node number 2: 315 observations,    complexity param=0.1097561
##   predicted class=No   expected loss=0.3111111  P(node) =0.7875
##     class counts:   217    98
##    probabilities: 0.689 0.311 
##   left son=4 (269 obs) right son=5 (46 obs)
##   Primary splits:
##       Price       &lt; 92.5  to the right, improve=15.930580, (0 missing)
##       Advertising &lt; 7.5   to the left,  improve=11.432570, (0 missing)
##       ShelveLoc   splits as  L-R,       improve= 7.543912, (0 missing)
##       Age         &lt; 50.5  to the right, improve= 6.369905, (0 missing)
##       Income      &lt; 60.5  to the left,  improve= 5.984509, (0 missing)
##   Surrogate splits:
##       CompPrice &lt; 95.5  to the right, agree=0.873, adj=0.13, (0 split)
## 
## Node number 3: 85 observations,    complexity param=0.03658537
##   predicted class=Yes  expected loss=0.2235294  P(node) =0.2125
##     class counts:    19    66
##    probabilities: 0.224 0.776 
##   left son=6 (12 obs) right son=7 (73 obs)
##   Primary splits:
##       Price       &lt; 142.5 to the right, improve=7.745608, (0 missing)
##       US          splits as  LR,        improve=5.112440, (0 missing)
##       Income      &lt; 35    to the left,  improve=4.529433, (0 missing)
##       Advertising &lt; 6     to the left,  improve=3.739996, (0 missing)
##       Education   &lt; 15.5  to the left,  improve=2.565856, (0 missing)
##   Surrogate splits:
##       CompPrice &lt; 154.5 to the right, agree=0.882, adj=0.167, (0 split)
## 
## Node number 4: 269 observations,    complexity param=0.04573171
##   predicted class=No   expected loss=0.2453532  P(node) =0.6725
##     class counts:   203    66
##    probabilities: 0.755 0.245 
##   left son=8 (224 obs) right son=9 (45 obs)
##   Primary splits:
##       Advertising &lt; 13.5  to the left,  improve=10.400090, (0 missing)
##       Age         &lt; 49.5  to the right, improve= 8.083998, (0 missing)
##       ShelveLoc   splits as  L-R,       improve= 7.023150, (0 missing)
##       CompPrice   &lt; 124.5 to the left,  improve= 6.749986, (0 missing)
##       Price       &lt; 126.5 to the right, improve= 5.646063, (0 missing)
## 
## Node number 5: 46 observations,    complexity param=0.02439024
##   predicted class=Yes  expected loss=0.3043478  P(node) =0.115
##     class counts:    14    32
##    probabilities: 0.304 0.696 
##   left son=10 (10 obs) right son=11 (36 obs)
##   Primary splits:
##       Income      &lt; 57    to the left,  improve=4.000483, (0 missing)
##       ShelveLoc   splits as  L-R,       improve=3.189762, (0 missing)
##       Advertising &lt; 9.5   to the left,  improve=1.388592, (0 missing)
##       Price       &lt; 80.5  to the right, improve=1.388592, (0 missing)
##       Age         &lt; 64.5  to the right, improve=1.172885, (0 missing)
## 
## Node number 6: 12 observations
##   predicted class=No   expected loss=0.25  P(node) =0.03
##     class counts:     9     3
##    probabilities: 0.750 0.250 
## 
## Node number 7: 73 observations
##   predicted class=Yes  expected loss=0.1369863  P(node) =0.1825
##     class counts:    10    63
##    probabilities: 0.137 0.863 
## 
## Node number 8: 224 observations,    complexity param=0.02743902
##   predicted class=No   expected loss=0.1830357  P(node) =0.56
##     class counts:   183    41
##    probabilities: 0.817 0.183 
##   left son=16 (96 obs) right son=17 (128 obs)
##   Primary splits:
##       CompPrice   &lt; 124.5 to the left,  improve=4.881696, (0 missing)
##       Age         &lt; 49.5  to the right, improve=3.960418, (0 missing)
##       ShelveLoc   splits as  L-R,       improve=3.654633, (0 missing)
##       Price       &lt; 126.5 to the right, improve=3.234428, (0 missing)
##       Advertising &lt; 6.5   to the left,  improve=2.371276, (0 missing)
##   Surrogate splits:
##       Price      &lt; 115.5 to the left,  agree=0.741, adj=0.396, (0 split)
##       Age        &lt; 50.5  to the right, agree=0.634, adj=0.146, (0 split)
##       Population &lt; 405   to the right, agree=0.629, adj=0.135, (0 split)
##       Education  &lt; 11.5  to the left,  agree=0.585, adj=0.031, (0 split)
##       Income     &lt; 22.5  to the left,  agree=0.580, adj=0.021, (0 split)
## 
## Node number 9: 45 observations,    complexity param=0.04573171
##   predicted class=Yes  expected loss=0.4444444  P(node) =0.1125
##     class counts:    20    25
##    probabilities: 0.444 0.556 
##   left son=18 (20 obs) right son=19 (25 obs)
##   Primary splits:
##       Age       &lt; 54.5  to the right, improve=6.722222, (0 missing)
##       CompPrice &lt; 121.5 to the left,  improve=4.629630, (0 missing)
##       ShelveLoc splits as  L-R,       improve=3.250794, (0 missing)
##       Income    &lt; 99.5  to the left,  improve=3.050794, (0 missing)
##       Price     &lt; 127   to the right, improve=2.933429, (0 missing)
##   Surrogate splits:
##       Population  &lt; 363.5 to the left,  agree=0.667, adj=0.25, (0 split)
##       Income      &lt; 39    to the left,  agree=0.644, adj=0.20, (0 split)
##       Advertising &lt; 17.5  to the left,  agree=0.644, adj=0.20, (0 split)
##       CompPrice   &lt; 106.5 to the left,  agree=0.622, adj=0.15, (0 split)
##       Price       &lt; 135.5 to the right, agree=0.622, adj=0.15, (0 split)
## 
## Node number 10: 10 observations
##   predicted class=No   expected loss=0.3  P(node) =0.025
##     class counts:     7     3
##    probabilities: 0.700 0.300 
## 
## Node number 11: 36 observations
##   predicted class=Yes  expected loss=0.1944444  P(node) =0.09
##     class counts:     7    29
##    probabilities: 0.194 0.806 
## 
## Node number 16: 96 observations
##   predicted class=No   expected loss=0.0625  P(node) =0.24
##     class counts:    90     6
##    probabilities: 0.938 0.062 
## 
## Node number 17: 128 observations,    complexity param=0.02743902
##   predicted class=No   expected loss=0.2734375  P(node) =0.32
##     class counts:    93    35
##    probabilities: 0.727 0.273 
##   left son=34 (107 obs) right son=35 (21 obs)
##   Primary splits:
##       Price     &lt; 109.5 to the right, improve=9.764582, (0 missing)
##       ShelveLoc splits as  L-R,       improve=6.320022, (0 missing)
##       Age       &lt; 49.5  to the right, improve=2.575061, (0 missing)
##       Income    &lt; 108.5 to the right, improve=1.799546, (0 missing)
##       CompPrice &lt; 143.5 to the left,  improve=1.741982, (0 missing)
## 
## Node number 18: 20 observations
##   predicted class=No   expected loss=0.25  P(node) =0.05
##     class counts:    15     5
##    probabilities: 0.750 0.250 
## 
## Node number 19: 25 observations
##   predicted class=Yes  expected loss=0.2  P(node) =0.0625
##     class counts:     5    20
##    probabilities: 0.200 0.800 
## 
## Node number 34: 107 observations,    complexity param=0.01219512
##   predicted class=No   expected loss=0.1869159  P(node) =0.2675
##     class counts:    87    20
##    probabilities: 0.813 0.187 
##   left son=68 (65 obs) right son=69 (42 obs)
##   Primary splits:
##       Price     &lt; 126.5 to the right, improve=2.9643900, (0 missing)
##       CompPrice &lt; 147.5 to the left,  improve=2.2337090, (0 missing)
##       ShelveLoc splits as  L-R,       improve=2.2125310, (0 missing)
##       Age       &lt; 49.5  to the right, improve=2.1458210, (0 missing)
##       Income    &lt; 60.5  to the left,  improve=0.8025853, (0 missing)
##   Surrogate splits:
##       CompPrice   &lt; 129.5 to the right, agree=0.664, adj=0.143, (0 split)
##       Advertising &lt; 3.5   to the right, agree=0.664, adj=0.143, (0 split)
##       Population  &lt; 53.5  to the right, agree=0.645, adj=0.095, (0 split)
##       Age         &lt; 77.5  to the left,  agree=0.636, adj=0.071, (0 split)
##       US          splits as  RL,        agree=0.626, adj=0.048, (0 split)
## 
## Node number 35: 21 observations
##   predicted class=Yes  expected loss=0.2857143  P(node) =0.0525
##     class counts:     6    15
##    probabilities: 0.286 0.714 
## 
## Node number 68: 65 observations
##   predicted class=No   expected loss=0.09230769  P(node) =0.1625
##     class counts:    59     6
##    probabilities: 0.908 0.092 
## 
## Node number 69: 42 observations,    complexity param=0.01219512
##   predicted class=No   expected loss=0.3333333  P(node) =0.105
##     class counts:    28    14
##    probabilities: 0.667 0.333 
##   left son=138 (22 obs) right son=139 (20 obs)
##   Primary splits:
##       Age         &lt; 49.5  to the right, improve=5.4303030, (0 missing)
##       CompPrice   &lt; 137.5 to the left,  improve=2.1000000, (0 missing)
##       Advertising &lt; 5.5   to the left,  improve=1.8666670, (0 missing)
##       ShelveLoc   splits as  L-R,       improve=1.4291670, (0 missing)
##       Population  &lt; 382   to the right, improve=0.8578431, (0 missing)
##   Surrogate splits:
##       Income      &lt; 46.5  to the left,  agree=0.595, adj=0.15, (0 split)
##       Education   &lt; 12.5  to the left,  agree=0.595, adj=0.15, (0 split)
##       CompPrice   &lt; 131.5 to the right, agree=0.571, adj=0.10, (0 split)
##       Advertising &lt; 5.5   to the left,  agree=0.571, adj=0.10, (0 split)
##       Population  &lt; 221.5 to the left,  agree=0.571, adj=0.10, (0 split)
## 
## Node number 138: 22 observations
##   predicted class=No   expected loss=0.09090909  P(node) =0.055
##     class counts:    20     2
##    probabilities: 0.909 0.091 
## 
## Node number 139: 20 observations
##   predicted class=Yes  expected loss=0.4  P(node) =0.05
##     class counts:     8    12
##    probabilities: 0.400 0.600</code></pre>
<p>Once the tree gets more than a couple of nodes it can become hard to read the printed diagram. The <code>rpart.plot</code> package provides functions to let us easily visualize the decision tree. As the name implies, it only works with <code>rpart</code> trees.</p>
<div class="sourceCode" id="cb270"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">class_tree_fit</span><span class="op">$</span><span class="va">fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-9-1.png" width="672"></div>
<p>We can see that the most important variable to predict high sales appears to be shelving location as it forms the first node.</p>
<p>The training accuracy of this model is 85%</p>
<div class="sourceCode" id="cb271"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">class_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">accuracy</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">High</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.848</code></pre>
<p>Let us take a look at the confusion matrix to see if the balance is there</p>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">class_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">conf_mat</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">High</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span></code></pre></div>
<pre><code>##           Truth
## Prediction  No Yes
##        No  200  25
##        Yes  36 139</code></pre>
<p>And the model appears to work well overall. But this model was fit on the whole data set so we only get the training accuracy which could be misleading if the model is overfitting. Let us redo the fitting by creating a validation split and fit the model on the training data set.</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">Carseats_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span>

<span class="va">Carseats_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">Carseats_split</span><span class="op">)</span>
<span class="va">Carseats_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">Carseats_split</span><span class="op">)</span></code></pre></div>
<p>Now we can fit the model on the training data set.</p>
<div class="sourceCode" id="cb276"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">class_tree_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">class_tree_spec</span>, <span class="va">High</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Carseats_train</span><span class="op">)</span></code></pre></div>
<p>Let us take a look at the confusion matrix for the training data set and testing data set.</p>
<div class="sourceCode" id="cb277"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">class_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Carseats_train</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">conf_mat</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">High</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span></code></pre></div>
<pre><code>##           Truth
## Prediction  No Yes
##        No  159  21
##        Yes  21  99</code></pre>
<p>The training data set performs well as we would expect</p>
<div class="sourceCode" id="cb279"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">class_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Carseats_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">conf_mat</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">High</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span></code></pre></div>
<pre><code>##           Truth
## Prediction No Yes
##        No  41   8
##        Yes 15  36</code></pre>
<p>but the testing data set doesn’t perform just as well and get a smaller accuracy of 77%</p>
<div class="sourceCode" id="cb281"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">class_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Carseats_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">accuracy</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">High</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary          0.77</code></pre>
<p>Let us try to tune the <code>cost_complexity</code> of the decision tree to find a more optimal complexity. We use the <code>class_tree_spec</code> object and use the <code>set_args()</code> function to specify that we want to tune <code>cost_complexity</code>. This is then passed directly into the workflow object to avoid creating an intermediate object.</p>
<div class="sourceCode" id="cb283"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">class_tree_wf</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">class_tree_spec</span> <span class="op">%&gt;%</span> <span class="fu">set_args</span><span class="op">(</span>cost_complexity <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">add_formula</span><span class="op">(</span><span class="va">High</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></code></pre></div>
<p>To be able to tune the variable we need 2 more objects. S <code>resamples</code> object, we will use a k-fold cross-validation data set, and a <code>grid</code> of values to try. Since we are only tuning 1 hyperparameter it is fine to stay with a regular grid.</p>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">Carseats_fold</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="va">Carseats_train</span><span class="op">)</span>

<span class="va">param_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">cost_complexity</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>

<span class="va">tune_res</span> <span class="op">&lt;-</span> <span class="fu">tune_grid</span><span class="op">(</span>
  <span class="va">class_tree_wf</span>, 
  resamples <span class="op">=</span> <span class="va">Carseats_fold</span>, 
  grid <span class="op">=</span> <span class="va">param_grid</span>, 
  metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">accuracy</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>using <code>autoplot()</code> shows which values of <code>cost_complexity</code> appear to produce the highest accuracy</p>
<div class="sourceCode" id="cb285"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">autoplot</span><span class="op">(</span><span class="va">tune_res</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-19-1.png" width="672"></div>
<p>We can now select the best performing value with <code>select_best()</code>, finalize the workflow by updating the value of <code>cost_complexity</code> and fit the model on the full training data set.</p>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_complexity</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">tune_res</span><span class="op">)</span>

<span class="va">class_tree_final</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">class_tree_wf</span>, <span class="va">best_complexity</span><span class="op">)</span>

<span class="va">class_tree_final_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">class_tree_final</span>, data <span class="op">=</span> <span class="va">Carseats_train</span><span class="op">)</span>
<span class="va">class_tree_final_fit</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: decision_tree()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## High ~ .
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## n= 300 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 300 120 No (0.6000000 0.4000000)  
##   2) ShelveLoc=Bad,Medium 242  73 No (0.6983471 0.3016529)  
##     4) Price&gt;=92.5 213  51 No (0.7605634 0.2394366) *
##     5) Price&lt; 92.5 29   7 Yes (0.2413793 0.7586207) *
##   3) ShelveLoc=Good 58  11 Yes (0.1896552 0.8103448) *</code></pre>
<p>At last, we can visualize the model, and we see that the better-performing model is less complex than the original model we fit.</p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">class_tree_final_fit</span><span class="op">$</span><span class="va">fit</span><span class="op">$</span><span class="va">fit</span><span class="op">$</span><span class="va">fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-21-1.png" width="672"></div>
</div>
<div id="fitting-regression-trees" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Fitting Regression Trees<a class="anchor" aria-label="anchor" href="#fitting-regression-trees"><i class="fas fa-link"></i></a>
</h2>
<p>We will now show how we fit a regression tree. This is very similar to what we saw in the last section. The main difference here is that the response we are looking at will be continuous instead of categorical. We can reuse <code>tree_spec</code> as a base for the regression decision tree specification.</p>
<div class="sourceCode" id="cb289"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">reg_tree_spec</span> <span class="op">&lt;-</span> <span class="va">tree_spec</span> <span class="op">%&gt;%</span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span></code></pre></div>
<p>We are using the <code>Boston</code> data set here so we will do a validation split here.</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">Boston_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span>

<span class="va">Boston_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">Boston_split</span><span class="op">)</span>
<span class="va">Boston_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">Boston_split</span><span class="op">)</span></code></pre></div>
<p>fitting the model to the training data set</p>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">reg_tree_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">reg_tree_spec</span>, <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">Boston_train</span><span class="op">)</span>
<span class="va">reg_tree_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  13ms 
## n= 379 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 379 33574.1000 23.10554  
##    2) rm&lt; 6.92 316 13278.0400 20.28924  
##      4) lstat&gt;=14.395 118  2331.0780 15.06186  
##        8) nox&gt;=0.607 72   952.7732 12.81528  
##         16) lstat&gt;=18.885 44   358.7518 11.01364 *
##         17) lstat&lt; 18.885 28   226.7696 15.64643 *
##        9) nox&lt; 0.607 46   446.1183 18.57826 *
##      5) lstat&lt; 14.395 198  5800.9460 23.40455  
##       10) dis&gt;=1.5604 191  2826.0840 22.86963  
##         20) rm&lt; 6.543 149  1320.8880 21.67383 *
##         21) rm&gt;=6.543 42   536.2640 27.11190 *
##       11) dis&lt; 1.5604 7  1429.0200 38.00000 *
##    3) rm&gt;=6.92 63  5218.0570 37.23175  
##      6) rm&lt; 7.437 39  1770.9840 32.18718  
##       12) lstat&gt;=8.33 7   470.3343 23.57143 *
##       13) lstat&lt; 8.33 32   667.3647 34.07188 *
##      7) rm&gt;=7.437 24   841.8696 45.42917 *</code></pre>
<div class="sourceCode" id="cb293"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">reg_tree_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">rmse</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">medv</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard        3.51</code></pre>
<p>and the <code><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot()</a></code> function works for the regression decision tree as well</p>
<div class="sourceCode" id="cb295"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">reg_tree_fit</span><span class="op">$</span><span class="va">fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-26-1.png" width="672"></div>
<p>Notice how the result is a numeric variable instead of a class.</p>
<p>Now let us again try to tune the <code>cost_complexity</code> to find the best performing model.</p>
<div class="sourceCode" id="cb296"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">reg_tree_wf</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">reg_tree_spec</span> <span class="op">%&gt;%</span> <span class="fu">set_args</span><span class="op">(</span>cost_complexity <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">add_formula</span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">Boston_fold</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="va">Boston_train</span><span class="op">)</span>

<span class="va">param_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">cost_complexity</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>

<span class="va">tune_res</span> <span class="op">&lt;-</span> <span class="fu">tune_grid</span><span class="op">(</span>
  <span class="va">reg_tree_wf</span>, 
  resamples <span class="op">=</span> <span class="va">Boston_fold</span>, 
  grid <span class="op">=</span> <span class="va">param_grid</span>
<span class="op">)</span></code></pre></div>
<p>And it appears that higher complexity works are to be preferred according to our cross-validation</p>
<div class="sourceCode" id="cb297"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">autoplot</span><span class="op">(</span><span class="va">tune_res</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-28-1.png" width="672"></div>
<p>We select the best-performing model according to <code>"rmse"</code> and fit the final model on the whole training data set.</p>
<div class="sourceCode" id="cb298"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_complexity</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">tune_res</span>, metric <span class="op">=</span> <span class="st">"rmse"</span><span class="op">)</span>

<span class="va">reg_tree_final</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">reg_tree_wf</span>, <span class="va">best_complexity</span><span class="op">)</span>

<span class="va">reg_tree_final_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">reg_tree_final</span>, data <span class="op">=</span> <span class="va">Boston_train</span><span class="op">)</span>
<span class="va">reg_tree_final_fit</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: decision_tree()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## medv ~ .
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## n= 379 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##    1) root 379 33574.10000 23.10554  
##      2) rm&lt; 6.92 316 13278.04000 20.28924  
##        4) lstat&gt;=14.395 118  2331.07800 15.06186  
##          8) nox&gt;=0.607 72   952.77320 12.81528  
##           16) lstat&gt;=18.885 44   358.75180 11.01364  
##             32) tax&gt;=551.5 36   225.74560 10.26111  
##               64) lstat&gt;=23.88 16    91.40438  9.21875 *
##               65) lstat&lt; 23.88 20   103.04950 11.09500  
##                130) crim&gt;=11.48635 10    64.56100  9.83000 *
##                131) crim&lt; 11.48635 10     6.48400 12.36000 *
##             33) tax&lt; 551.5 8    20.88000 14.40000 *
##           17) lstat&lt; 18.885 28   226.76960 15.64643  
##             34) crim&gt;=6.88166 11    42.42182 13.37273 *
##             35) crim&lt; 6.88166 17    90.68471 17.11765 *
##          9) nox&lt; 0.607 46   446.11830 18.57826  
##           18) age&gt;=60.55 38   318.05890 17.89474  
##             36) crim&gt;=0.592985 15   206.00000 16.50000 *
##             37) crim&lt; 0.592985 23    63.84957 18.80435  
##               74) age&gt;=93.15 8    25.03500 17.62500 *
##               75) age&lt; 93.15 15    21.75333 19.43333 *
##           19) age&lt; 60.55 8    25.97500 21.82500 *
##        5) lstat&lt; 14.395 198  5800.94600 23.40455  
##         10) dis&gt;=1.5604 191  2826.08400 22.86963  
##           20) rm&lt; 6.543 149  1320.88800 21.67383  
##             40) lstat&gt;=7.57 113   926.73010 20.91327  
##               80) rm&lt; 6.064 61   351.43410 19.99016  
##                160) dis&gt;=6.6261 10     8.86400 18.36000 *
##                161) dis&lt; 6.6261 51   310.78510 20.30980  
##                  322) black&gt;=396.255 17    99.48235 19.15294 *
##                  323) black&lt; 396.255 34   177.17530 20.88824  
##                    646) indus&gt;=6.58 23    92.67304 20.28261  
##                     1292) rm&lt; 5.844 11    40.70182 19.17273 *
##                     1293) rm&gt;=5.844 12    26.00000 21.30000 *
##                    647) indus&lt; 6.58 11    58.42727 22.15455 *
##               81) rm&gt;=6.064 52   462.33920 21.99615  
##                162) lstat&gt;=9.98 33    97.62970 21.10303  
##                  324) lstat&gt;=11.755 19    44.50632 20.45789 *
##                  325) lstat&lt; 11.755 14    34.48357 21.97857 *
##                163) lstat&lt; 9.98 19   292.66740 23.54737 *
##             41) lstat&lt; 7.57 36   123.62560 24.06111  
##               82) dis&gt;=6.12605 15    26.71333 23.03333 *
##               83) dis&lt; 6.12605 21    69.74952 24.79524  
##                166) indus&gt;=7.17 14    29.93214 24.16429 *
##                167) indus&lt; 7.17 7    23.09714 26.05714 *
##           21) rm&gt;=6.543 42   536.26400 27.11190  
## 
## ...
## and 16 more lines.</code></pre>
<p>Visualizing the model reveals a much more complex tree than what we saw in the last section.</p>
<div class="sourceCode" id="cb300"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">reg_tree_final_fit</span><span class="op">$</span><span class="va">fit</span><span class="op">$</span><span class="va">fit</span><span class="op">$</span><span class="va">fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-30-1.png" width="672"></div>
</div>
<div id="bagging-and-random-forests" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Bagging and Random Forests<a class="anchor" aria-label="anchor" href="#bagging-and-random-forests"><i class="fas fa-link"></i></a>
</h2>
<p>Here we apply bagging and random forests to the <code>Boston</code> data set. We will be using the <code>randomForest</code> package as the engine. A bagging model is the same as a random forest where <code>mtry</code> is equal to the number of predictors. We can specify the <code>mtry</code> to be <code>.cols()</code> which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and useable to many different data sets. <code>.cols()</code> is one of many <a href="https://parsnip.tidymodels.org/reference/descriptors.html">descriptors</a> in the parsnip package.
We also set <code>importance = TRUE</code> in <code>set_engine()</code> to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the <code>vip</code> package later.</p>
<div class="sourceCode" id="cb301"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bagging_spec</span> <span class="op">&lt;-</span> <span class="fu">rand_forest</span><span class="op">(</span>mtry <span class="op">=</span> <span class="fu">.cols</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"randomForest"</span>, importance <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span></code></pre></div>
<p>We fit the model like normal</p>
<div class="sourceCode" id="cb302"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bagging_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">bagging_spec</span>, <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston_train</span><span class="op">)</span></code></pre></div>
<p>and we take a look at the testing performance. Which we see is an improvement over the decision tree.</p>
<div class="sourceCode" id="cb303"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">bagging_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">rmse</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">medv</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard        2.53</code></pre>
<p>We can also create a quick scatterplot between the true and predicted value to see if we can make any diagnostics.</p>
<div class="sourceCode" id="cb305"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">bagging_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">medv</span>, <span class="va">.pred</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_abline</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-34-1.png" width="672"></div>
<p>There isn’t anything weird going on here so we are happy. Next, let us take a look at the variable importance</p>
<div class="sourceCode" id="cb306"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/vip/man/vip.html">vip</a></span><span class="op">(</span><span class="va">bagging_fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-35-1.png" width="672"></div>
<p>Next, let us take a look at a random forest. By default, <code>randomForest()</code> <code>p / 3</code> variables when building a random forest of regression trees, and <code><a href="https://rdrr.io/r/base/MathFun.html">sqrt(p)</a></code> variables when building a random forest of classification trees. Here we use <code>mtry = 6</code>.</p>
<div class="sourceCode" id="cb307"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rf_spec</span> <span class="op">&lt;-</span> <span class="fu">rand_forest</span><span class="op">(</span>mtry <span class="op">=</span> <span class="fl">6</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"randomForest"</span>, importance <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span></code></pre></div>
<p>and fitting the model like normal</p>
<div class="sourceCode" id="cb308"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rf_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">rf_spec</span>, <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston_train</span><span class="op">)</span></code></pre></div>
<p>this model has a slightly better performance than the bagging model</p>
<div class="sourceCode" id="cb309"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">rf_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">rmse</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">medv</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard        2.50</code></pre>
<p>We can likewise plot the true value against the predicted value</p>
<div class="sourceCode" id="cb311"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">rf_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">medv</span>, <span class="va">.pred</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_abline</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-39-1.png" width="672"></div>
<p>it looks fine. No discernable difference between this chart and the one we created for the bagging model.</p>
<p>The variable importance plot is also quite similar to what we saw for the bagging model which isn’t surprising.</p>
<div class="sourceCode" id="cb312"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/vip/man/vip.html">vip</a></span><span class="op">(</span><span class="va">rf_fit</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-40-1.png" width="672"></div>
<p>you would normally want to perform hyperparameter tuning for the random forest model to get the best out of your forest. This exercise is left for the reader.</p>
</div>
<div id="boosting" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> Boosting<a class="anchor" aria-label="anchor" href="#boosting"><i class="fas fa-link"></i></a>
</h2>
<p>We will now fit a boosted tree model. The <code>xgboost</code> packages give a good implementation of boosted trees. It has many parameters to tune and we know that setting <code>trees</code> too high can lead to overfitting. Nevertheless, let us try fitting a boosted tree. We set <code>tree = 5000</code> to grow 5000 trees with a maximal depth of 4 by setting <code>tree_depth = 4</code>.</p>
<div class="sourceCode" id="cb313"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boost_spec</span> <span class="op">&lt;-</span> <span class="fu">boost_tree</span><span class="op">(</span>trees <span class="op">=</span> <span class="fl">5000</span>, tree_depth <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"xgboost"</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span></code></pre></div>
<p>fitting the model like normal</p>
<div class="sourceCode" id="cb314"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boost_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">boost_spec</span>, <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston_train</span><span class="op">)</span></code></pre></div>
<p>and the <code>rmse</code> is a little high in this case which is properly because we didn’t tune any of the parameters.</p>
<div class="sourceCode" id="cb315"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">boost_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">rmse</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">medv</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard        2.57</code></pre>
<p>We can look at the scatterplot and we don’t see anything weird going on.</p>
<div class="sourceCode" id="cb317"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">augment</span><span class="op">(</span><span class="va">boost_fit</span>, new_data <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">medv</span>, <span class="va">.pred</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_abline</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="08-tree-based-methods_files/figure-html/unnamed-chunk-44-1.png" width="672"></div>
<p>You would normally want to perform hyperparameter tuning for the boosted tree model to get the best out of your model. This exercise is left for the reader. Look at the <a href="https://www.tmwr.org/iterative-search.html">Iterative search</a> chapter of <a href="https://www.tmwr.org/">Tidy Modeling with R</a> for inspiration.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="moving-beyond-linearity.html"><span class="header-section-number">7</span> Moving Beyond Linearity</a></div>
<div class="next"><a href="support-vector-machines.html"><span class="header-section-number">9</span> Support Vector Machines</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#tree-based-methods"><span class="header-section-number">8</span> Tree-Based Methods</a></li>
<li><a class="nav-link" href="#fitting-classification-trees"><span class="header-section-number">8.1</span> Fitting Classification Trees</a></li>
<li><a class="nav-link" href="#fitting-regression-trees"><span class="header-section-number">8.2</span> Fitting Regression Trees</a></li>
<li><a class="nav-link" href="#bagging-and-random-forests"><span class="header-section-number">8.3</span> Bagging and Random Forests</a></li>
<li><a class="nav-link" href="#boosting"><span class="header-section-number">8.4</span> Boosting</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>ISLR tidymodels Labs</strong>" was written by Emil Hvitfeldt. It was last built on 2021-06-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>
</body>
</html>
