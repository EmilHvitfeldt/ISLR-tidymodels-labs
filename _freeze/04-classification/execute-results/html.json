{
  "hash": "9398b5b54f0fbde0bdffcda081fc27fb",
  "result": {
    "markdown": "# Classification\n\n\n::: {.cell}\n\n:::\n\n\nThis lab will be our first experience with classification models. These models differ from the regression model we saw in the last chapter by the fact that the response variable is a qualitative variable instead of a continuous variable.\nThis chapter will use [parsnip](https://www.tidymodels.org/start/models/) for model fitting and [recipes and workflows](https://www.tidymodels.org/start/recipes/) to perform the transformations.\n\n## The Stock Market Data\n\nWe load the tidymodels for modeling functions, ISLR and ISLR2 for data sets, [discrim](https://discrim.tidymodels.org/) to give us access to discriminant analysis models such as LDA and QDA as well as the Naive Bayes model and [poissonreg](https://poissonreg.tidymodels.org/) for Poisson Regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ISLR) # For the Smarket data set\nlibrary(ISLR2) # For the Bikeshare data set\nlibrary(discrim)\nlibrary(poissonreg)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nWe will be examining the `Smarket` data set for this lab. It contains a number of numeric variables plus a variable called `Direction` which has the two labels `\"Up\"` and `\"Down\"`. Before we do on to modeling, let us take a look at the correlation between the variables.\n\nTo look at the correlation, we will use the [corrr](https://corrr.tidymodels.org/) package. The `correlate()` function will calculate the correlation matrix between all the variables that it is being fed. We will therefore remove `Direction` as it is not numeric.\nThen we pass that to `rplot()` to quickly visualize the correlation matrix. I have also changed the `colours` argument to better see what is going on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corrr)\ncor_Smarket <- Smarket %>%\n  select(-Direction) %>%\n  correlate()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n```\n:::\n\n```{.r .cell-code}\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-4-1.png){fig-alt='Correlation chart. Most values are very close to 0.\nYear and Volume appear quite correlated.' width=672}\n:::\n:::\n\n\nAnd we see that these variables are more or less uncorrelated with each other. The other pair is `Year` and `Volume` that is a little correlated.\n\nIf you want to create heatmap styled correlation chart you can also create it manually.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(paletteer)\ncor_Smarket %>%\n  stretch() %>%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1)\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-5-1.png){fig-alt='Correlation chart. Most values are very close to 0.\nYear and Volume appear quite correlated.' width=672}\n:::\n:::\n\n\nIf we plot `Year` against `Volume` we see that there is an upwards trend in `Volume` with time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Smarket, aes(Year, Volume)) +\n  geom_jitter(height = 0)\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-6-1.png){fig-alt='Jittered scatter chart. Jittered around year along the\nx-axis. Volume along the y-axis. Fairly wide scattering\nalong volume. Slight increase in volumne as year increase.' width=672}\n:::\n:::\n\n\n## Logistic Regression\n\nNow we will fit a logistic regression model. We will again use the parsnip package, and we will use `logistic_reg()` to create a logistic regression model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\nNotice that while I did set the engine and mode, they are just restating the defaults.\n\nWe can now fit the model like normal. We want to model the `Direction` of the stock market based on the percentage return from the 5 previous days plus the volume of shares traded.\nWhen fitting a classification with parsnip requires that the response variable is a factor. This is the case for the `Smarket` data set so we don't need to do adjustments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_fit <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket\n    )\n\nlr_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  \n  -0.126000    -0.073074    -0.042301     0.011085     0.009359     0.010313  \n     Volume  \n   0.135441  \n\nDegrees of Freedom: 1249 Total (i.e. Null);  1243 Residual\nNull Deviance:\t    1731 \nResidual Deviance: 1728 \tAIC: 1742\n```\n:::\n:::\n\n\nthis fit is done using the `glm()` function, and it comes with a very handy `summary()` method as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_fit %>%\n  pluck(\"fit\") %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nThis lets us see a couple of different things such as; parameter estimates, standard errors, p-values, and model fit statistics. we can use the `tidy()` function to extract some of these model attributes for further analysis or presentation.\n \n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lr_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept) -0.126      0.241     -0.523   0.601\n2 Lag1        -0.0731     0.0502    -1.46    0.145\n3 Lag2        -0.0423     0.0501    -0.845   0.398\n4 Lag3         0.0111     0.0499     0.222   0.824\n5 Lag4         0.00936    0.0500     0.187   0.851\n6 Lag5         0.0103     0.0495     0.208   0.835\n7 Volume       0.135      0.158      0.855   0.392\n```\n:::\n:::\n\n\nPredictions are done much the same way. Here we use the model to predict on the data it was trained on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lr_fit, new_data = Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,250 × 1\n   .pred_class\n   <fct>      \n 1 Up         \n 2 Down       \n 3 Down       \n 4 Up         \n 5 Up         \n 6 Up         \n 7 Down       \n 8 Up         \n 9 Up         \n10 Down       \n# ℹ 1,240 more rows\n```\n:::\n:::\n\n\nThe result is a tibble with a single column `.pred_class` which will be a factor variable of the same labels as the original training data set.\n\nWe can also get back probability predictions, by specifying `type = \"prob\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lr_fit, new_data = Smarket, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,250 × 2\n   .pred_Down .pred_Up\n        <dbl>    <dbl>\n 1      0.493    0.507\n 2      0.519    0.481\n 3      0.519    0.481\n 4      0.485    0.515\n 5      0.489    0.511\n 6      0.493    0.507\n 7      0.507    0.493\n 8      0.491    0.509\n 9      0.482    0.518\n10      0.511    0.489\n# ℹ 1,240 more rows\n```\n:::\n:::\n\n\nnote that we get back a column for each of the classes. This is a little reductive since we could easily calculate the inverse, but once we get to multi-classification models it becomes quite handy.\n\nUsing `augment()` we can add the predictions to the data.frame and then use that to look at model performance metrics. before we calculate the metrics directly, I find it useful to look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). This will show you how well your predictive model is performing by given a table of predicted values against the true value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down  Up\n      Down  145 141\n      Up    457 507\n```\n:::\n:::\n\n\nA good performing model would ideally have high numbers along the diagonal (up-left to down-right) with small numbers on the off-diagonal. We see here that the model isn't great, as it tends to predict `\"Down\"` as `\"Up\"` more often than it should.\n\nif you want a more visual representation of the confusion matrix you can pipe the result of `conf_mat()` into `autoplot()` to generate a ggplot2 chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lr_fit, new_data = Smarket) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-14-1.png){fig-alt='Confusion matrix chart. Truth along the x-axis,\nprediction along the y-axis. Up is predicted vastly\nmore than Down, regardless of the true value.' width=672}\n:::\n:::\n\n\nWe can also calculate various performance metrics. One of the most common metrics is accuracy, which is how often the model predicted correctly as a percentage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lr_fit, new_data = Smarket) %>%\n  accuracy(truth = Direction, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.522\n```\n:::\n:::\n\n\nand we see that the accuracy isn't great either which is obvious already looking at the confusion matrix.\n\nWe just fit a model and evaluated it on the same data. This doesn't give us that much information about the model performance. Let us instead split up the data, train it on some of it and then evaluate it on the other part of the data. Since we are working with some data that has a time component, it is natural to fit the model using the first year's worth of data and evaluate it on the last year. This would more closely match how such a model would be used in real life.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSmarket_train <- Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- Smarket %>%\n  filter(Year == 2005)\n```\n:::\n\n\nNow that we have split the data into `Smarket_train` and `Smarket_test` we can fit a logistic regression model to `Smarket_train` and evaluate it on `Smarket_test` to see how well the model generalizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_fit2 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket_train\n    )\n```\n:::\n\n\nAnd we will evaluate on the testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lr_fit2, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down Up\n      Down   77 97\n      Up     34 44\n```\n:::\n\n```{.r .cell-code}\naugment(lr_fit2, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.480\n```\n:::\n:::\n\n\nWe see that this model is not more likely to predict `\"Down\"` rather than `\"Up\"`. Also, note how the model performs worse than the last model. This is expected since we are evaluating on new data.\n\nWe recall that the logistic regression model had underwhelming p-values. Let us see what happens if we remove some of the variables that appear not to be helpful we might achieve a more predictive model since the variables that do not have a relationship with the response will cause an increase in variance without a decrease in bias.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_fit3 <- lr_spec %>%\n  fit(\n    Direction ~ Lag1 + Lag2,\n    data = Smarket_train\n    )\n\naugment(lr_fit3, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n```\n:::\n\n```{.r .cell-code}\naugment(lr_fit3, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560\n```\n:::\n:::\n\n\nAnd we see an increase in performance. The model is still not perfect but it is starting to perform better.\n\nSuppose that we want to predict the returns associated with particular values of `Lag1` and `Lag2`. In particular, we want to predict `Direction` on a day when `Lag1` and `Lag2` equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and −0.8.\n\nFor this we start by creating a tibble corresponding to the scenarios we want to predict for\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSmarket_new <- tibble(\n  Lag1 = c(1.2, 1.5), \n  Lag2 = c(1.1, -0.8)\n)\n```\n:::\n\n\nAnd then we will use `predict()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  lr_fit3,\n  new_data = Smarket_new, \n  type = \"prob\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  .pred_Down .pred_Up\n       <dbl>    <dbl>\n1      0.521    0.479\n2      0.504    0.496\n```\n:::\n:::\n\n\n## Linear Discriminant Analysis\n\nNow we will perform LDA on the `Smarket` data. We will use the `discrim_linear()` function to create a LDA specification. We will continue to use 2 predictors for easy comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nlda_fit <- lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nlda_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = data)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n```\n:::\n:::\n\n\nOne of the things to look for in the LDA output is the group means. We see that there is a slight difference between the means of the two groups. These suggest that there is a tendency for the previous 2 days' returns to be negative on days when the market increases, and a tendency for the previous day's returns to be positive on days when the market declines.\n\nPredictions are done just the same as with logistic regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lda_fit, new_data = Smarket_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 252 × 1\n   .pred_class\n   <fct>      \n 1 Up         \n 2 Up         \n 3 Up         \n 4 Up         \n 5 Up         \n 6 Up         \n 7 Up         \n 8 Up         \n 9 Up         \n10 Up         \n# ℹ 242 more rows\n```\n:::\n\n```{.r .cell-code}\npredict(lda_fit, new_data = Smarket_test, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 252 × 2\n   .pred_Down .pred_Up\n        <dbl>    <dbl>\n 1      0.490    0.510\n 2      0.479    0.521\n 3      0.467    0.533\n 4      0.474    0.526\n 5      0.493    0.507\n 6      0.494    0.506\n 7      0.495    0.505\n 8      0.487    0.513\n 9      0.491    0.509\n10      0.484    0.516\n# ℹ 242 more rows\n```\n:::\n:::\n\n\nAnd we can take a look at the performance. \n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n```\n:::\n\n```{.r .cell-code}\naugment(lda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560\n```\n:::\n:::\n\n\nAnd we see no markedly different performance between this model and the logistic regression model.\n\n## Quadratic Discriminant Analysis\n\nWe will now fit a QDA model. The `discrim_quad()` function is used here.\n\nOnce we have the model specification fitting the model is just like before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nqda_fit <- qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(qda_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down  Up\n      Down   30  20\n      Up     81 121\n```\n:::\n\n```{.r .cell-code}\naugment(qda_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.599\n```\n:::\n:::\n\n\nAnd we are seeing another increase in accuracy. However this model still rarely predicts `\"Down\"`. This make it appear that the quadratic form assumed by QDA captures the relationship more clearly.\n\n## Naive Bayes\n\nWe will now fit a Naive Bayes model to the `Smarket` data. For this, we will be using the `naive_Bayes()` function to create the specification and also set the `usekernel` argument to `FALSE`. This means that we are assuming that the predictors `Lag1` and `Lag2` are drawn from Gaussian distributions.\n\nOnce the model is specified, the fitting process is exactly like before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_spec <- naive_Bayes() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"klaR\") %>% \n  set_args(usekernel = FALSE)  \n\nnb_fit <- nb_spec %>% \n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n```\n:::\n\n\nOnce the model is fit, we can create the confusion matrix based on the testing data and also assess the model accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(nb_fit, new_data = Smarket_test) %>% \n  conf_mat(truth = Direction, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down  Up\n      Down   28  20\n      Up     83 121\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(nb_fit, new_data = Smarket_test) %>% \n  accuracy(truth = Direction, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.591\n```\n:::\n:::\n\n\nThe accuracy of the Naive Bayes is very similar to that of the QDA model. This seems reasonable since the below scatter plot shows that there is no apparent relationship between `Lag1` vs `Lag2` and thus the Naive Bayes' assumption of independently distributed predictors is not unreasonable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Smarket, aes(Lag1, Lag2)) +\n  geom_point(alpha = 0.1, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"No apparent correlation between Lag1 and Lag2\")\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-30-1.png){fig-alt='Scatter chart. Lag1 along the x-axis and Lag2 along the\ny-axis. No apparent correlation between Lag1 and Lag2.' width=672}\n:::\n:::\n\n\n## K-Nearest Neighbors\n\nLastly let us take a look at a K-Nearest Neighbors model. This is the first model we have looked at that has a hyperparameter we need to specify. I have set it to 3 with `neighbors = 3`. Fitting is done like normal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\nknn_fit <- knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)\n\nknn_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.492986\nBest kernel: optimal\nBest k: 3\n```\n:::\n:::\n\n\nAnd evaluation is done the same way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(knn_fit, new_data = Smarket_test) %>%\n  conf_mat(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction Down Up\n      Down   43 58\n      Up     68 83\n```\n:::\n\n```{.r .cell-code}\naugment(knn_fit, new_data = Smarket_test) %>%\n  accuracy(truth = Direction, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary           0.5\n```\n:::\n:::\n\n\nIt appears that this model is not performing that well.\n\nWe will try using a K-nearest neighbors model in an application to caravan insurance data. This data set includes 85 predictors that measure demographic characteristics for 5822 individuals. The response variable is `Purchase`, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\n\nWe want to build a predictive model that uses the demographic characteristics to predict whether an individual is going to purchase a caravan insurance. Before we go on, we split the data set into a training data set and testing data set. (This is a not the proper way this should be done. See next chapter for the correct way.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCaravan_test <- Caravan[seq_len(1000), ]\nCaravan_train <- Caravan[-seq_len(1000), ]\n```\n:::\n\n\nSince we are using a K-nearest neighbor model, it is importance that the variables are centered and scaled to make sure that the variables have a uniform influence. We can accomplish this transformation with `step_normalize()`, which does centering and scaling in one go.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\nWe will be trying different values of K to see how the number of neighbors affect the model performance. A workflow object is created, with just the recipe added.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCaravan_wf <- workflow() %>%\n  add_recipe(rec_spec)\n```\n:::\n\n\nNext we create a general KNN model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_spec <- nearest_neighbor() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n```\n:::\n\n\nWe can then use this model specification along with `Caravan_wf` to create 3 full workflow objects for `K = 1,3,5`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn1_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 1))\n\nknn3_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 3))\n\nknn5_wf <- Caravan_wf %>%\n  add_model(knn_spec %>% set_args(neighbors = 5))\n```\n:::\n\n\nWith all these workflow specification we can fit all the models one by one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn1_fit <- fit(knn1_wf, data = Caravan_train)\nknn3_fit <- fit(knn3_wf, data = Caravan_train)\nknn5_fit <- fit(knn5_wf, data = Caravan_train)\n```\n:::\n\n\nAnd we can calculate all the confusion matrices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(knn1_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  No Yes\n       No  874  50\n       Yes  67   9\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(knn3_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  No Yes\n       No  875  50\n       Yes  66   9\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(knn5_fit, new_data = Caravan_test) %>%\n  conf_mat(truth = Purchase, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  No Yes\n       No  874  50\n       Yes  67   9\n```\n:::\n:::\n\n\nAnd it appears that the model performance doesn't change much when changing from 1 to 5.\n\n## Poisson Regression\n\nSo far we have been using the `Smarket` data set to predict the stock price movement. We will now shift to a new data set, `Bikeshare`, and look at the number of bike rentals per hour in Washington, D.C.\n\nThe variable of interest, *number of bike rentals per hour*, can take on non-negative integer values. This makes Poisson Regression a suitable candidate to model the same.\n\nWe start with specifying the model using the `poisson_reg()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_spec <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\n```\n:::\n\n\nHere we will be predicting `bikers` using the following predictors:\n\n* `mnth` - month of the year, coded as a factor\n* `hr` - hour of the day, coded as a factor from 0 to 23\n* `workingday` - Is it a workday? Already coded as a dummy variable with Yes = 1, No = 0\n* `temp` - normalized temperature in Celsius\n* `weathersit` - weather condition, again coded as a factor with the following levels:\n \n  * clear\n  * cloudy/misty\n  * light rain/snow\n  * heavy rain/snow\n\nAs we can see, apart from `temp` all other predictors are categorical in nature. Thus, we will first create a recipe to convert these into dummy variables and then bundle the model spec and recipe using a workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_rec_spec <- recipe(\n  bikers ~ mnth + hr + workingday + temp + weathersit,\n  data = Bikeshare\n) %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npois_wf <- workflow() %>% \n  add_recipe(pois_rec_spec) %>% \n  add_model(pois_spec)\n```\n:::\n\n\nWith the workflow in place, we follow the same pattern to fit the model and look at the predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit <- pois_wf %>% fit(data = Bikeshare)\n\naugment(pois_fit, new_data = Bikeshare, type.predict = \"response\") %>% \n  ggplot(aes(bikers, .pred)) +\n  geom_point(alpha = 0.1) +\n  geom_abline(slope = 1, size = 1, color = \"grey40\") +\n  labs(title = \"Predicting the number of bikers per hour using Poission Regression\",\n       x = \"Actual\", y = \"Predicted\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-45-1.png){fig-alt='Scatter chart. bikers along the x-axis and .pred along\nthe y-axis. A diagonal line has been added. Points are\nscattered around the diagonal. More closely for low values.' width=672}\n:::\n:::\n\n\nWe can also look at the model coefficients to get a feel for the working of the model and comparing it with our own understanding.\n\nLooking at the coefficients corresponding to the `mnth` variable, we note that it is lower in the winter months and higher in the summer months. This seems logical as we would expect the number of bike rentals to be higher during summertime.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit_coef_mnths <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^mnth\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"mnth_\", \"\"),\n    term = forcats::fct_inorder(term)\n  ) \n\npois_fit_coef_mnths %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"Month\", y = \"Coefficient\")\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-46-1.png){fig-alt='Line chart. months along the x-axis, coefficient along the\ny-axis. Coefficient values for January and February are low\nRest of the months are high.' width=672}\n:::\n:::\n\n\nWe can similarly also look at the coefficients corresponding to the `hr` variable. Here the peaks occur at 8:00 AM and 5:00 PM, i.e. during normal office start and end times. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit_coef_hr <- \n  tidy(pois_fit) %>% \n  filter(grepl(\"^hr\", term)) %>% \n  mutate(\n    term = stringr::str_replace(term, \"hr_X\", \"\"),\n    term = forcats::fct_inorder(term)\n  )\n\npois_fit_coef_hr %>% \n  ggplot(aes(term, estimate)) +\n  geom_line(group = 1) +\n  geom_point(shape = 21, size = 3, stroke = 1.5, \n             fill = \"black\", color = \"white\") +\n  labs(title = \"Coefficient value from Poission Regression\",\n       x = \"hours\", y = \"Coefficient\")\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-47-1.png){fig-alt='Line chart. hours along the x-axis, coefficient along the\ny-axis. Coefficient values for hour between 1 and 7 are\nlow, the rest are higher.' width=672}\n:::\n:::\n\n\n## Extra - comparing multiple models\n\nThis section is new and not part of ISLR. We have fitted a lot of different models in this lab. And we were able to calculate the performance metrics one by one, but it is not ideal if we want to compare the different models. Below is an example of how you can more conveniently calculate performance metrics for multiple models at the same time.\n\nStart of by creating a named list of the fitted models you want to evaluate. I have made sure only to include models that were fitted on the same parameters to make it easier to compare them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(\"logistic regression\" = lr_fit3,\n               \"LDA\" = lda_fit,\n               \"QDA\" = qda_fit,\n               \"KNN\" = knn_fit)\n```\n:::\n\n\nNext use `imap_dfr()` from the [purrr](https://purrr.tidyverse.org/) package to apply `augment()` to each of the models using the testing data set. `.id = \"model\"` creates a column named `\"model\"` that is added to the resulting tibble using the names of `models`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- imap_dfr(models, augment, \n                  new_data = Smarket_test, .id = \"model\")\n\npreds %>%\n  select(model, Direction, .pred_class, .pred_Down, .pred_Up)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,008 × 5\n   model               Direction .pred_class .pred_Down .pred_Up\n   <chr>               <fct>     <fct>            <dbl>    <dbl>\n 1 logistic regression Down      Up               0.490    0.510\n 2 logistic regression Down      Up               0.479    0.521\n 3 logistic regression Down      Up               0.467    0.533\n 4 logistic regression Up        Up               0.474    0.526\n 5 logistic regression Down      Up               0.493    0.507\n 6 logistic regression Up        Up               0.494    0.506\n 7 logistic regression Down      Up               0.495    0.505\n 8 logistic regression Up        Up               0.487    0.513\n 9 logistic regression Down      Up               0.491    0.509\n10 logistic regression Up        Up               0.484    0.516\n# ℹ 998 more rows\n```\n:::\n:::\n\n\nWe have seen how to use `accuracy()` a lot of times by now, but it is not the only metric to use for classification, and yardstick provides [many more](https://yardstick.tidymodels.org/reference/index.html#section-classification-metrics).\nYou can combine multiple different metrics together with `metric_set()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_metric <- metric_set(accuracy, sensitivity, specificity)\n```\n:::\n\n\nand then the resulting function can be applied to calculate multiple metrics at the same time. All of the yardstick works with grouped tibbles so by calling `group_by(model)` we can calculate the metrics for each of the models in one go.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds %>%\n  group_by(model) %>%\n  multi_metric(truth = Direction, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 × 4\n   model               .metric     .estimator .estimate\n   <chr>               <chr>       <chr>          <dbl>\n 1 KNN                 accuracy    binary         0.5  \n 2 LDA                 accuracy    binary         0.560\n 3 QDA                 accuracy    binary         0.599\n 4 logistic regression accuracy    binary         0.560\n 5 KNN                 sensitivity binary         0.387\n 6 LDA                 sensitivity binary         0.315\n 7 QDA                 sensitivity binary         0.270\n 8 logistic regression sensitivity binary         0.315\n 9 KNN                 specificity binary         0.589\n10 LDA                 specificity binary         0.752\n11 QDA                 specificity binary         0.858\n12 logistic regression specificity binary         0.752\n```\n:::\n:::\n\n\nThe same technique can be used to create ROC curves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds %>%\n  group_by(model) %>%\n  roc_curve(Direction, .pred_Down) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](04-classification_files/figure-html/unnamed-chunk-52-1.png){fig-alt='A ROC curve plot. 1-specificity along the x-axis and\nsensitivity along the y-axis. A dotted line is drawn\nalong the diagonal. One curve for each of the 4 models\nKNN, LDA, QDA and Logistic regression is drawn.\nThe curves are all fairly close th the diagonal for\nall models with KNN doing the absolutely worst.' width=672}\n:::\n:::\n\n\nHere you can't see the LDA because it lies perfectly under the logistic regression.\n",
    "supporting": [
      "04-classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}