{
  "hash": "ca12536bbb3f02d206522fa612fcc8a6",
  "result": {
    "markdown": "# Tree-Based Methods\n\n\n::: {.cell}\n\n:::\n\n\nThis lab will take a look at different tree-based models, in doing so we will explore how changing the hyperparameters can help improve performance. \nThis chapter will use [parsnip](https://www.tidymodels.org/start/models/) for model fitting and [recipes and workflows](https://www.tidymodels.org/start/recipes/) to perform the transformations, and [tune and dials](https://www.tidymodels.org/start/tuning/) to tune the hyperparameters of the model. `rpart.plot` is used to visualize the decision trees created using the `rpart` package as engine, and `vip` is used to visualize variable importance for later models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ISLR)\nlibrary(rpart.plot)\nlibrary(vip)\n\ndata(\"Boston\", package = \"MASS\")\n\nBoston <- as_tibble(Boston)\n```\n:::\n\n\nThe `Boston` data set contain various statistics for 506 neighborhoods in Boston. We will build a regression model that related the median value of owner-occupied homes (`medv`) as the response with the remaining variables as predictors. \n\n:::{.callout-important}\nThe `Boston` data set is quite outdated and contains some really unfortunate variables.\n:::\n\nWe will also use the `Carseats` data set from the `ISLR` package to demonstrate a classification model. We create a new variable `High` to denote if `Sales <= 8`, then the `Sales` predictor is removed as it is a perfect predictor of `High`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCarseats <- as_tibble(Carseats) %>%\n  mutate(High = factor(if_else(Sales <= 8, \"No\", \"Yes\"))) %>%\n  select(-Sales)\n```\n:::\n\n\n## Fitting Classification Trees\n\nWe will both be fitting a classification and regression tree in this section, so we can save a little bit of typing by creating a general decision tree specification using `rpart` as the engine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\")\n```\n:::\n\n\nThen this decision tree specification can be used to create a classification decision tree engine. This is a good example of how the flexible composition system created by parsnip can be used to create multiple model specifications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_spec <- tree_spec %>%\n  set_mode(\"classification\")\n```\n:::\n\n\nWith both a model specification and our data are we ready to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_fit <- class_tree_spec %>%\n  fit(High ~ ., data = Carseats)\n```\n:::\n\n\nWhen we look at the model output we see a quite informative summary of the model. It tries to give a written description of the tree that is created.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nn= 400 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 164 No (0.59000000 0.41000000)  \n    2) ShelveLoc=Bad,Medium 315  98 No (0.68888889 0.31111111)  \n      4) Price>=92.5 269  66 No (0.75464684 0.24535316)  \n        8) Advertising< 13.5 224  41 No (0.81696429 0.18303571)  \n         16) CompPrice< 124.5 96   6 No (0.93750000 0.06250000) *\n         17) CompPrice>=124.5 128  35 No (0.72656250 0.27343750)  \n           34) Price>=109.5 107  20 No (0.81308411 0.18691589)  \n             68) Price>=126.5 65   6 No (0.90769231 0.09230769) *\n             69) Price< 126.5 42  14 No (0.66666667 0.33333333)  \n              138) Age>=49.5 22   2 No (0.90909091 0.09090909) *\n              139) Age< 49.5 20   8 Yes (0.40000000 0.60000000) *\n           35) Price< 109.5 21   6 Yes (0.28571429 0.71428571) *\n        9) Advertising>=13.5 45  20 Yes (0.44444444 0.55555556)  \n         18) Age>=54.5 20   5 No (0.75000000 0.25000000) *\n         19) Age< 54.5 25   5 Yes (0.20000000 0.80000000) *\n      5) Price< 92.5 46  14 Yes (0.30434783 0.69565217)  \n       10) Income< 57 10   3 No (0.70000000 0.30000000) *\n       11) Income>=57 36   7 Yes (0.19444444 0.80555556) *\n    3) ShelveLoc=Good 85  19 Yes (0.22352941 0.77647059)  \n      6) Price>=142.5 12   3 No (0.75000000 0.25000000) *\n      7) Price< 142.5 73  10 Yes (0.13698630 0.86301370) *\n```\n:::\n:::\n\n\nOnce the tree gets more than a couple of nodes it can become hard to read the printed diagram. The `rpart.plot` package provides functions to let us easily visualize the decision tree. As the name implies, it only works with `rpart` trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-8-1.png){fig-alt='Decision tree chart. A total of 10 splits. Color is used to\nrepresent the prediction of High, blue values represent No\ngreen represent Yes.' width=672}\n:::\n:::\n\n\nWe can see that the most important variable to predict high sales appears to be shelving location as it forms the first node.\n\nThe training accuracy of this model is 85%\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(class_tree_fit, new_data = Carseats) %>%\n  accuracy(truth = High, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.848\n```\n:::\n:::\n\n\nLet us take a look at the confusion matrix to see if the balance is there\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(class_tree_fit, new_data = Carseats) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  No Yes\n       No  200  25\n       Yes  36 139\n```\n:::\n:::\n\n\nAnd the model appears to work well overall. But this model was fit on the whole data set so we only get the training accuracy which could be misleading if the model is overfitting. Let us redo the fitting by creating a validation split and fit the model on the training data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nCarseats_split <- initial_split(Carseats)\n\nCarseats_train <- training(Carseats_split)\nCarseats_test <- testing(Carseats_split)\n```\n:::\n\n\nNow we can fit the model on the training data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_fit <- fit(class_tree_spec, High ~ ., data = Carseats_train)\n```\n:::\n\n\nLet us take a look at the confusion matrix for the training data set and testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(class_tree_fit, new_data = Carseats_train) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  No Yes\n       No  159  21\n       Yes  21  99\n```\n:::\n:::\n\n\nThe training data set performs well as we would expect\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  conf_mat(truth = High, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction No Yes\n       No  41   8\n       Yes 15  36\n```\n:::\n:::\n\n\nbut the testing data set doesn't perform just as well and get a smaller accuracy of 77%\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(class_tree_fit, new_data = Carseats_test) %>%\n  accuracy(truth = High, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary          0.77\n```\n:::\n:::\n\n\nLet us try to tune the `cost_complexity` of the decision tree to find a more optimal complexity. We use the `class_tree_spec` object and use the `set_args()` function to specify that we want to tune `cost_complexity`. This is then passed directly into the workflow object to avoid creating an intermediate object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_wf <- workflow() %>%\n  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(High ~ .)\n```\n:::\n\n\nTo be able to tune the variable we need 2 more objects. S `resamples` object, we will use a k-fold cross-validation data set, and a `grid` of values to try. Since we are only tuning 1 hyperparameter it is fine to stay with a regular grid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nCarseats_fold <- vfold_cv(Carseats_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  class_tree_wf, \n  resamples = Carseats_fold, \n  grid = param_grid, \n  metrics = metric_set(accuracy)\n)\n```\n:::\n\n\nusing `autoplot()` shows which values of `cost_complexity` appear to produce the highest accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tune_res)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-18-1.png){fig-alt='Connected scatter chart. Cost-complexity along the x-axis,\naccuracy along the y-axis. The accuracy stays constant for\nlow values of cost-complexity. When cost-complexity is larger\nthan 0.01 the accuracy shoots up and down rapidly.' width=672}\n:::\n:::\n\n\nWe can now select the best performing value with `select_best()`, finalize the workflow by updating the value of `cost_complexity` and fit the model on the full training data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_complexity <- select_best(tune_res)\n\nclass_tree_final <- finalize_workflow(class_tree_wf, best_complexity)\n\nclass_tree_final_fit <- fit(class_tree_final, data = Carseats_train)\nclass_tree_final_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nHigh ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 300 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 300 120 No (0.6000000 0.4000000)  \n  2) ShelveLoc=Bad,Medium 242  73 No (0.6983471 0.3016529)  \n    4) Price>=92.5 213  51 No (0.7605634 0.2394366) *\n    5) Price< 92.5 29   7 Yes (0.2413793 0.7586207) *\n  3) ShelveLoc=Good 58  11 Yes (0.1896552 0.8103448) *\n```\n:::\n:::\n\n\nAt last, we can visualize the model, and we see that the better-performing model is less complex than the original model we fit. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-20-1.png){fig-alt='Decision tree chart. A total of 2 splits. Color is used to\nrepresent the prediction of High, blue values represent No\ngreen represent Yes.' width=672}\n:::\n:::\n\n\n## Fitting Regression Trees\n\nWe will now show how we fit a regression tree. This is very similar to what we saw in the last section. The main difference here is that the response we are looking at will be continuous instead of categorical. We can reuse `tree_spec` as a base for the regression decision tree specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_tree_spec <- tree_spec %>%\n  set_mode(\"regression\")\n```\n:::\n\n\nWe are using the `Boston` data set here so we will do a validation split here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nBoston_split <- initial_split(Boston)\n\nBoston_train <- training(Boston_split)\nBoston_test <- testing(Boston_split)\n```\n:::\n\n\nfitting the model to the training data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_tree_fit <- fit(reg_tree_spec, medv ~ ., Boston_train)\nreg_tree_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.9500 22.54802  \n   2) rm< 6.941 320 13602.3100 19.86281  \n     4) lstat>=14.395 129  2582.1090 14.51550  \n       8) nox>=0.607 80   984.7339 12.35875  \n        16) lstat>=19.34 47   388.6332 10.35957 *\n        17) lstat< 19.34 33   140.7188 15.20606 *\n       9) nox< 0.607 49   617.6939 18.03673 *\n     5) lstat< 14.395 191  4840.3640 23.47435  \n      10) rm< 6.543 151  2861.3990 22.21192  \n        20) dis>=1.68515 144  1179.5970 21.82083 *\n        21) dis< 1.68515 7  1206.6970 30.25714 *\n      11) rm>=6.543 40   829.8560 28.24000 *\n   3) rm>=6.941 59  4199.1020 37.11186  \n     6) rm< 7.437 35  1012.4100 32.08286 *\n     7) rm>=7.437 24  1010.6200 44.44583  \n      14) ptratio>=15.4 12   585.0767 40.71667 *\n      15) ptratio< 15.4 12    91.7825 48.17500 *\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(reg_tree_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        4.78\n```\n:::\n:::\n\n\nand the `rpart.plot()` function works for the regression decision tree as well\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_tree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-25-1.png){fig-alt='Decision tree chart. A total of 8 splits. Color is used to\nrepresent medv. Light blue colors represent small values, dark\nblue represent high values.' width=672}\n:::\n:::\n\n\nNotice how the result is a numeric variable instead of a class.\n\nNow let us again try to tune the `cost_complexity` to find the best performing model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_tree_wf <- workflow() %>%\n  add_model(reg_tree_spec %>% set_args(cost_complexity = tune())) %>%\n  add_formula(medv ~ .)\n\nset.seed(1234)\nBoston_fold <- vfold_cv(Boston_train)\n\nparam_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)\n\ntune_res <- tune_grid(\n  reg_tree_wf, \n  resamples = Boston_fold, \n  grid = param_grid\n)\n```\n:::\n\n\nAnd it appears that higher complexity works are to be preferred according to our cross-validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tune_res)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-27-1.png){fig-alt='Facetted connected scatter chart. Cost-complexity along the\nx-axis. Performance values along the y-axis. The facets are\nrmse and rsq. Both are fairly constant for low values of\ncost-complexity, rmse starts moderately increasing and rsq\nstarts moderately decreasing once the cost-complexity\ngets larger.' width=672}\n:::\n:::\n\n\nWe select the best-performing model according to `\"rmse\"` and fit the final model on the whole training data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_complexity <- select_best(tune_res, metric = \"rmse\")\n\nreg_tree_final <- finalize_workflow(reg_tree_wf, best_complexity)\n\nreg_tree_final_fit <- fit(reg_tree_final, data = Boston_train)\nreg_tree_final_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmedv ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 379 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 379 32622.95000 22.54802  \n   2) rm< 6.941 320 13602.31000 19.86281  \n     4) lstat>=14.395 129  2582.10900 14.51550  \n       8) nox>=0.607 80   984.73390 12.35875  \n        16) lstat>=19.34 47   388.63320 10.35957  \n          32) tax>=551.5 40   243.94980  9.67750 *\n          33) tax< 551.5 7    19.73714 14.25714 *\n        17) lstat< 19.34 33   140.71880 15.20606 *\n       9) nox< 0.607 49   617.69390 18.03673  \n        18) crim>=0.381565 25   313.20000 16.20000 *\n        19) crim< 0.381565 24   132.30000 19.95000 *\n     5) lstat< 14.395 191  4840.36400 23.47435  \n      10) rm< 6.543 151  2861.39900 22.21192  \n        20) dis>=1.68515 144  1179.59700 21.82083  \n          40) rm< 6.062 56   306.22860 20.28571 *\n          41) rm>=6.062 88   657.41950 22.79773  \n            82) lstat>=9.98 35    98.32686 21.02571 *\n            83) lstat< 9.98 53   376.61550 23.96792 *\n        21) dis< 1.68515 7  1206.69700 30.25714 *\n      11) rm>=6.543 40   829.85600 28.24000  \n        22) lstat>=4.44 33   274.06180 27.15455 *\n        23) lstat< 4.44 7   333.61710 33.35714 *\n   3) rm>=6.941 59  4199.10200 37.11186  \n     6) rm< 7.437 35  1012.41000 32.08286  \n      12) nox>=0.4885 14   673.46930 28.89286 *\n      13) nox< 0.4885 21   101.49810 34.20952 *\n     7) rm>=7.437 24  1010.62000 44.44583  \n      14) ptratio>=15.4 12   585.07670 40.71667 *\n      15) ptratio< 15.4 12    91.78250 48.17500 *\n```\n:::\n:::\n\n\nVisualizing the model reveals a much more complex tree than what we saw in the last section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_tree_final_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-29-1.png){fig-alt='Decision tree chart. A total of 14 splits. Color is used to\nrepresent medv. Light blue colors represent small values, dark\nblue represent high values.' width=672}\n:::\n:::\n\n\n## Bagging and Random Forests\n\nHere we apply bagging and random forests to the `Boston` data set. We will be using the `randomForest` package as the engine. A bagging model is the same as a random forest where `mtry` is equal to the number of predictors. We can specify the `mtry` to be `.cols()` which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and useable to many different data sets. `.cols()` is one of many [descriptors](https://parsnip.tidymodels.org/reference/descriptors.html) in the parsnip package.\nWe also set `importance = TRUE` in `set_engine()` to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the `vip` package later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbagging_spec <- rand_forest(mtry = .cols()) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\n```\n:::\n\n\nWe fit the model like normal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbagging_fit <- fit(bagging_spec, medv ~ ., data = Boston_train)\n```\n:::\n\n\nand we take a look at the testing performance. Which we see is an improvement over the decision tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(bagging_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.43\n```\n:::\n:::\n\n\nWe can also create a quick scatterplot between the true and predicted value to see if we can make any diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(bagging_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-33-1.png){fig-alt='Scatter chart. medv along the x-axis and .pred along the\ny-axis. A diagonal line have been added, most of the points\nfollows fairly close to the line, with points for high values\nof medv being under the line.' width=672}\n:::\n:::\n\n\nThere isn't anything weird going on here so we are happy. Next, let us take a look at the variable importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvip(bagging_fit)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-34-1.png){fig-alt='Horizontal bar chart. Importance along the x-axis, predictors\nalong the y-axis. Highest values are rm, lstat, nox and dis.\nLowest are indus, black and age.' width=672}\n:::\n:::\n\n\nNext, let us take a look at a random forest. By default, `randomForest()` `p / 3` variables when building a random forest of regression trees, and `sqrt(p)` variables when building a random forest of classification trees. Here we use `mtry = 6`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mtry = 6) %>%\n  set_engine(\"randomForest\", importance = TRUE) %>%\n  set_mode(\"regression\")\n```\n:::\n\n\nand fitting the model like normal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- fit(rf_spec, medv ~ ., data = Boston_train)\n```\n:::\n\n\nthis model has a slightly better performance than the bagging model\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(rf_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.26\n```\n:::\n:::\n\n\nWe can likewise plot the true value against the predicted value\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(rf_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-38-1.png){fig-alt='Scatter chart. medv along the x-axis and .pred along the\ny-axis. A diagonal line have been added, most of the points\nfollows fairly close to the line, with points for high values\nof medv being under the line.' width=672}\n:::\n:::\n\n\nit looks fine. No discernible difference between this chart and the one we created for the bagging model. \n\nThe variable importance plot is also quite similar to what we saw for the bagging model which isn't surprising. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvip(rf_fit)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-39-1.png){fig-alt='Horizontal bar chart. Importance along the x-axis, predictors\nalong the y-axis. Highest values are rm, lstat, nox and dis.\nLowest are black, indus and age.' width=672}\n:::\n:::\n\n\nyou would normally want to perform hyperparameter tuning for the random forest model to get the best out of your forest. This exercise is left for the reader.\n\n## Boosting\n\nWe will now fit a boosted tree model. The `xgboost` packages give a good implementation of boosted trees. It has many parameters to tune and we know that setting `trees` too high can lead to overfitting. Nevertheless, let us try fitting a boosted tree. We set `tree = 5000` to grow 5000 trees with a maximal depth of 4 by setting `tree_depth = 4`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboost_spec <- boost_tree(trees = 5000, tree_depth = 4) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n\nfitting the model like normal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboost_fit <- fit(boost_spec, medv ~ ., data = Boston_train)\n```\n:::\n\n\nand the `rmse` is a little high in this case which is properly because we didn't tune any of the parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(boost_fit, new_data = Boston_test) %>%\n  rmse(truth = medv, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        3.34\n```\n:::\n:::\n\n\nWe can look at the scatterplot and we don't see anything weird going on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(boost_fit, new_data = Boston_test) %>%\n  ggplot(aes(medv, .pred)) +\n  geom_abline() +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](08-tree-based-methods_files/figure-html/unnamed-chunk-43-1.png){fig-alt='Scatter chart. medv along the x-axis and .pred along the\ny-axis. A diagonal line have been added, most of the points\nfollows fairly close to the line, with points for high values\nof medv being under the line.' width=672}\n:::\n:::\n\n\nYou would normally want to perform hyperparameter tuning for the boosted tree model to get the best out of your model. This exercise is left for the reader. Look at the [Iterative search](https://www.tmwr.org/iterative-search.html) chapter of [Tidy Modeling with R](https://www.tmwr.org/) for inspiration.\n\n## Bayesian Additive Regression Trees\n\nThis section is WIP.\n",
    "supporting": [
      "08-tree-based-methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}