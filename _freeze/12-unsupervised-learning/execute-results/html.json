{
  "hash": "2cf5f8cea963e43ca401d30986205f28",
  "result": {
    "markdown": "# Unsupervised Learning\n\n\n::: {.cell}\n\n:::\n\n\nThis final chapter talks about unsupervised learning. This is broken into two parts. Dimensionality reduction and clustering. Dimensionality reduction will be handled mostly as a preprocessor which is done with [recipes](https://recipes.tidymodels.org/) package, and clustering is done with the [tidyclust](https://github.com/emilhvitfeldt/tidyclust) package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyclust)\nlibrary(factoextra)\nlibrary(patchwork)\nlibrary(proxy)\nlibrary(ISLR)\n```\n:::\n\n\n## Principal Components Analysis\n\nThis section will be used to explore the `USArrests` data set using PCA. Before we move on, let is turn `USArrests` into a tibble and move the rownames into a column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests <- as_tibble(USArrests, rownames = \"state\")\nUSArrests\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 5\n   state       Murder Assault UrbanPop  Rape\n   <chr>        <dbl>   <int>    <int> <dbl>\n 1 Alabama       13.2     236       58  21.2\n 2 Alaska        10       263       48  44.5\n 3 Arizona        8.1     294       80  31  \n 4 Arkansas       8.8     190       50  19.5\n 5 California     9       276       91  40.6\n 6 Colorado       7.9     204       78  38.7\n 7 Connecticut    3.3     110       77  11.1\n 8 Delaware       5.9     238       72  15.8\n 9 Florida       15.4     335       80  31.9\n10 Georgia       17.4     211       60  25.8\n# … with 40 more rows\n```\n:::\n:::\n\n\nNotice how the mean of each of the variables is quite different. if we were to apply PCA directly to the data set then `Murder` would have a very small influence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests %>%\n  select(-state) %>%\n  map_dfr(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  Murder Assault UrbanPop  Rape\n   <dbl>   <dbl>    <dbl> <dbl>\n1   7.79    171.     65.5  21.2\n```\n:::\n:::\n\n\nWe will show how to perform PCA in two different ways in this section. Firstly, by using `prcomp()` directly, using `broom::tidy()` to extract the information we need, and secondly by using recipes.\n`prcomp()` takes 1 required argument `x` which much be a fully numeric data.frame or matrix. Then we pass that to `prcomp()`. We also set `scale = TRUE` in `prcomp()` which will perform the scaling we need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSArrests_pca <- USArrests %>%\n  select(-state) %>%\n  prcomp(scale = TRUE)\n\nUSArrests_pca\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n```\n:::\n:::\n\n\nNow we can use our favorite broom function to extract information from this `prcomp` object. \nWe start with `tidy()`. `tidy()` can be used to extract a couple of different things, see `?broom:::tidy.prcomp()` for more information. `tidy()` will by default extract the scores of a PCA object in long tidy format. The score is the location of the observation in PCA space. So we can \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 × 3\n     row    PC  value\n   <int> <dbl>  <dbl>\n 1     1     1 -0.976\n 2     1     2  1.12 \n 3     1     3 -0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2  1.06 \n 7     2     3  2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2 -0.738\n# … with 190 more rows\n```\n:::\n:::\n\n\nWe can also explicitly say we want the scores by setting `matrix = \"scores\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca, matrix = \"scores\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 × 3\n     row    PC  value\n   <int> <dbl>  <dbl>\n 1     1     1 -0.976\n 2     1     2  1.12 \n 3     1     3 -0.440\n 4     1     4  0.155\n 5     2     1 -1.93 \n 6     2     2  1.06 \n 7     2     3  2.02 \n 8     2     4 -0.434\n 9     3     1 -1.75 \n10     3     2 -0.738\n# … with 190 more rows\n```\n:::\n:::\n\n\nNext, we can get the loadings of the PCA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca, matrix = \"loadings\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 × 3\n   column      PC   value\n   <chr>    <dbl>   <dbl>\n 1 Murder       1 -0.536 \n 2 Murder       2  0.418 \n 3 Murder       3 -0.341 \n 4 Murder       4  0.649 \n 5 Assault      1 -0.583 \n 6 Assault      2  0.188 \n 7 Assault      3 -0.268 \n 8 Assault      4 -0.743 \n 9 UrbanPop     1 -0.278 \n10 UrbanPop     2 -0.873 \n11 UrbanPop     3 -0.378 \n12 UrbanPop     4  0.134 \n13 Rape         1 -0.543 \n14 Rape         2 -0.167 \n15 Rape         3  0.818 \n16 Rape         4  0.0890\n```\n:::\n:::\n\n\nThis information tells us how each variable contributes to each principal component. If you don't have too many principal components you can visualize the contribution without filtering\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca, matrix = \"loadings\") %>%\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col() +\n  scale_x_continuous(labels = scales::percent)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-9-1.png){fig-alt='Facetted barchart of the principal component loadings.\nThe 4 variables are shown across the y-axis and the amount\nof the loading is show as the bar height across the x-axis.\nThe 4 variables: UnbanPop, Rape, Murder and Assault are more\nor less evenly represented in the first loading, with\nUnbanPop least. Second loading has UnbanPop highest, third\nloading has Rape highest. Murder and Assult highest in forth\nand final loading.' width=672}\n:::\n:::\n\n\nLastly, we can set `matrix = \"eigenvalues\"` and get back the explained standard deviation for each PC including as a percent and cumulative which is quite handy for plotting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca, matrix = \"eigenvalues\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n     PC std.dev percent cumulative\n  <dbl>   <dbl>   <dbl>      <dbl>\n1     1   1.57   0.620       0.620\n2     2   0.995  0.247       0.868\n3     3   0.597  0.0891      0.957\n4     4   0.416  0.0434      1    \n```\n:::\n:::\n\n\nIf we want to see how the percent standard deviation explained drops off for each PC we can easily get that by using `tidy()` with `matrix = \"eigenvalues\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(USArrests_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-11-1.png){fig-alt='Bar chart of percent standard deviation explained for the\n4 principal components. First PC is a little over 60%, second\nis at around 25%, third is a little under 10% and forth is at\naround 5%.' width=672}\n:::\n:::\n\n\nLastly, we have the `augment()` function which will give you back the fitted PC transformation if you apply it to the `prcomp()` object directly\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(USArrests_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 5\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4\n   <chr>          <dbl>      <dbl>      <dbl>      <dbl>\n 1 1            -0.976      1.12      -0.440     0.155  \n 2 2            -1.93       1.06       2.02     -0.434  \n 3 3            -1.75      -0.738      0.0542   -0.826  \n 4 4             0.140      1.11       0.113    -0.181  \n 5 5            -2.50      -1.53       0.593    -0.339  \n 6 6            -1.50      -0.978      1.08      0.00145\n 7 7             1.34      -1.08      -0.637    -0.117  \n 8 8            -0.0472    -0.322     -0.711    -0.873  \n 9 9            -2.98       0.0388    -0.571    -0.0953 \n10 10           -1.62       1.27      -0.339     1.07   \n# … with 40 more rows\n```\n:::\n:::\n\n\nand will apply this transformation to new data by passing the new data to `newdata`\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(USArrests_pca, newdata = USArrests[1:5, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 10\n  .rownames state Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3\n  <chr>     <chr>  <dbl>   <int>    <int> <dbl>      <dbl>      <dbl>      <dbl>\n1 1         Alab…   13.2     236       58  21.2     -0.976      1.12     -0.440 \n2 2         Alas…   10       263       48  44.5     -1.93       1.06      2.02  \n3 3         Ariz…    8.1     294       80  31       -1.75      -0.738     0.0542\n4 4         Arka…    8.8     190       50  19.5      0.140      1.11      0.113 \n5 5         Cali…    9       276       91  40.6     -2.50      -1.53      0.593 \n# … with 1 more variable: .fittedPC4 <dbl>\n```\n:::\n:::\n\n\nIf you are using PCA as a preprocessing method I recommend you use recipes to apply the PCA transformation. This is a good way of doing it since recipe will correctly apply the same transformation to new data that the recipe is used on.\n\nWe `step_normalize()` to make sure all the variables are on the same scale. By using `all_numeric()` we are able to apply PCA on the variables we want without having to remove `state`. We are also setting an `id` for `step_pca()` to make it easier to `tidy()` later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_rec <- recipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), id = \"pca\") %>%\n  prep()\n```\n:::\n\n\nBy calling `bake(new_data = NULL)` we can get the fitted PC transformation of our numerical variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_rec %>%\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 5\n   state           PC1     PC2     PC3      PC4\n   <fct>         <dbl>   <dbl>   <dbl>    <dbl>\n 1 Alabama     -0.976   1.12   -0.440   0.155  \n 2 Alaska      -1.93    1.06    2.02   -0.434  \n 3 Arizona     -1.75   -0.738   0.0542 -0.826  \n 4 Arkansas     0.140   1.11    0.113  -0.181  \n 5 California  -2.50   -1.53    0.593  -0.339  \n 6 Colorado    -1.50   -0.978   1.08    0.00145\n 7 Connecticut  1.34   -1.08   -0.637  -0.117  \n 8 Delaware    -0.0472 -0.322  -0.711  -0.873  \n 9 Florida     -2.98    0.0388 -0.571  -0.0953 \n10 Georgia     -1.62    1.27   -0.339   1.07   \n# … with 40 more rows\n```\n:::\n:::\n\n\nbut we can also supply our own data to `new_data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_rec %>%\n  bake(new_data = USArrests[40:45, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  state             PC1    PC2    PC3     PC4\n  <fct>           <dbl>  <dbl>  <dbl>   <dbl>\n1 South Carolina -1.31   1.91  -0.298 -0.130 \n2 South Dakota    1.97   0.815  0.385 -0.108 \n3 Tennessee      -0.990  0.852  0.186  0.646 \n4 Texas          -1.34  -0.408 -0.487  0.637 \n5 Utah            0.545 -1.46   0.291 -0.0815\n6 Vermont         2.77   1.39   0.833 -0.143 \n```\n:::\n:::\n\n\nWe can get back the same information as we could for `prcomp()` but we have to specify the slightly different inside `tidy()`. Here `id = \"pca\"` refers to the second step of `pca_rec`. We get the `scores` with `type = \"coef\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(pca_rec, id = \"pca\", type = \"coef\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 × 4\n   terms      value component id   \n   <chr>      <dbl> <chr>     <chr>\n 1 Murder   -0.536  PC1       pca  \n 2 Assault  -0.583  PC1       pca  \n 3 UrbanPop -0.278  PC1       pca  \n 4 Rape     -0.543  PC1       pca  \n 5 Murder    0.418  PC2       pca  \n 6 Assault   0.188  PC2       pca  \n 7 UrbanPop -0.873  PC2       pca  \n 8 Rape     -0.167  PC2       pca  \n 9 Murder   -0.341  PC3       pca  \n10 Assault  -0.268  PC3       pca  \n11 UrbanPop -0.378  PC3       pca  \n12 Rape      0.818  PC3       pca  \n13 Murder    0.649  PC4       pca  \n14 Assault  -0.743  PC4       pca  \n15 UrbanPop  0.134  PC4       pca  \n16 Rape      0.0890 PC4       pca  \n```\n:::\n:::\n\n\nAnd the eigenvalues with `type = \"variance\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(pca_rec, id = \"pca\", type = \"variance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 × 4\n   terms                         value component id   \n   <chr>                         <dbl>     <int> <chr>\n 1 variance                      2.48          1 pca  \n 2 variance                      0.990         2 pca  \n 3 variance                      0.357         3 pca  \n 4 variance                      0.173         4 pca  \n 5 cumulative variance           2.48          1 pca  \n 6 cumulative variance           3.47          2 pca  \n 7 cumulative variance           3.83          3 pca  \n 8 cumulative variance           4             4 pca  \n 9 percent variance             62.0           1 pca  \n10 percent variance             24.7           2 pca  \n11 percent variance              8.91          3 pca  \n12 percent variance              4.34          4 pca  \n13 cumulative percent variance  62.0           1 pca  \n14 cumulative percent variance  86.8           2 pca  \n15 cumulative percent variance  95.7           3 pca  \n16 cumulative percent variance 100             4 pca  \n```\n:::\n:::\n\n\nSometimes you don't want to get back all the principal components of the data. We can either specify how many components we want with `num_comp` (or `rank.` in `prcomp()`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), num_comp = 3) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 4\n   state           PC1     PC2     PC3\n   <fct>         <dbl>   <dbl>   <dbl>\n 1 Alabama     -0.976   1.12   -0.440 \n 2 Alaska      -1.93    1.06    2.02  \n 3 Arizona     -1.75   -0.738   0.0542\n 4 Arkansas     0.140   1.11    0.113 \n 5 California  -2.50   -1.53    0.593 \n 6 Colorado    -1.50   -0.978   1.08  \n 7 Connecticut  1.34   -1.08   -0.637 \n 8 Delaware    -0.0472 -0.322  -0.711 \n 9 Florida     -2.98    0.0388 -0.571 \n10 Georgia     -1.62    1.27   -0.339 \n# … with 40 more rows\n```\n:::\n:::\n\n\nor using a `threshold` to specify how many components to keep by the variance explained. So by setting `threshold = 0.7`, `step_pca()` will generate enough principal components to explain 70% of the variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe(~., data = USArrests) %>%\n  step_normalize(all_numeric()) %>%\n  step_pca(all_numeric(), threshold = 0.7) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n   state           PC1     PC2\n   <fct>         <dbl>   <dbl>\n 1 Alabama     -0.976   1.12  \n 2 Alaska      -1.93    1.06  \n 3 Arizona     -1.75   -0.738 \n 4 Arkansas     0.140   1.11  \n 5 California  -2.50   -1.53  \n 6 Colorado    -1.50   -0.978 \n 7 Connecticut  1.34   -1.08  \n 8 Delaware    -0.0472 -0.322 \n 9 Florida     -2.98    0.0388\n10 Georgia     -1.62    1.27  \n# … with 40 more rows\n```\n:::\n:::\n\n\n## Matrix Completion\n\nThis section is WIP.\n\n## Kmeans Clustering\n\nWe will be using the tidyclust package to perform these clustering tasks. It was a similar interface to parsnip, and it interfaces well with the rest of tidymodels.\n\nBefore we get going let us create a synthetic data set that we know has groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\n\nx_df <- tibble(\n  V1 = rnorm(n = 50, mean = rep(c(0, 3), each = 25)),\n  V2 = rnorm(n = 50, mean = rep(c(0, -4), each = 25))\n)\n```\n:::\n\n\nAnd we can plot it with ggplot2 to see that the groups are really there. Note that we didn't include this grouping information in `x_df` as we are trying to emulate a situation where we don't know of the possible underlying clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_df %>%\n  ggplot(aes(V1, V2, color = rep(c(\"A\", \"B\"), each = 25))) +\n  geom_point() +\n  labs(color = \"groups\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-22-1.png){fig-alt='Scatter chart of x_df data set with V1 on the x-axis and V2\non the y-axis. Colors correspending to the two groups in the\ndata. The data neatly seperates into gaussian clusters.' width=672}\n:::\n:::\n\n\nNow that we have the data, it is time to create a cluster specification. Since we want to perform K-means clustering, we will use the `k_means()` function from tidyclust. We use the `num_clusters` argument to specify how many centroids the K-means algorithm need to use. We also set a mode and engine, which this time are set to the same as the defaults. We also set `nstart = 20`, this allows the algorithm to have multiple initial starting positions, which we use in the hope of finding global maxima instead of local maxima.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans_spec <- k_means(num_clusters = 3) %>%\n  set_mode(\"partition\") %>%\n  set_engine(\"stats\") %>%\n  set_args(nstart = 20)\n\nkmeans_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = 3\n\nEngine-Specific Arguments:\n  nstart = 20\n\nComputational engine: stats \n```\n:::\n:::\n\n\nOnce we have this specification we can fit it to our data. We remember to set a seed because the K-means algorithm starts with random initialization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nkmeans_fit <- kmeans_spec %>%\n  fit(~., data = x_df)\n```\n:::\n\n\nThis fitted model has a lot of different kinds of information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntidyclust cluster object\n\nK-means clustering with 3 clusters of sizes 11, 23, 16\n\nCluster means:\n         V1          V2\n1 2.5355362 -2.48605364\n2 0.2339095  0.04414551\n3 2.8241300 -5.01221675\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  3 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n 1  1  1  3  1  3  3  3  3  1  3  3  3  1  1  1  3  3  3  3  1  3  3  3 \n\nWithin cluster sum of squares by cluster:\n[1] 14.56698 54.84869 26.98215\n (between_SS / total_SS =  76.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n\nAn otherall function to inspect your fitted tidyclust models is `extract_fit_summary()` which returns all different kind of information\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_fit_summary(kmeans_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$cluster_names\n[1] Cluster_1 Cluster_2 Cluster_3\nLevels: Cluster_1 Cluster_2 Cluster_3\n\n$centroids\n# A tibble: 3 × 2\n     V1      V2\n  <dbl>   <dbl>\n1 0.234  0.0441\n2 2.54  -2.49  \n3 2.82  -5.01  \n\n$n_members\n[1] 23 11 16\n\n$sse_within_total_total\n[1] 54.84869 14.56698 26.98215\n\n$sse_total\n[1] 415.9045\n\n$orig_labels\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 3 1 1 1 3 1 3 3 3 3 1 3 3\n[39] 3 1 1 1 3 3 3 3 1 3 3 3\n\n$cluster_assignments\n [1] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [8] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[15] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_2\n[22] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_3 Cluster_2 Cluster_2\n[29] Cluster_2 Cluster_3 Cluster_2 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[36] Cluster_2 Cluster_3 Cluster_3 Cluster_3 Cluster_2 Cluster_2 Cluster_2\n[43] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_2 Cluster_3 Cluster_3\n[50] Cluster_3\nLevels: Cluster_1 Cluster_2 Cluster_3\n```\n:::\n:::\n\n\nWe can also extract some of these quantities directly using `extract_centroids()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_centroids(kmeans_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .cluster     V1      V2\n  <fct>     <dbl>   <dbl>\n1 Cluster_1 0.234  0.0441\n2 Cluster_2 2.54  -2.49  \n3 Cluster_3 2.82  -5.01  \n```\n:::\n:::\n\n\nand `extract_cluster_assignment()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_cluster_assignment(kmeans_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 1\n   .cluster \n   <fct>    \n 1 Cluster_1\n 2 Cluster_1\n 3 Cluster_1\n 4 Cluster_1\n 5 Cluster_1\n 6 Cluster_1\n 7 Cluster_1\n 8 Cluster_1\n 9 Cluster_1\n10 Cluster_1\n# … with 40 more rows\n```\n:::\n:::\n\n\nprediction in a clustering model isn't well defined. But we can think of it as \"what cluster would these observations be in if they were part of the data set\". For the k-means case, it looks at which centroid these observations are closest to.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(kmeans_fit, new_data = x_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 1\n   .pred_cluster\n   <fct>        \n 1 Cluster_1    \n 2 Cluster_1    \n 3 Cluster_1    \n 4 Cluster_1    \n 5 Cluster_1    \n 6 Cluster_1    \n 7 Cluster_1    \n 8 Cluster_1    \n 9 Cluster_1    \n10 Cluster_1    \n# … with 40 more rows\n```\n:::\n:::\n\n\n\nLastly, we can see what cluster each observation belongs to by using `augment()`, which does the same thing as `predict()` but add it to the orginial data set. This makes it handy for EDA and plotting the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(kmeans_fit, new_data = x_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n        V1     V2 .pred_cluster\n     <dbl>  <dbl> <fct>        \n 1 -0.897  -0.838 Cluster_1    \n 2  0.185   2.07  Cluster_1    \n 3  1.59   -0.562 Cluster_1    \n 4 -1.13    1.28  Cluster_1    \n 5 -0.0803 -1.05  Cluster_1    \n 6  0.132  -1.97  Cluster_1    \n 7  0.708  -0.323 Cluster_1    \n 8 -0.240   0.936 Cluster_1    \n 9  1.98    1.14  Cluster_1    \n10 -0.139   1.67  Cluster_1    \n# … with 40 more rows\n```\n:::\n:::\n\n\nWe can visualize the result of `augment()` to see how well the clustering performed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(kmeans_fit, new_data = x_df) %>%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-31-1.png){fig-alt='Scatter chart of augmented data set with V1 on the x-axis and V2\non the y-axis. Colors correspending to the .pred_cluster variables.\nLeft-most cluster is one color, right-most cluster is another\ncolor and the points between them in each real cluster is\ncontained in a third color.' width=672}\n:::\n:::\n\n\nThis is all well and good, but it would be nice if we could try out a number of different clusters and then find the best one. For this we will use `tune_cluster()`. `tune_cluster()` works pretty much like `tune_grid()` expect that it works with cluster models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans_spec_tuned <- kmeans_spec %>% \n  set_args(num_clusters = tune())\n\nkmeans_wf <- workflow() %>%\n  add_model(kmeans_spec_tuned) %>%\n  add_formula(~.)\n```\n:::\n\n\nnow we can use this workflow with `tune_cluster()` to fit it many times for different values of `num_clusters`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nx_boots <- bootstraps(x_df, times = 10)\n\nnum_clusters_grid <- tibble(num_clusters = seq(1, 10))\n\ntune_res <- tune_cluster(\n  object = kmeans_wf,\n  resamples = x_boots,\n  grid = num_clusters_grid\n)\n```\n:::\n\n\nAnd we can use `collect_metrics()` as before\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 7\n   num_clusters .metric          .estimator  mean     n std_err .config         \n          <int> <chr>            <chr>      <dbl> <int>   <dbl> <chr>           \n 1            1 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 2            1 sse_within_total standard   387.     10   8.86  Preprocessor1_M…\n 3            2 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 4            2 sse_within_total standard   121.     10   4.00  Preprocessor1_M…\n 5            3 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 6            3 sse_within_total standard    82.6    10   2.29  Preprocessor1_M…\n 7            4 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n 8            4 sse_within_total standard    59.0    10   2.07  Preprocessor1_M…\n 9            5 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n10            5 sse_within_total standard    43.8    10   1.99  Preprocessor1_M…\n11            6 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n12            6 sse_within_total standard    33.3    10   1.48  Preprocessor1_M…\n13            7 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n14            7 sse_within_total standard    25.0    10   1.25  Preprocessor1_M…\n15            8 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n16            8 sse_within_total standard    20.8    10   1.10  Preprocessor1_M…\n17            9 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n18            9 sse_within_total standard    17.0    10   1.00  Preprocessor1_M…\n19           10 sse_total        standard   387.     10   8.86  Preprocessor1_M…\n20           10 sse_within_total standard    13.9    10   0.695 Preprocessor1_M…\n```\n:::\n:::\n\n\nNow that we have the total within-cluster sum-of-squares we can plot them against `k` so we can use the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) to find the optimal number of clusters. This actually pops right out if we use `autoplot()` on the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\nWe see an elbow when the number of clusters is equal to 2 which makes us happy since the data set is specifically created to have 2 clusters. We can now construct the final kmeans model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_kmeans <- kmeans_wf %>%\n  update_model(kmeans_spec %>% set_args(num_clusters = 2)) %>%\n  fit(x_df)\n```\n:::\n\n\nAnd we can finish by visualizing the clusters it found.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(final_kmeans, new_data = x_df) %>%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-37-1.png){fig-alt='Scatter chart of augmented data set with V1 on the x-axis and V2\non the y-axis. Colors correspending to the two cluster in the\ndata. These results align closely with the true clusters.' width=672}\n:::\n:::\n\n\n## Hierarchical Clustering\n\nThe `hclust()` function is one way to perform hierarchical clustering in R. It only needs one input and that is a dissimilarity structure as produced by `dist()`. Furthermore, we can specify a couple of things, including the agglomeration method. Let us cluster this data in a couple of different ways to see how the choice of agglomeration method changes the clustering. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_hclust_complete <- x_df %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\nres_hclust_average <- x_df %>%\n  dist() %>%\n  hclust(method = \"average\")\n\nres_hclust_single <- x_df %>%\n  dist() %>%\n  hclust(method = \"single\")\n```\n:::\n\n\nThe [factoextra](https://rpkgs.datanovia.com/factoextra/index.html) package provides functions (`fviz_dend()`) to visualize the clustering created using `hclust()`. We use `fviz_dend()` to show the dendrogram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(res_hclust_complete, main = \"complete\", k = 2)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-39-1.png){fig-alt='Dendrogram visualization. Both left and right side looks more\nor less even.' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(res_hclust_average, main = \"average\", k = 2)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-40-1.png){fig-alt='Dendrogram visualization. Both left and right side looks more\nor less even.' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(res_hclust_single, main = \"single\", k = 2)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-41-1.png){fig-alt='Dendrogram visualization. Left side has 1 leaf and the right\nside contain the remaining leaves.' width=672}\n:::\n:::\n\n\nIf we don't know the importance of the different predictors in data set it could be beneficial to scale the data such that each variable has the same influence. We can perform scaling by using `scale()` before `dist()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_df %>%\n  scale() %>%\n  dist() %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend(k = 2)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-42-1.png){fig-alt='Dendrogram visualization. Both left and right side looks more\nor less even.' width=672}\n:::\n:::\n\nAnother way of calculating distances is based on correlation. This only makes sense if the data set has 3 or more variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# correlation based distance\nset.seed(2)\nx <- matrix(rnorm(30 * 3), ncol = 3)\n\nx %>%\n  proxy::dist(method = \"correlation\") %>%\n  hclust(method = \"complete\") %>%\n  fviz_dend()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-43-1.png){fig-alt='Dendrogram visualization. Not colored, has a couple of splits\nearly on at high correlations values such as 0.9, 0.5 and 0.3.\nThe remaining splits occur at very low values.' width=672}\n:::\n:::\n\n\n## PCA on the NCI60 Data\n\nWe will now explore the `NCI60` data set. It is genomic data set, containing cancer cell line microarray data, which consists of 6830 gene expression measurements on 64 cancer cell lines. The data comes as a list containing a matrix and its labels. We do a little work to turn the data into a tibble we will use for the rest of the chapter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(NCI60, package = \"ISLR\")\nnci60 <- NCI60$data %>%\n  as_tibble(.name_repair = ~ paste0(\"V_\", .x)) %>%\n  mutate(label = factor(NCI60$labs)) %>%\n  relocate(label)\n```\n:::\n\n\nWe do not expect to use the `label` variable doing the analysis since we are emulating an unsupervised analysis. Since we are an exploratory task we will be fine with using `prcomp()` since we don't need to apply these transformations to anything else. We remove `label` and remember to set `scale = TRUE` to perform scaling of all the variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pca <- nci60 %>%\n  select(-label) %>%\n  prcomp(scale = TRUE)\n```\n:::\n\n\nFor visualization purposes, we will now join up the labels into the result of `augment(nci60_pca)` so we can visualize how close similar labeled points are to each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pcs <- bind_cols(\n  augment(nci60_pca),\n  nci60 %>% select(label)\n)\n```\n:::\n\n\nWe have 14 different labels, so we will make use of the `\"Polychrome 36\"` palette to help us better differentiate between the labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolors <- unname(palette.colors(n = 14, palette = \"Polychrome 36\"))\n```\n:::\n\n\nOr we can plot the different PCs against each other. It is a good idea to compare the first PCs against each other since they carry the most information. We will just compare the pairs 1-2 and 1-3 but you can do more yourself. It tends to be a good idea to stop once interesting things appear in the plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-48-1.png){fig-alt='Scatter plot of nci60_pcs across the first 2 principal\ncomponents. Colors by label which has 14 unique values.\nObservations with same label appears fairly close together\nfor most labels.' width=672}\n:::\n:::\n\n\nWe see there is some local clustering of the different cancer types which is promising, it is not perfect but let us see what happens when we compare PC1 against PC3 now. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pcs %>%\n  ggplot(aes(.fittedPC1, .fittedPC3, color = label)) +\n  geom_point() +\n  scale_color_manual(values = colors)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-49-1.png){fig-alt='Scatter plot of nci60_pcs across the first and third principal\ncomponents. Colors by label which has 14 unique values.\nObservations with same label appears fairly close together\nfor most labels.' width=672}\n:::\n:::\n\n\nLastly, we will plot the variance explained of each principal component. We can use `tidy()` with `matrix = \"eigenvalues\"` to accomplish this easily, so we start with the percentage of each PC\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 60, by = 5)) +\n  scale_y_continuous(labels = scales::percent)\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-50-1.png){fig-alt='Connected line chart of percent variance explained for each\nprincipal components, with percent variance explained on the\ny-axis and PCs on the x-axis. 11% for PC1, 7% for PC2, 6% for\nPC3, 4% for PC4 and the remaining 60 PCs more or less linearly\ngoes towards 0%.' width=672}\n:::\n:::\n\n\nwith the first PC having a little more than 10% and a fairly fast drop. \n\nAnd we can get the cumulative variance explained just the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(nci60_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, cumulative)) +\n  geom_point() +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-51-1.png){fig-alt='Connected line chart of cumulative percent variance explained\nfor each principal components, with percent variance explained\non the y-axis and PCs on the x-axis.' width=672}\n:::\n:::\n\n\n## Clustering on nci60 dataset\n\nLet us now see what happens if we perform clustering on the `nci60` data set. Before we start it would be good if we create a scaled version of this data set. We can use the recipes package to perform those transformations. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_scaled <- recipe(~ ., data = nci60) %>%\n  step_rm(label) %>%\n  step_normalize(all_predictors()) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n:::\n\n\nNow we start by fitting multiple hierarchical clustering models using different agglomeration methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_complete <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"complete\")\n\nnci60_average <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"average\")\n\nnci60_single <- nci60_scaled %>%\n    dist() %>%\n    hclust(method = \"single\")\n```\n:::\n\n\nWe then visualize them to see if any of them have some good natural separations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(nci60_complete, main = \"Complete\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-54-1.png){fig-alt='Dendrogram visualization. Not colored, has most of the splits\nhappen at larger hights.' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(nci60_average, main = \"Average\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-55-1.png){fig-alt='Dendrogram visualization. Not colored, has most of the splits\nhappen at larger hights.' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(nci60_single, main = \"Single\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-56-1.png){fig-alt='Dendrogram visualization. Not colored, has most of the splits\nhappen at larger hight, very close together, with a few splits\na lower heights.' width=672}\n:::\n:::\n\n\nWe now color according to `k = 4` and we get the following separations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_complete %>%\n  fviz_dend(k = 4, main = \"hclust(complete) on nci60\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-57-1.png){fig-alt='Dendrogram visualization. Colors for 4 clusters.' width=672}\n:::\n:::\n\n\nWe now take the clustering id extracted with `cutree` and calculate which label is the most common one within each cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  label = nci60$label,\n  cluster_id = cutree(nci60_complete, k = 4)\n) %>%\n  count(label, cluster_id) %>%\n  group_by(cluster_id) %>%\n  mutate(prop = n / sum(n)) %>%\n  slice_max(n = 1, order_by = prop) %>%\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  label    cluster_id     n  prop\n  <fct>         <int> <int> <dbl>\n1 MELANOMA          1     8 0.2  \n2 NSCLC             1     8 0.2  \n3 RENAL             1     8 0.2  \n4 BREAST            2     3 0.429\n5 LEUKEMIA          3     6 0.75 \n6 COLON             4     5 0.556\n```\n:::\n:::\n\n\nWe can also see what happens if we try to fit a K-means clustering. We liked 4 clusters from earlier so let's stick with that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nres_kmeans_scaled <- kmeans(nci60_scaled, centers = 4, nstart = 50)\n```\n:::\n\n\nWe can again use `tidy()` to extract cluster information, note that we only look at `cluster`, `size`, and `withinss` as there are thousands of other variables denoting the location of the cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(res_kmeans_scaled) %>%\n  select(cluster, size, withinss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  cluster  size withinss\n  <fct>   <int>    <dbl>\n1 1           8   44071.\n2 2          20  108801.\n3 3          27  154545.\n4 4           9   37150.\n```\n:::\n:::\n\n\nLastly, let us see how the two different methods we used compare against each other. Let us save the cluster ids in `cluster_kmeans` and `cluster_hclust` and then use `conf_mat()` in a different way to quickly generate a heatmap between the two methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_kmeans <- res_kmeans_scaled$cluster\ncluster_hclust <- cutree(nci60_complete, k = 4)\n\ntibble(\n  kmeans = factor(cluster_kmeans),\n  hclust = factor(cluster_hclust)\n) %>%\n  conf_mat(kmeans, hclust) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-61-1.png){fig-alt='Confusion matrix, truth along x-axis and prediction along\ny-axis. No agreement between labels.' width=672}\n:::\n:::\n\n\nThere is not a lot of agreement between labels which makes sense, since the labels themselves are arbitrarily added. What is important is that they tend to agree quite a lot (the confusion matrix is sparse).\n\nOne last thing is that it is sometimes useful to perform dimensionality reduction before using the clustering method. Let us use the recipes package to calculate the PCA of `nci60` and keep the 5 first components (we could have started with `nci60` too if we added `step_rm()` and `step_normalize()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pca <- recipe(~., nci60_scaled) %>%\n  step_pca(all_predictors(), num_comp = 5) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n:::\n\n\nWe can now use `hclust()` on this reduced data set, and sometimes we get quite good results since the clustering method doesn't have to work in high dimensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnci60_pca %>%\n  dist() %>%\n  hclust() %>%\n  fviz_dend(k = 4, main = \"hclust on first five PCs\")\n```\n\n::: {.cell-output-display}\n![](12-unsupervised-learning_files/figure-html/unnamed-chunk-63-1.png){fig-alt='Dendrogram visualization. Colors to produce 4 clusters.' width=672}\n:::\n:::\n",
    "supporting": [
      "12-unsupervised-learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}