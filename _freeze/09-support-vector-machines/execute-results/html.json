{
  "hash": "34af4e6d6a9ce3a7fb12a4a362e2ce89",
  "result": {
    "markdown": "# Support Vector Machines\n\n\n::: {.cell}\n\n:::\n\n\nThis lab will take a look at support vector machines, in doing so we will explore how changing the hyperparameters can help improve performance. \nThis chapter will use [parsnip](https://www.tidymodels.org/start/models/) for model fitting and [recipes and workflows](https://www.tidymodels.org/start/recipes/) to perform the transformations, and [tune and dials](https://www.tidymodels.org/start/tuning/) to tune the hyperparameters of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ISLR)\n```\n:::\n\n\n## Support Vector Classifier\n\nLet us start by creating a synthetic data set. We will use some normally distributed data with an added offset to create 2 separate classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsim_data <- tibble(\n  x1 = rnorm(40),\n  x2 = rnorm(40),\n  y  = factor(rep(c(-1, 1), 20))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n```\n:::\n\n\nPlotting it shows that we are having two slightly overlapping classes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe can then create a linear SVM specification by setting `degree = 1` in a polynomial SVM model. We furthermore set `scaled = FALSE` in `set_engine()` to have the engine scale the data for us. Once we get to it later we can be performing this scaling in a recipe instead. \n\n:::{.callout-note}\n`set_engine()` can be used to pass in additional arguments directly to the underlying engine. In this case, I'm passing in `scaled = FALSE` to `kernlab::ksvm()` which is the engine function.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n```\n:::\n\n\nTaking the specification, we can add a specific `cost` of 10 before fitting the model to the data. Using `set_args()` allows us to set the `cost` argument without modifying the model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 10 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 17 \n\nObjective Function Value : -152.0188 \nTraining error : 0.125 \nProbability model included. \n```\n:::\n:::\n\n\nThe `kernlab` models can be visualized using the `plot()` function if you load the `kernlab` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kernlab)\nsvm_linear_fit %>%\n  extract_fit_engine() %>%\n  plot()\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-7-1.png){fig-alt='Scatter chart of sim_data with x2 on the x-axis and x1 on the\ny-axis. A gradient going from red through white to blue,\nis overlaid. Blue values occur when both x1 and x2 sum to more\nthan 2 and red values when x1 and x2 sum to less than 2.\nThe gradient does not appear to seperate the two classes\nwhich is represented by shapes.' width=672}\n:::\n:::\n\n\nwhat if we instead used a smaller value of the `cost` parameter?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_fit <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 0.1 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 25 \n\nObjective Function Value : -2.0376 \nTraining error : 0.15 \nProbability model included. \n```\n:::\n:::\n\n\nNow that a smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider.\n\nLet us set up a `tune_grid()` section to find the value of `cost` that leads to the highest accuracy for the SVM model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_wf <- workflow() %>%\n  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\nautoplot(tune_res)\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-9-1.png){fig-alt='Facetted connected scatter chart. x-axis shows different\nvalues of cost, and the y-axis show the performance metric\nvalue. The facets correspond to the two performance metrics\naccuracy and roc_auc. Both charts show a constant value for\nall values of cost, expect for once where the accuracy skipes.' width=672}\n:::\n:::\n\n\nusing the `tune_res` object and `select_best()` function allows us to find the value of `cost` that gives the best cross-validated accuracy. Finalize the workflow with `finalize_workflow()` and fit the new workflow on the data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\n```\n:::\n\n\nWe can now generate a different data set to act as the test data set. We will make sure that it is generated using the same model but with a different seed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nsim_data_test <- tibble(\n  x1 = rnorm(20),\n  x2 = rnorm(20),\n  y  = factor(rep(c(-1, 1), 10))\n) %>%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n```\n:::\n\n\nand accessing the model on this testing data set shows us that the model still performs very well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(svm_linear_fit, new_data = sim_data_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction -1 1\n        -1  8 3\n        1   2 7\n```\n:::\n:::\n\n\n## Support Vector Machine\n\nWe will now see how we can fit an SVM using a non-linear kernel. Let us start by generating some data, but this time generate with a non-linear class boundary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsim_data2 <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n\nsim_data2 %>%\n  ggplot(aes(x1, x2, color = y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-13-1.png){fig-alt='Scatter plot of sim_data2. Data is in a oblong shape with\npoints in the middle having color and both ends having\nanother color.' width=672}\n:::\n:::\n\n\nWe will try an SVM with a radial basis function. Such a kernel would allow us to capture the non-linearity in our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_rbf_spec <- svm_rbf() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n```\n:::\n\n\nfitting the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_rbf_fit <- svm_rbf_spec %>%\n  fit(y ~ ., data = sim_data2)\n```\n:::\n\n\nand plotting reveals that the model was able to separate the two classes, even though they were non-linearly separated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_rbf_fit %>%\n  extract_fit_engine() %>%\n  plot()\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-16-1.png){fig-alt='Scatter chart of sim_data with x2 on the x-axis and x1 on the\ny-axis. A gradient going from red through white to blue,\nis overlaid. The grading is blue in the middle of the data\nand red at the edges, with a non-linear seperation between\nthe colors.' width=672}\n:::\n:::\n\n\nBut let us see how well this model generalizes to new data from the same generating process. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nsim_data2_test <- tibble(\n  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),\n  y  = factor(rep(c(1, 2), c(150, 50)))\n)\n```\n:::\n\n\nAnd it works well!\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction   1   2\n         1 137   7\n         2  13  43\n```\n:::\n:::\n\n\n## ROC Curves\n\nROC curves can easily be created using the `roc_curve()` function from the yardstick package. We use this function much the same way as we have done using the `accuracy()` function, but the main difference is that we pass the predicted class probability to `estimate` instead of passing the predicted class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1   -Inf          0            1    \n 2      0.104      0            1    \n 3      0.113      0.0200       1    \n 4      0.114      0.0400       1    \n 5      0.115      0.0600       1    \n 6      0.117      0.0800       1    \n 7      0.118      0.1          1    \n 8      0.119      0.12         1    \n 9      0.124      0.14         1    \n10      0.124      0.14         0.993\n# … with 192 more rows\n```\n:::\n:::\n\n\nThis produces the different values of `specificity` and `sensitivity` for each threshold. We can get a quick visualization by passing the results of `roc_curve()` into `autoplot()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_curve(truth = y, estimate = .pred_1) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](09-support-vector-machines_files/figure-html/unnamed-chunk-20-1.png){fig-alt='A ROC curve plot. 1-specificity along the x-axis and\nsensitivity along the y-axis. A dotted line is drawn\nalong the diagonal. The line quite closely follows\nthe upper left side.' width=672}\n:::\n:::\n\n\nA common metric is t calculate the area under this curve. This can be done using the `roc_auc()` function (`_auc` stands for **a**rea **u**nder **c**urve).\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(svm_rbf_fit, new_data = sim_data2_test) %>%\n  roc_auc(truth = y, estimate = .pred_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.925\n```\n:::\n:::\n\n\n## Application to Gene Expression Data\n\nWe now examine the Khan data set, which consists of several tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set comes in the `Khan` list which we will wrangle a little bit to create two tibbles, 1 for the training data and 1 for the testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKhan_train <- bind_cols(\n  y = factor(Khan$ytrain),\n  as_tibble(Khan$xtrain)\n)\n\nKhan_test <- bind_cols(\n  y = factor(Khan$ytest),\n  as_tibble(Khan$xtest)\n)\n```\n:::\n\n\n\nlooking at the dimensions of the training data reveals that we have 63 observations with 20308 gene expression measurements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(Khan_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   63 2309\n```\n:::\n:::\n\n\nThere is a very large number of predictors compared to the number of rows. This indicates that a linear kernel will be preferable, as the added flexibility we would get from a polynomial or radial kernel is unnecessary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkhan_fit <- svm_linear_spec %>%\n  set_args(cost = 10) %>%\n  fit(y ~ ., data = Khan_train)\n```\n:::\n\n\nLet us take a look at the training confusion matrix. And look, we get a perfect confusion matrix. We are getting this because the hyperplane was able to fully separate the classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(khan_fit, new_data = Khan_train) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction  1  2  3  4\n         1  8  0  0  0\n         2  0 23  0  0\n         3  0  0 12  0\n         4  0  0  0 20\n```\n:::\n:::\n\n\nBut remember we don't measure the performance by how well it performs on the training data set. We measure the performance of a model on how well it performs on the testing data set, so let us look at the testing confusion matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(khan_fit, new_data = Khan_test) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction 1 2 3 4\n         1 3 0 0 0\n         2 0 6 2 0\n         3 0 0 4 0\n         4 0 0 0 5\n```\n:::\n:::\n\n\nAnd it performs fairly well. A couple of misclassifications but nothing too bad.\n",
    "supporting": [
      "09-support-vector-machines_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}