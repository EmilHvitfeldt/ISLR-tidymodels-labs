{
  "hash": "862e8a4582ce52735981934da76c60bd",
  "result": {
    "markdown": "# Moving Beyond Linearity\n\n\n::: {.cell}\n\n:::\n\n\nThis lab will look at the various ways we can introduce non-linearity into our model by doing preprocessing. Methods include: polynomials expansion, step functions, and splines.\n\nThe GAMs section is WIP since they are now supported in [parsnip](https://github.com/tidymodels/parsnip/pull/512).\n\nThis chapter will use [parsnip](https://www.tidymodels.org/start/models/) for model fitting and [recipes and workflows](https://www.tidymodels.org/start/recipes/) to perform the transformations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ISLR)\n\nWage <- as_tibble(Wage)\n```\n:::\n\n\n## Polynomial Regression and Step Functions\n\nPolynomial regression can be thought of as doing polynomial expansion on a variable and passing that expansion into a linear regression model. We will be very explicit in this formulation in this chapter. `step_poly()` allows us to do a polynomial expansion on one or more variables.\n\nThe following step will take `age` and replace it with the variables `age`, `age^2`, `age^3`, and `age^4` since we set `degree = 4`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n```\n:::\n\n\nThis recipe is combined with a linear regression specification and combined to create a workflow object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n\npoly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_poly)\n```\n:::\n\n\nThis object can now be `fit()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly_fit <- fit(poly_wf, data = Wage)\npoly_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)   age_poly_1   age_poly_2   age_poly_3   age_poly_4  \n     111.70       447.07      -478.32       125.52       -77.91  \n```\n:::\n:::\n\n\nAnd we cal pull the coefficients using `tidy()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(poly_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    112.      0.729    153.   0       \n2 age_poly_1     447.     39.9       11.2  1.48e-28\n3 age_poly_2    -478.     39.9      -12.0  2.36e-32\n4 age_poly_3     126.     39.9        3.14 1.68e- 3\n5 age_poly_4     -77.9    39.9       -1.95 5.10e- 2\n```\n:::\n:::\n\n\nI was lying when I said that `step_poly()` returned `age`, `age^2`, `age^3`, and `age^4`. What is happening is that it returns variables that are a basis of orthogonal polynomials, which means that each of the columns is a linear combination of the variables `age`, `age^2`, `age^3`, and `age^4`. We can see this by using `poly()` directly with `raw = FALSE` since it is the default\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly(1:6, degree = 4, raw = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              1          2          3          4\n[1,] -0.5976143  0.5455447 -0.3726780  0.1889822\n[2,] -0.3585686 -0.1091089  0.5217492 -0.5669467\n[3,] -0.1195229 -0.4364358  0.2981424  0.3779645\n[4,]  0.1195229 -0.4364358 -0.2981424  0.3779645\n[5,]  0.3585686 -0.1091089 -0.5217492 -0.5669467\n[6,]  0.5976143  0.5455447  0.3726780  0.1889822\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 3.5 3.5 3.5 3.5\n\nattr(,\"coefs\")$norm2\n[1]  1.00000  6.00000 17.50000 37.33333 64.80000 82.28571\n\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n```\n:::\n:::\n\n\nWe see that these variables don't directly have a format we would have assumed. But this is still a well-reasoned transformation.\nWe can get the raw polynomial transformation by setting `raw = TRUE`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly(1:6, degree = 4, raw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     1  2   3    4\n[1,] 1  1   1    1\n[2,] 2  4   8   16\n[3,] 3  9  27   81\n[4,] 4 16  64  256\n[5,] 5 25 125  625\n[6,] 6 36 216 1296\nattr(,\"degree\")\n[1] 1 2 3 4\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n```\n:::\n:::\n\n\nThese transformations align with what we would expect. It is still recommended to stick with the default of `raw = FALSE` unless you have a reason not to do that.\nOne of the benefits of using `raw = FALSE` is that the resulting variables are uncorrelated which is a desirable quality when using a linear regression model.\n\nYou can get the raw polynomials by setting `options = list(raw = TRUE)` in `step_poly()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_raw_poly <- recipe(wage ~ age, data = Wage) %>%\n  step_poly(age, degree = 4, options = list(raw = TRUE))\n\nraw_poly_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_raw_poly)\n\nraw_poly_fit <- fit(raw_poly_wf, data = Wage)\n\ntidy(raw_poly_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 5\n  term            estimate  std.error statistic  p.value\n  <chr>              <dbl>      <dbl>     <dbl>    <dbl>\n1 (Intercept) -184.        60.0           -3.07 0.00218 \n2 age_poly_1    21.2        5.89           3.61 0.000312\n3 age_poly_2    -0.564      0.206         -2.74 0.00626 \n4 age_poly_3     0.00681    0.00307        2.22 0.0264  \n5 age_poly_4    -0.0000320  0.0000164     -1.95 0.0510  \n```\n:::\n:::\n\n\nLet us try something new and visualize the polynomial fit on our data. We can do this easily because we only have 1 predictor and 1 response. Starting with creating a tibble with different ranges of `age`. Then we take this tibble and predict with it, this will give us the repression curve. We are additionally adding confidence intervals by setting `type = \"conf_int\"` which we can do since we are using a linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_range <- tibble(age = seq(min(Wage$age), max(Wage$age)))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = age_range),\n  predict(poly_fit, new_data = age_range, type = \"conf_int\")\n)\nregression_lines\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 63 × 4\n     age .pred .pred_lower .pred_upper\n   <int> <dbl>       <dbl>       <dbl>\n 1    18  51.9        41.5        62.3\n 2    19  58.5        49.9        67.1\n 3    20  64.6        57.5        71.6\n 4    21  70.2        64.4        76.0\n 5    22  75.4        70.5        80.2\n 6    23  80.1        76.0        84.2\n 7    24  84.5        80.9        88.1\n 8    25  88.5        85.2        91.7\n 9    26  92.1        89.1        95.2\n10    27  95.4        92.5        98.4\n# … with 53 more rows\n```\n:::\n:::\n\n\nWe will then use `ggplot2` to visualize the fitted line and confidence interval. The green line is the regression curve and the dashed blue lines are the confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](07-moving-beyond-linearity_files/figure-html/unnamed-chunk-11-1.png){fig-alt='Scatter chart, age against the x-axis and wage against y-axis.\nFairly normally distributed around wage == 100, with some\nanother blob around wage == 275. A curve in dark green follows\nthe middle of the data with two dottled curves follows closely\naround.' width=672}\n:::\n:::\n\n\nThe regression curve is now a curve instead of a line as we would have gotten with a simple linear regression model. Notice furthermore that the confidence bands are tighter when there is a lot of data and they wider towards the ends of the data.\n\nLet us take that one step further and see what happens to the regression line once we go past the domain it was trained on. the previous plot showed individuals within the age range 18-80. Let us see what happens once we push this to 18-100. This is not an impossible range but an unrealistic range.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwide_age_range <- tibble(age = seq(18, 100))\n\nregression_lines <- bind_cols(\n  augment(poly_fit, new_data = wide_age_range),\n  predict(poly_fit, new_data = wide_age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), color = \"darkgreen\",\n            data = regression_lines) +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](07-moving-beyond-linearity_files/figure-html/unnamed-chunk-12-1.png){fig-alt='Scatter chart, age against the x-axis and wage against y-axis.\nFairly normally distributed around wage == 100, with some\nanother blob around wage == 275. A curve in dark green follows\nthe middle of the data with two dottled curves follows closely\naround. The range for age has been increased beyond the data\npoints and the green curve trails negative and the dotted lines\nquickly move away from the green curve.' width=672}\n:::\n:::\n\n\nAnd we see that the curve starts diverging once we get to 93 the predicted `wage` is negative. The confidence bands also get wider and wider as we get farther away from the data.\n\nWe can also think of this problem as a classification problem, and we will do that just now by setting us the task of predicting whether an individual earns more than $250000 per year. We will add a new factor value denoting this response.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWage <- Wage %>%\n  mutate(high = factor(wage > 250, \n                       levels = c(TRUE, FALSE), \n                       labels = c(\"High\", \"Low\")))\n```\n:::\n\n\nWe cannot use the polynomial expansion recipe `rec_poly` we created earlier since it had `wage` as the response and now we want to have `high` as the response.\nWe also have to create a logistic regression specification that we will use as our classification model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_poly <- recipe(high ~ age, data = Wage) %>%\n  step_poly(age, degree = 4)\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nlr_poly_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_poly)\n```\n:::\n\n\nThis polynomial logistic regression model workflow can now be fit and predicted with as usual.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_poly_fit <- fit(lr_poly_wf, data = Wage)\n\npredict(lr_poly_fit, new_data = Wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,000 × 1\n   .pred_class\n   <fct>      \n 1 Low        \n 2 Low        \n 3 Low        \n 4 Low        \n 5 Low        \n 6 Low        \n 7 Low        \n 8 Low        \n 9 Low        \n10 Low        \n# … with 2,990 more rows\n```\n:::\n:::\n\n\nIf we want we can also get back the underlying probability predictions for the two classes, and their confidence intervals for these probability predictions by setting `type = \"prob\"` and `type = \"conf_int\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lr_poly_fit, new_data = Wage, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,000 × 2\n      .pred_High .pred_Low\n           <dbl>     <dbl>\n 1 0.00000000983     1.00 \n 2 0.000120          1.00 \n 3 0.0307            0.969\n 4 0.0320            0.968\n 5 0.0305            0.970\n 6 0.0352            0.965\n 7 0.0313            0.969\n 8 0.00820           0.992\n 9 0.0334            0.967\n10 0.0323            0.968\n# … with 2,990 more rows\n```\n:::\n\n```{.r .cell-code}\npredict(lr_poly_fit, new_data = Wage, type = \"conf_int\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,000 × 4\n   .pred_lower_High .pred_upper_High .pred_lower_Low .pred_upper_Low\n              <dbl>            <dbl>           <dbl>           <dbl>\n 1         2.22e-16          0.00166           0.998           1    \n 2         1.82e- 6          0.00786           0.992           1.00 \n 3         2.19e- 2          0.0428            0.957           0.978\n 4         2.31e- 2          0.0442            0.956           0.977\n 5         2.13e- 2          0.0434            0.957           0.979\n 6         2.45e- 2          0.0503            0.950           0.975\n 7         2.25e- 2          0.0434            0.957           0.977\n 8         3.01e- 3          0.0222            0.978           0.997\n 9         2.39e- 2          0.0465            0.953           0.976\n10         2.26e- 2          0.0458            0.954           0.977\n# … with 2,990 more rows\n```\n:::\n:::\n\n\nWe can use these to visualize the probability curve for the classification model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregression_lines <- bind_cols(\n  augment(lr_poly_fit, new_data = age_range, type = \"prob\"),\n  predict(lr_poly_fit, new_data = age_range, type = \"conf_int\")\n)\n\nregression_lines %>%\n  ggplot(aes(age)) +\n  ylim(c(0, 0.2)) +\n  geom_line(aes(y = .pred_High), color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower_High), color = \"blue\", linetype = \"dashed\") +\n  geom_line(aes(y = .pred_upper_High), color = \"blue\", linetype = \"dashed\") +\n  geom_jitter(aes(y = (high == \"High\") / 5), data = Wage, \n              shape = \"|\", height = 0, width = 0.2)\n```\n\n::: {.cell-output-display}\n![](07-moving-beyond-linearity_files/figure-html/unnamed-chunk-17-1.png){fig-alt='Line chart with age on the x-axis and .pred_High on the\ny-axis. The green curve starts at zero for low values age.\nA local maxima is seen at 35 and 60. Curve goes back to zero\naround 80. Two blue dotted lines representing the confidence\ninterval around the green curve. This confidence interval\nis around 1% away from the green curve excepts when age is\nlarger than 60, where it quickly widens.' width=672}\n:::\n:::\n\n\nNext, let us take a look at the step function and how to fit a model using it as a preprocessor. You can create step functions in a couple of different ways. `step_discretize()` will convert a numeric variable into a factor variable with `n` bins, `n` here is specified with `num_breaks`. These will have approximately the same number of points in them according to the training data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_discretize <- recipe(high ~ age, data = Wage) %>%\n  step_discretize(age, num_breaks = 4)\n\ndiscretize_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_discretize)\n\ndiscretize_fit <- fit(discretize_wf, data = Wage)\ndiscretize_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_discretize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      agebin2      agebin3      agebin4  \n      5.004       -1.492       -1.665       -1.705  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:\t    730.5 \nResidual Deviance: 710.4 \tAIC: 718.4\n```\n:::\n:::\n\n\nIf you already know where you want the step function to break then you can use `step_cut()` and supply the breaks manually.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_cut <- recipe(high ~ age, data = Wage) %>%\n  step_cut(age, breaks = c(30, 50, 70))\n\ncut_wf <- workflow() %>%\n  add_model(lr_spec) %>%\n  add_recipe(rec_cut)\n\ncut_fit <- fit(cut_wf, data = Wage)\ncut_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_cut()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)   age(30,50]   age(50,70]   age(70,80]  \n      6.256       -2.746       -3.038       10.310  \n\nDegrees of Freedom: 2999 Total (i.e. Null);  2996 Residual\nNull Deviance:\t    730.5 \nResidual Deviance: 704.3 \tAIC: 712.3\n```\n:::\n:::\n\n\n## Splines\n\nIn order to fit regression splines, or in other words, use splines as preprocessors when fitting a linear model, we use `step_bs()` to construct the matrices of basis functions. The `bs()` function is used and arguments such as `knots` can be passed to `bs()` by using passing a named list to `options`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spline <- recipe(wage ~ age, data = Wage) %>%\n  step_bs(age, options = list(knots = 25, 40, 60))\n```\n:::\n\n\nWe already have the linear regression specification `lm_spec` so we can create the workflow, fit the model and predict with it like we have seen how to do in the previous chapters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spline)\n\nspline_fit <- fit(spline_wf, data = Wage)\n\npredict(spline_fit, new_data = Wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,000 × 1\n   .pred\n   <dbl>\n 1  58.7\n 2  84.3\n 3 120. \n 4 120. \n 5 120. \n 6 119. \n 7 120. \n 8 102. \n 9 119. \n10 120. \n# … with 2,990 more rows\n```\n:::\n:::\n\n\nLastly, we can plot the basic spline on top of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregression_lines <- bind_cols(\n  augment(spline_fit, new_data = age_range),\n  predict(spline_fit, new_data = age_range, type = \"conf_int\")\n)\n\nWage %>%\n  ggplot(aes(age, wage)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .pred), data = regression_lines, color = \"darkgreen\") +\n  geom_line(aes(y = .pred_lower), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\") +\n  geom_line(aes(y = .pred_upper), data = regression_lines, \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](07-moving-beyond-linearity_files/figure-html/unnamed-chunk-22-1.png){fig-alt='Scatter chart, age against the x-axis and wage against y-axis.\nFairly normally distributed around wage == 100, with some\nanother blob around wage == 275. A curve in dark green follows\nthe middle of the data with two dottled curves follows closely\naround.' width=672}\n:::\n:::\n\n\n## GAMs\n\nGAM section is WIP since they are now supported in [parsnip](https://github.com/tidymodels/parsnip/pull/512).\n",
    "supporting": [
      "07-moving-beyond-linearity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}