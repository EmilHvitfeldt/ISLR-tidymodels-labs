{
  "hash": "0a110c17b4dfd9d2e4984c563df68797",
  "result": {
    "markdown": "# Linear Regression\n\n\n::: {.cell}\n\n:::\n\n\nThis lab will go over how to perform linear regression. This will include [simple linear regression](03-linear-regression.qmd#simple-linear-regression) and [multiple linear regression](03-linear-regression.qmd#multiple-linear-regression) in addition to how you can apply transformations to the predictors. This chapter will use [parsnip](https://www.tidymodels.org/start/models/) for model fitting and [recipes and workflows](https://www.tidymodels.org/start/recipes/) to perform the transformations.\n \n## Libraries\n\nWe load tidymodels and ISLR and MASS for data sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS) # For Boston data set\nlibrary(tidymodels)\nlibrary(ISLR)\n```\n:::\n\n\n## Simple linear regression\n\nThe `Boston` data set contains various statistics for 506 neighborhoods in Boston. We will build a simple linear regression model that related the median value of owner-occupied homes (`medv`) as the response with a variable indicating the percentage of the population that belongs to a lower status (`lstat`) as the predictor.\n\n:::{.callout-important}\nThe `Boston` data set is quite outdated and contains some really unfortunate variables.\n:::\n\nWe start by creating a parsnip specification for a linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"lm\")\n```\n:::\n\n\nWhile it is unnecessary to set the mode for a linear regression since it can only be regression, we continue to do it in these labs to be explicit.\n\nThe specification doesn't perform any calculations by itself. It is just a specification of what we want to do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nOnce we have the specification we can `fit` it by supplying a formula expression and the data we want to fit the model on.\nThe formula is written on the form `y ~ x` where `y` is the name of the response and `x` is the name of the predictors.\nThe names used in the formula should match the names of the variables in the data set passed to `data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm_spec %>%\n  fit(medv ~ lstat, data = Boston)\n\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n```\n:::\n:::\n\n\nThe result of this fit is a parsnip model object. This object contains the underlying fit as well as some parsnip-specific information. If we want to look at the underlying fit object we can access it with `lm_fit$fit` or with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% \n  pluck(\"fit\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n```\n:::\n:::\n\n\nThe `lm` object has a nice `summary()` method that shows more information about the fit, including parameter estimates and lack-of-fit statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% \n  pluck(\"fit\") %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,\tAdjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWe can use packages from the [broom](https://broom.tidymodels.org/) package to extract key information out of the model objects in tidy formats.\n\nthe `tidy()` function returns the parameter estimates of a `lm` object\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236\n2 lstat         -0.950    0.0387     -24.5 5.08e- 88\n```\n:::\n:::\n\n\nand `glance()` can be used to extract the model statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.544         0.543  6.22      602. 5.08e-88     1 -1641. 3289. 3302.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n\nSuppose that we like the model fit and we want to generate predictions, we would typically use the `predict()` function like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in predict_numeric(object = object, new_data = new_data, ...): argument \"new_data\" is missing, with no default\n```\n:::\n:::\n\n\nBut this produces an error when used on a parsnip model object. This is happening because we need to explicitly supply the data set that the predictions should be performed on via the `new_data` argument\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit, new_data = Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 506 × 1\n   .pred\n   <dbl>\n 1 29.8 \n 2 25.9 \n 3 30.7 \n 4 31.8 \n 5 29.5 \n 6 29.6 \n 7 22.7 \n 8 16.4 \n 9  6.12\n10 18.3 \n# … with 496 more rows\n```\n:::\n:::\n\n\nNotice how the predictions are returned as a tibble. This will always be the case for parsnip models, no matter what engine is used. This is very useful since consistency allows us to combine data sets easily.\n\nWe can also return other types of predicts by specifying the `type` argument. Setting `type = \"conf_int\"` return a 95% confidence interval. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit, new_data = Boston, type = \"conf_int\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 506 × 2\n   .pred_lower .pred_upper\n         <dbl>       <dbl>\n 1       29.0        30.6 \n 2       25.3        26.5 \n 3       29.9        31.6 \n 4       30.8        32.7 \n 5       28.7        30.3 \n 6       28.8        30.4 \n 7       22.2        23.3 \n 8       15.6        17.1 \n 9        4.70        7.54\n10       17.7        18.9 \n# … with 496 more rows\n```\n:::\n:::\n\n\n:::{.callout-note}\nNot all engines can return all types of predictions.\n:::\n\nIf you want to evaluate the performance of a model, you might want to compare the observed value and the predicted value for a data set. You \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_cols(\n  predict(lm_fit, new_data = Boston),\n  Boston\n) %>%\n  select(medv, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 506 × 2\n    medv .pred\n   <dbl> <dbl>\n 1  24   29.8 \n 2  21.6 25.9 \n 3  34.7 30.7 \n 4  33.4 31.8 \n 5  36.2 29.5 \n 6  28.7 29.6 \n 7  22.9 22.7 \n 8  27.1 16.4 \n 9  16.5  6.12\n10  18.9 18.3 \n# … with 496 more rows\n```\n:::\n:::\n\n\nYou can get the same results using the `augment()` function to save you a little bit of typing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lm_fit, new_data = Boston) %>% \n  select(medv, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 506 × 2\n    medv .pred\n   <dbl> <dbl>\n 1  24   29.8 \n 2  21.6 25.9 \n 3  34.7 30.7 \n 4  33.4 31.8 \n 5  36.2 29.5 \n 6  28.7 29.6 \n 7  22.9 22.7 \n 8  27.1 16.4 \n 9  16.5  6.12\n10  18.9 18.3 \n# … with 496 more rows\n```\n:::\n:::\n\n\n## Multiple linear regression\n\nThe multiple linear regression model can be fit in much the same way as the [simple linear regression](03-linear-regression.qmd#simple-linear-regression) model. The only difference is how we specify the predictors. We are using the same formula expression `y ~ x`, but we can specify multiple values by separating them with `+`s.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit2 <- lm_spec %>% \n  fit(medv ~ lstat + age, data = Boston)\n\nlm_fit2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat + age, data = data)\n\nCoefficients:\n(Intercept)        lstat          age  \n   33.22276     -1.03207      0.03454  \n```\n:::\n:::\n\n\nEverything else works the same. From extracting parameter estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180\n2 lstat        -1.03      0.0482    -21.4  8.42e- 73\n3 age           0.0345    0.0122      2.83 4.91e-  3\n```\n:::\n:::\n\n\nto predicting new values\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit2, new_data = Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 506 × 1\n   .pred\n   <dbl>\n 1 30.3 \n 2 26.5 \n 3 31.2 \n 4 31.8 \n 5 29.6 \n 6 29.9 \n 7 22.7 \n 8 16.8 \n 9  5.79\n10 18.5 \n# … with 496 more rows\n```\n:::\n:::\n\n\nA shortcut when using formulas is to use the form `y ~ .` which means; set `y` as the response and set the remaining variables as predictors. This is very useful if you have a lot of variables and you don't want to type them out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit3 <- lm_spec %>% \n  fit(medv ~ ., data = Boston)\n\nlm_fit3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ ., data = data)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  \n         rm          age          dis          rad          tax      ptratio  \n  3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  \n      black        lstat  \n  9.312e-03   -5.248e-01  \n```\n:::\n:::\n\n\nFor more formula syntax look at `?formula`.\n\n## Interaction terms\n\nAdding interaction terms is quite easy to do using formula expressions. However, the syntax used to describe them isn't accepted by all engines so we will go over how to include interaction terms using recipes as well.\n\nThere are two ways on including an interaction term; `x:y` and `x * y`\n\n- `x:y` will include the interaction between `x` and `y`,\n- `x * y` will include the interaction between `x` and `y`, `x`, and `y`, i.e. it is short for `x:y + x + y`.\n\nwith that out of the way let expand `lm_fit2` by adding an interaction term\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit4 <- lm_spec %>%\n  fit(medv ~ lstat * age, data = Boston)\n\nlm_fit4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = medv ~ lstat * age, data = data)\n\nCoefficients:\n(Intercept)        lstat          age    lstat:age  \n 36.0885359   -1.3921168   -0.0007209    0.0041560  \n```\n:::\n:::\n\n\nnote that the interaction term is named `lstat:age`.\n\nSometimes we want to perform transformations, and we want those transformations to be applied, as part of the model fit as a pre-processing step. We will use the recipes package for this task.\n\nWe use the `step_interact()` to specify the interaction term. Next, we create a workflow object to combine the linear regression model specification `lm_spec` with the pre-processing specification `rec_spec_interact` which can then be fitted much like a parsnip model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec_interact <- recipe(medv ~ lstat + age, data = Boston) %>%\n  step_interact(~ lstat:age)\n\nlm_wf_interact <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_interact)\n\nlm_wf_interact %>% fit(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat          age  lstat_x_age  \n 36.0885359   -1.3921168   -0.0007209    0.0041560  \n```\n:::\n:::\n\n\nNotice that since we specified the variables in the recipe we don't need to specify them when fitting the workflow object. Furthermore, take note of the name of the interaction term. `step_interact()` tries to avoid special characters in variables.\n\n## Non-linear transformations of the predictors\n\nMuch like we could use recipes to create interaction terms between values are we able to apply transformations to individual variables as well. If you are familiar with the dplyr package then you know how to `mutate()` which works in much the same way using `step_mutate()`.\n\nYou would want to keep as much of the pre-processing inside recipes such that the transformation will be applied consistently to new data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec_pow2 <- recipe(medv ~ lstat, data = Boston) %>%\n  step_mutate(lstat2 = lstat ^ 2)\n\nlm_wf_pow2 <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_pow2)\n\nlm_wf_pow2 %>% fit(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_mutate()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat       lstat2  \n   42.86201     -2.33282      0.04355  \n```\n:::\n:::\n\n\nYou don't have to hand-craft every type of linear transformation since recipes have a bunch created already [here](https://recipes.tidymodels.org/reference/index.html#section-step-functions-individual-transformations) such as `step_log()` to take logarithms of variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec_log <- recipe(medv ~ lstat, data = Boston) %>%\n  step_log(lstat)\n\nlm_wf_log <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec_log)\n\nlm_wf_log %>% fit(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_log()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)        lstat  \n      52.12       -12.48  \n```\n:::\n:::\n\n\n## Qualitative predictors\n\nWe will now turn our attention to the `Carseats` data set. We will attempt to predict `Sales` of child car seats in 400 locations based on a number of predictors. One of these variables is `ShelveLoc` which is a qualitative predictor that indicates the quality of the shelving location. `ShelveLoc` takes on three possible values\n\n- Bad\n- Medium\n- Good\n\nIf you pass such a variable to `lm()` it will read it and generate dummy variables automatically using the following convention.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCarseats %>%\n  pull(ShelveLoc) %>%\n  contrasts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n```\n:::\n:::\n\n\nSo we have no problems including qualitative predictors when using `lm` as the engine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec %>% \n  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sales ~ . + Income:Advertising + Price:Age, \n    data = data)\n\nCoefficients:\n       (Intercept)           CompPrice              Income         Advertising  \n         6.5755654           0.0929371           0.0108940           0.0702462  \n        Population               Price       ShelveLocGood     ShelveLocMedium  \n         0.0001592          -0.1008064           4.8486762           1.9532620  \n               Age           Education            UrbanYes               USYes  \n        -0.0579466          -0.0208525           0.1401597          -0.1575571  \nIncome:Advertising           Price:Age  \n         0.0007510           0.0001068  \n```\n:::\n:::\n\n\nHowever, as with so many things, we can not always guarantee that the underlying engine knows how to deal with qualitative variables. Recipes can be used to handle this as well. The `step_dummy()` will perform the same transformation of turning 1 qualitative with `C` levels into `C-1` indicator variables.\nWhile this might seem unnecessary right now, some of the engines, later on, do not handle qualitative variables and this step would be necessary. We are also using the `all_nominal_predictors()` selector to select all character and factor predictor variables. This allows us to select by type rather than having to type out the names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec <- recipe(Sales ~ ., data = Carseats) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~ Income:Advertising + Price:Age)\n\nlm_wf <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(rec_spec)\n\nlm_wf %>% fit(Carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n         (Intercept)             CompPrice                Income  \n           6.5755654             0.0929371             0.0108940  \n         Advertising            Population                 Price  \n           0.0702462             0.0001592            -0.1008064  \n                 Age             Education        ShelveLoc_Good  \n          -0.0579466            -0.0208525             4.8486762  \n    ShelveLoc_Medium             Urban_Yes                US_Yes  \n           1.9532620             0.1401597            -0.1575571  \nIncome_x_Advertising           Price_x_Age  \n           0.0007510             0.0001068  \n```\n:::\n:::\n\n\n## Writing functions\n\nThis book will not talk about how to write functions in R. If you still want to know how to write functions we recommend the [functions](https://r4ds.had.co.nz/functions.html) of R for Data Science.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}